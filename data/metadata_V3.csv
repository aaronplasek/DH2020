authors,organisations,title,title_plain,keywords,topics,tg1_Language,tg3_Geography,tg2_Temporal,tg4_Methods,tg5_Disciplines_Fields_of_Study,abstract_plain,all_emails,acceptance
"Hoover, David Lowell","New York University - Main Campus, United States of America","Booth Tarkington, Blindness, Dictation, and the Durability of Style","Booth Tarkington, Blindness, Dictation, and the Durability of Style","style, dictation, literary composition","English, North America, 19th Century, 20th Century, attribution studies and stylometric analysis, Literary studies",English,North America,"19th Century, 20th Century",attribution studies and stylometric analysis,Literary studies,"When Booth Tarkington suffered from severe vision problems and temporary complete blindness in 1929, he began to dictate almost all of his literary works, a practice he continued even after regaining good eyesight in 1931. This submission investigates the possible effects of this change in mode of composition on Tarkington's style. The tentative conclusion is that there is almost no evidence of any effect, which suggests that authorial style can be quite durable in the face of a complete change in the way the author produces his text.",david.hoover@nyu.edu,Short Presentation
"Hoover, David Lowell","New York University - Main Campus, United States of America",Testing Rolling.Classify,Testing Rolling.Classify,"Rolling Classify, collaboration, simulation","Europe, English, North America, 19th Century, 20th Century, Contemporary, attribution studies and stylometric analysis, Humanities computing, Literary studies",English,"Europe, North America","19th Century, 20th Century, Contemporary",attribution studies and stylometric analysis,"Humanities computing, Literary studies","Rolling.Classify is a recently developed tool for studying collaboration (Eder, Rybicki, and Kestemont 2016; Eder 2016) that builds on earlier work that tested successive overlapping sections of texts (van Dalen-Oskam and van Zundert 2007, Burrows 2010, Hoover 2012). The power and ease of use of Rolling. Classify (and its related Rolling.Delta) have led to several studies based on various kinds of texts. Rigorous testing of this new method on problems with known solutions seems especially important because its results vary greatly with the choice of classification method other parameters. I will begin with simulated collaborations comprising text sections of varied lengths assembled to model different kinds of collaboration. I will then test collaborations with known contributions by the authors, and finally some in which no clear evidence of the nature of the collaboration exists.",david.hoover@nyu.edu,Short Presentation
"Anderson, Talea","Washington State University, United States of America","Effect of Promotion, Rank, and Tenure Guidelines on Open Data Distribution","Effect of Promotion, Rank, and Tenure Guidelines on Open Data Distribution",open data,"English, North America, Contemporary, data publishing projects, systems, and methods, data, object, and artefact preservation, Library & information science",English,North America,Contemporary,"data publishing projects, systems, and methods, data, object, and artefact preservation",Library & information science,"Promotion, rank, and tenure (PRT) guidelines have been cited as a key motivation for why and how faculty choose to publish their research. Open-access (OA) advocates have noted in particular that lukewarm or negative portrayals of OA venues in PRT guidelines can result in decreased participation in OA publishing. In response to this concern, some universities have begun to experiment with adding language to PRT guidelines that invites broader participation in publishing venues, including OA publishing. Of interest in this study is the impact of PRT guidelines on the distribution of open data. Specifically, this poster will rely on information gathered from SHARE to consider the prevalence of participation in open data publishing at schools that include more expensive PRT guidelines.",talea.anderson@wsu.edu,Poster
"Palladino, Chiara (1); Zhang, Anna (1); Foradi, Maryam (2); Yousef, Tariq (3)","1: Furman University, United States of America; 2: Digital Humanities, University of Leipzig, Germany; 3: NLP Group, University of Leipzig, Germany",How to Read All Languages: Translation Alignment with Ugarit,How to Read All Languages: Translation Alignment with Ugarit,"translation alignment, digital pedagogy, language learning","Comparative (2 or more geographical areas), English, BCE-4th Century, 5th-14th Century, 15th-17th Century, electronic literature production and analysis, public humanities collaborations and methods, Literary studies, Translation studies",English,Comparative (2 or more geographical areas),"BCE-4th Century, 5th-14th Century, 15th-17th Century","electronic literature production and analysis, public humanities collaborations and methods","Literary studies, Translation studies","This workshop will illustrate the importance of translation alignment in the field of slow reading and language learning. We will provide a short theoretical overview on the principles of translation alignment, together with a hands-on tutorial on Ugarit (http://ugarit.ialigner.com/), a web-based translation alignment editor. Ugarit is designed as a Citizen Science tool, aiming at collecting training datasets of manually aligned words from diverse text corpora. The ultimate goal of Ugarit is to improve automatic translation alignment methods and to implement a set of dynamic lexica, with particular regard for languages with less supported infrastructures. However, the tool also has a strong pedagogical potential, which has been assessed in the course of various hands-on workshops and in an ongoing integration in school curricula: we have tested how translation alignment with Ugarit can help readers to engage with languages that they have never seen, grasping their essential semantic and morphological aspects. We propose text alignment as a way to empower the perception of the complexity of a language, but also as a method to leverage usual obstacles in the process of reading apparently “impenetrable” sources by directly engaging with them. At present, Ugarit includes aligned pairs from 36 languages (including less represented languages, like Bulgarian, Ethiopic, Sanskrit, Yiddish, and Armenian), 277 unique users, and about 23,400 parallel texts hosted.","chiara.palladino@furman.edu, anna.zhang@furman.edu, maryam.foradi@uni-leipzig.de, tariq@informatik.uni-leipzig.de",Workshop/Tutorial 4
"Bordalejo, Barbara (1); O'Donnell, Daniel Paul (2)","1: University of Saskatchewan, Canada; 2: University of Lethbridge, Canada",Diversity and Inclusion for Digital Humanists,Diversity and Inclusion for Digital Humanists,"Diversity, Inclusion, Bias, Privilege","Global, English, Contemporary, digital activism and advocacy, Disability and differently-abled studies, Feminist studies",English,Global,Contemporary,digital activism and advocacy,"Disability and differently-abled studies, Feminist studies","This workshop highlights issues of diversity, inclusivity, and collaboration in Digital Humanities.Through practical exercises and dialogue, we build a safe atmosphere for the discussion of strategies to isolate obstacles preventing diversity and offer solutions for the development of inclusive environments.As part of our work, we developed The Privilege Game, used to emphasize and showcase the many different kinds of privilege derived from our society's power structures and to create awareness among practitioners of the contrasts to be found in the rich and ever-growing space of the Digital Humanities.We cover topics such as ""implicit bias,"" ""cultural cloning,"" privilege, intersectionality and solutions to be implemented in the creation of safe and inclusive environments.","barbara.bordalejo@usask.ca, daniel.odonnell@uleth.ca",Workshop/Tutorial 4
"Boateng, Akwasi Bosompem","North-West University, South Africa",Social media in political engagements in Africa: A study of Twitter use in the intra-party elections of political parties in Ghana ,Social media in political engagements in Africa: A study of Twitter use in the intra-party elections of political parties in Ghana ,"Relationship Management, Elections, Political Engagement, Social Media, Twitter","Africa, English, Contemporary, social media analysis and methods, Communication studies, Political science",English,Africa,Contemporary,social media analysis and methods,"Communication studies, Political science","The advent of social media is changing the dynamics of political communication, engagements and election campaigns across the world. These platforms extend opportunities for political actors especially political parties, governmenta and citizens to engage directly to build mutually beneficial relationships through engagements ans interactions. Using interviews and content analysis, this article explores “Twittering” in political communication and engagements; how political parties use social media especially during their 2018 intra-party elections in Ghana. It examines how the New Patriotic Party and National Democratic Congress in Ghana use Twitter in intra-party elections. The article gathers data from political parties and their Twitter pages to theorize the political appropriation of social media in Ghana.The findings show that the political parties minimally appropriate Twitter; as they occasionally used it for public information purposes than interactive engagements and mutually beneficial relationships via two-way symmetrical communication. There were insignificant amount of tweet made by the political parties during their intra-party elections in 2018, which indicate that political use of social media in Ghana is still infantile. Therefore, political parties need to improve their use of Twitter and other social media platforms to take maxim advantage of the interactive features and relationship building potentials to garner support from the public and votes in elections. In this regard, political parties require enlightenEd communication strategies and professional political public relations tact for improved use to mature in social media communication.",beebeeboateng@gmail.com,Poster
"Heintzman, Kit","Harvard University, Canada",Greening the Digital Humanities,Greening the Digital Humanities,"carbon emissions, environmentalism","Global, English, Contemporary, eco-criticism and environmental analysis, sustainable procedures, systems, and methods, History of science",English,Global,Contemporary,"eco-criticism and environmental analysis, sustainable procedures, systems, and methods",History of science,"This workshop would introduce audiences to the following issues: recent historiography attention to the internet's carbon footprint, a case study in a semester long system of tracking the carbon emissions of a digital humanities course, trnasferable pedagogical strategies with regard to eco-conscious digital scholarshp, and critical reflections upon issues of ""individualism"" in discourse of digital consumption patterns versus system changes.",kheintzman@fas.harvard.edu,Workshop/Tutorial 2
"Ohya, Kazushi","Tsurumi University, Japan","An online course system easy to make, preserve, and promote critical thinking","An online course system easy to make, preserve, and promote critical thinking","radio lecture system, HTML Imports, Web Components","Global, English, Contemporary, data publishing projects, systems, and methods, sustainable procedures, systems, and methods, Education/ pedagogy, Media studies",English,Global,Contemporary,"data publishing projects, systems, and methods, sustainable procedures, systems, and methods","Education/ pedagogy, Media studies","In this poster presentation we will show a new online course consisting of talks and chalks that is easy to make, edit, and preserve, and is a traditional and old-fashioned lecture style, that substantially helps students learn the content spontaneously. The course materials we need to make are only scripts for talks and XML data for slides, and the process we need to undertake separately is just recording the talk. The mechanisms to realize this course are backed with HTML5 and simple codes in JavaScript based on the specification HTML Imports. This lecture system is important not only for an implementation to realize a new teaching/learning channel conveying the knowledge instantly to learners but also to show the evidence for usefullness of HTML Imports as a sign of the existence of users' requirements.",oya-k@tsurumi-u.ac.jp,Poster
"Calvo Tello, José",Göttingen State and University Library," What is a Genre? A Graph Unified Model of Categories, Texts, and Features "," What is a Genre? A Graph Unified Model of Categories, Texts, and Features ","genre, model, graph, novels, Spanish","Comparative (2 or more geographical areas), Europe, English, BCE-4th Century, 19th Century, 20th Century, cultural analytics, text mining and analysis, Literary studies",English,"Comparative (2 or more geographical areas), Europe","BCE-4th Century, 19th Century, 20th Century","cultural analytics, text mining and analysis",Literary studies,"Several theoretical models have been proposed for genre, such as the Aristotelian scholastic taxonomy, the family resemblance and the prototype theory. However, these models lack of empirical applications to real examples of genres. This proposal is the culmination of a series of analysis, presenting a theoretical, computational and visual graph-based model that fits several observations. This formalization unifies components of the previous theories, offering visually the intention (internal features) and extension (the best representatives and instances) of each category. Besides, it allows two intuitive interpretations based on the evaluation: the centrality as classification results, and the distance as similarity through shared features. The model is applied to three data-sets of different periods and languages: modern Spanish novels, classic French plays and the books of the Bible.",jose.calvo@uni-wuerzburg.de,Long Presentation
"Hannesschläger, Vanessa; Wissik, Tanja","Austrian Centre for Digital Humanities, Austrian Academy of Sciences, Austria",Opening up Open Data: Strategies & success stories,Opening up Open Data: Strategies & success stories,"Open Data, Open Source, Hackathon, Community involvement","Global, Europe, English, Contemporary, open access methods, public humanities collaborations and methods, Humanities computing",English,"Global, Europe",Contemporary,"open access methods, public humanities collaborations and methods",Humanities computing,"Not only in the context of Digital Humanities but also in other research areas the Open Data movement is gaining momentum. For this reason, the Austrian Centre for Digital Humanities of the Austrian Academy of Sciences (ACDH) started an experiment at the beginning of 2019: For the first time ever, we published the calls for participation in three virtual hackathons funded by the Austrian manifestations of DARIAH and CLARIN, CLARIAH-AT. These hackathons focused on Open Data sets that are publicly available online, and the tasks to perform on these data involved the creation of Open Source code. Each of the hackathons had a special theme and was co-timed with an event that involved an aspect of Openness. These events also inspired the choice of the respective data sets.Usually hackathons take place on site, participants are given tasks to be solved within a given timeframe in a fixed location. This requires the programmers to be flexible and available and to have access to travel funding. A virtual hackathon on the other hand offers people all over the globe the possibility to participate and contribute without having to travel. Therefore, our approach enabled a much larger community to participate in the event on the one hand, thus also promoting the benefits of Open Data on the other.The first hackathon was carried out in cooperation with the European Lexicographical Infrastructure (ELEXIS) and focused on lexicographical data (the Digital Dictionary of Tunis Arabic). The task was to develop a creative mode of processing it, e.g. by enriching it, visualizing it, doing statistical analysis, or integrating it with other resources (e.g. LOD). The submissions were to be handed in by the end of the first ELEXIS observer event taking place in February 2019.For the second hackathon, the ACDH cooperated with the City of Vienna and chose a set from the city’s Open Government Data platform to be processed. The task was to be completed on the Open Data Day Vienna 2019 (28 February 2019). The aim was to develop a creative mode of processing cartographical data showing damage to buildings in Vienna during World War II.The third and final hackathon of the series offered a task to be completed on the International Open Data Day 2019 (2 March 2019). For this final highlight, a choice of two Open Data sets was offered to the participants. The first data set to be worked on in this task was a collection of XML/TEI transcriptions of early German travel guides. The second data set consisted of German historical newspapers and was provided in cooperation with the Europeana newspaper project.The best contributions were determined by an international board of judges and received cash prizes. The criteria for judgement were creativity and innovation, accessibility, reusability and reproducibility, as well as elegance. In our presentation, we will share the lessons learned and show how Open Science was the necessary precondition for this project, as well as what inspired its ultimate success. We will make our case by giving insights into the lessons learned in 2019 and sharing how we improved the concept for the second round of the hackathon series in 2020.","vanessa.hannesschlaeger@oeaw.ac.at, tanja.wissik@oeaw.ac.at",Lightning
"SAIBU, ISRAEL ABAYOMI (1); SAIBU, AYOMIDE JOSEPH (2)","1: ANCHOR UNIVERSITY, LAGOS NIGERIA.; 2: LAGOS STATE UNIVERSITY OJO, LAGOS NIGERIA",PRESERVATION OF OSUN-OSOGBO CULTURAL HERITAGE IN NIGERIA: A DIGITIZATION DISCOURSE,PRESERVATION OF OSUN-OSOGBO CULTURAL HERITAGE IN NIGERIA: A DIGITIZATION DISCOURSE,"Preservation, Digitization, Cultural Heritage, Osun-Osogbo, Nigeria","Africa, English, 19th Century, Contemporary, digital archiving, digitization (2D & 3D), Art history, History",English,Africa,"19th Century, Contemporary","digital archiving, digitization (2D & 3D)","Art history, History","The Osun-Osogbo cultural heritage has become one of the most striking cultural identities to have emerged in Nigeria. The uniqueness and importance of Osun-Osogbo has led to its recognition by the United Nations Educational Scientific and Cultural Organization (UNESCO) as a global cultural heritage. Thus, it is imperative that this cultural heritage be digitized for the preservation of its essence for posterity and global visibility.","alerosaibu@gmail.com, ayomidesaibu2001@gmail.com",Lightning
"Herold, Nastasia (1,2); Ottawa, Thérèse (2)","1: University of Leipzig, Germany; 2: Wikipetcia Atikamekw Nehiromowin, Canada",Ethics and responsibilities of open access - lessons learned from the Wikipedia project of the Atikamekw First Nation,Ethics and responsibilities of open access - lessons learned from the Wikipedia project of the Atikamekw First Nation,"open access, wikipedia, atikamekw, first nations, collaborative","English, North America, Contemporary, digital access, privacy, and ethics analysis, open access methods, First nations and indigenous studies, Linguistics",English,North America,Contemporary,"digital access, privacy, and ethics analysis, open access methods","First nations and indigenous studies, Linguistics","Abstract by Nastasia Herold & Thérèse Ottawa Ethics and responsibilities of open access – lessons learned from the Wikipedia project of the Atikamekw First Nation With 97,9% (cf. INAC 2019), in Canada, the Atikamekw First Nation has the highest percentage of people who speak their native language (Atikamekw) at home. The Atikamekw live in Quebec, Canada, and have a population of 8,000 people in three communities.In 2013, Nastasia Herold, one of the authors of this paper did a field study in Manawan, one of the three Atikamekw communities, for a research on the local bilingualism (Atikamekw and French). Despite the vitality of the Atikamekw language, a survey and interviews showed that francization and language change are processes noticed by all living generations of the Atikamekw (cf. Herold 2020: N.N.).Communication takes place more and more often digitally (cf. Reichert 2017: 26-27), and this in written language rather than orally. Atikamekw has a standardized orthography since 1994 (cf. Dinnison 21997) and is taught in Manawan’s primary school as a first language and as medium of alphabetization. However, Herold’s (2019: 103) research in 2013 showed that the Internet contained no written text in the Atikamekw language, the Atikamekw used the Internet mainly in French. This is why a school project at Manawan’s secondary school was initiated in 2013 in order to create a Wikipedia site in the Atikamekw language.Many lessons have been learned during the collaboration of academics, teachers, pupils, local language experts and other local voluntary contributors. The lessons we would like to focus on in this presentation are the lessons learned when implementing cultural knowledge to an open access platform. Open access is the free provision of (scientific) texts on the web without restriction of use (cf. Kohle 2017: 203), and this free provision has advantages and disadvantages.We will give four examples which show that the Atikamekw made sure that the knowledge was published respecting their own principles, beliefs and tradition. These four examples and the development of the project show how it is important that the community itself dictates rules that have to be respected when publishing their knowledge under open access.Finally we will answer Rehbein’s and Thies’ (2017: 355) question they developed as a schema for questions of responsibilities and ethics of a specific project: Who (1) is responsible for what (2) to whom (3) before which instance (4) according to which standards (5).","nastasia.herold@uni-leipzig.de, thereseottawa@gmail.com",Short Presentation
"Shibutani, Ayako (1); Goto, Makoto (2)","1: The University Museum, the University of Tokyo, Japan; 2: National Museum of Japanese History, Japan",How Do Research Data Develop? International Standardisation of Scientific Data in Historical Studies,How Do Research Data Develop? International Standardisation of Scientific Data in Historical Studies,"Cultural Heritage Science, Japanese history, linked data, open data, standardization","Asia, English, 15th-17th Century, 18th Century, 19th Century, digital research infrastructures development and analysis, open access methods, History, History of science",English,Asia,"15th-17th Century, 18th Century, 19th Century","digital research infrastructures development and analysis, open access methods","History, History of science","The scientific approach to historical resources creates a synthetic discipline benefitting from open access to data from the humanities and sciences. However, technological challenges exist because of dispersed and heterogeneous resource data. Through data sharing of historical resources, this paper proposes that the establishment of integrated data repositories will capture data provenance and diversity while promoting attribution and acknowledgment of its use. TTo enhance and accelerate scientific advances in historical studies, we enumerate digital humanities applications to solve the technological and sociological challenges that have limited open access to resource data in the world. We also standardise the scientific methodology of historical materials research using the following approaches: a qualitative analysis focusing on component details to compare our findings with the classifications granted to historical materials in previous studies, and the reconstruction of papermaking methods using DNA analysis.","ashibutani@hi.u-tokyo.ac.jp, m-goto@rekihaku.ac.jp",Poster
"Licastro, Amanda Marie (1); Skallerup Bessette, Lee Elaine (2); Whalen, Zachary N (3); McGrail, Anne B. (4)","1: Stevenson University, United States of America; 2: Georgetown University; 3: University of Mary Washington; 4: Lane Community College, Eugene, Oregon US",Exploring the Undiscovered Contours of DH,Exploring the Undiscovered Contours of DH,"pedagogy, marginalization, digital studies, infrastructure, curriculum","English, North America, Contemporary, curricular and pedagogical development and analysis, digital activism and advocacy, Education/ pedagogy",English,North America,Contemporary,"curricular and pedagogical development and analysis, digital activism and advocacy",Education/ pedagogy,"How can scholars on the margins of DH articulate their work in DH publications, grants, and other professional and disciplinary outlets? In this interactive forum, we aim to explore inclusion--or exclusion--in what counts as “digital humanities” among scholars across disciplines, institutional contexts, and employment statuses. We begin by surveying audience members about the alternative ways that they represent their digital work and their different institutional contexts. We then ask participants to explore how esoteric terms such as “digital humanities” may be illegible to administrators and the public and the effects of this illegibility on their pedagogy and professional work. After collectively articulating the problem as it stands today, forum leaders will facilitate a problem-solving conversation that might begin addressing the issue.","amanda.licastro@gmail.com, ls1335@georgetown.edu, zwhalen@umw.edu, mcgraila@lanecc.edu",Forum
"Ma, Rongqian","University of Pittsburgh, United States of America",CR/10 Website as a Digital Public Humanities (DPH) Site,CR/10 Website as a Digital Public Humanities (DPH) Site,"Public digital humanities, user experience design, CR/10, cultural revolution","Asia, English, North America, 20th Century, Contemporary, public humanities collaborations and methods, user experience design and analysis, Design studies, Library & information science",English,"Asia, North America","20th Century, Contemporary","public humanities collaborations and methods, user experience design and analysis","Design studies, Library & information science","Initiated by Mao Zedong (1893-1976) and lasting from 1966 to 1976, the Cultural Revolution in China has been defined as a 10-year disaster by the Chinese Communist Party (CCP), which caused civil unrest to the party, the state, and the people (CCP, 1981). Forty years after the end of the Cultural Revolution, with the intention to promote the public remembrance and discussion of this significant period of Chinese history, the East Asian Library at the University of Pittsburgh launched CR/10 (University of Pittsburgh, 2019), a digital oral history project that aims to collect and preserve authentic memories of the Cultural Revolution through 10-minute semi-structured interviews. Using the technique of snowball sampling for interviewee selection, the project has thus far collected more than 300 interviews with ordinary people from different generations, geographies, social and educational backgrounds who experienced the incident or learned about it from family, school, or other resources. Among all the interviewees, some currently live in the U.S. while some in China, whose everyday experiences post the Cultural Revolution may have exerted impacts on their individual memories. Most interviews were conducted in mandarin Chinese and were then translated into English as subtitles by the CR/10 team. All the interviews were video-recorded and made open-access on the interactive CR/10 website. The website consists of four major components: an introduction to the project, a trailer for the project that showcases interviews, a timeline and a map of China demonstrating the temporal and geographical distributions of the videos in the collection.In this paper, I treat the CR/10 website as the object of study and demonstrate its value as a digital public humanities site bridging the gap between the academy and the general public. Cox and Tilton (2019) defined the term “digital public humanities (DPH)” as practices that “facilitate reflection and collaboration with participants outside of the academy through digital theories and technologies” (p. 130). Focusing on interactive and mindful design (Drucker, 2013), the CR/10 website invites the public to participate in, as well as contributing to, developing a diversified and multifaceted understanding of the Cultural Revolution. In this study, I present a series of user experience research conducted regarding the design of the website with 15 users outside of the academy, to examine the current usability as well as to identify further design possibilities of the website. Through techniques of semi-structured contextual interviews and focus group study, this research study aims to reflect on the reception and use of the CR/10 website as an interactive teaching and learning platform, rather than solely a repository of collective memories or database of historical information. More specifically, I examine how the timeline and map features of the website enrich interpretations of the Cultural Revolution, especially in terms of inspiring users to reflect upon the following questions: How could the Cultural Revolution be defined? What factors (e.g., geography, generation, family, class backgrounds, and education) influence impressions and memories of the Cultural Revolution? Extending from the user experience research, this study also proposes recommendations to improve the design of the CR/10 website to create a collective “memory atlas” (Cornell University Library, 2013; Forster, 1976) out of the video collection. Findings of this study contribute to facilitating academy-public collaborations in building DPH sites from the user perspective and a design-mediated approach.",rom77@pitt.edu,Short Presentation
"Van Zundert, Joris J. (1); Mar, Raymond A. (2); van Dalen–Oskam, Karina (1,3); Temple, Emily (4); Bowman, Isabel (5); Heidari, Farzaneh (6); Nguyen, Ahn T.P. (2)","1: Department of Literary Studies, Huygens Institute for the History of the Netherlands – Royal Netherlands Academy of Arts and Sciences. Amsterdam, The Netherlands; 2: Department of Psychology, York University. Toronto, Canada; 3: Faculty of Humanities, University of Amsterdam. Amsterdam, The Netherlands; 4: Literary Hub; 5: Department of Psychology, University of Toronto. Toronto, Canada; 6: Department of Electrical Engineering and Computer Sciences, York University. Toronto, Canada",Features of Timelessness: Intermediate Report on a Quest for Stylistic Features that Mark Literary Canonicity,Features of Timelessness: Intermediate Report on a Quest for Stylistic Features that Mark Literary Canonicity,"stylometry, literature, features, canonicity","Europe, English, North America, 20th Century, Contemporary, attribution studies and stylometric analysis, Humanities computing, Literary studies",English,"Europe, North America","20th Century, Contemporary",attribution studies and stylometric analysis,"Humanities computing, Literary studies","We report on our ongoing quest to establish a validated complex of stylistic features that act as markers for literary canonicity, in specific contexts. Currentely we present a stylometric analysis of literature investigating the stylistic markers that differentiate former bestsellers from fiction that remains popular across several decades using a TfIdf vectorization of texts and UMAP dimenision reduction approach. We find that especially a greater variation in sentence length is associated with the chances of a novel to remain popular.","joris.van.zundert@huygens.knaw.nl, mar@yorku.ca, karina.van.dalen@huygens.knaw.nl, etemple@lithub.com, isabel.bowman@mail.utoronto.ca, farzanah@cse.yorku.ca, ntpanh1602@gmail.com",Short Presentation
"Schildkamp, Philip (1); Harzenetter, Lukas (2); Leymann, Frank (2); Mathiak, Brigitte (1); Neuefeind, Claes (3); Breitenbücher, Uwe (4)","1: Data Center for the Humanities, University of Cologne, Germany; 2: Institute of Architecture of Application Systems, University of Stuttgart, Germany; 3: Cologne Center for eHumanities, University of Cologne, Germany; 4: University of Stuttgart, Germany",Workshop on Modelling and Maintaining Research Applications in TOSCA,Workshop on Modelling and Maintaining Research Applications in TOSCA,"Living Systems, Research Applications, Software Stacks, Sustainability, TOSCA","Global, English, Contemporary, software development, systems, analysis and methods, sustainable procedures, systems, and methods, Humanities computing, Informatics",English,Global,Contemporary,"software development, systems, analysis and methods, sustainable procedures, systems, and methods","Humanities computing, Informatics","AbstractThe project ""SustainLife – Sustaining Living Digital Systems in the Humanities"" that is currently running at the Institute of Architecture of Application Systems (IAAS, University of Stuttgart) and the Data Center for the Humanities (DCH, University of Cologne) deals with the conservation of research applications in the field of Digital Humanities (DH). By employing the TOSCA standard (Topology and Orchestration Specification for Cloud Applications) to fully automate the deployment of DH applications and to keep them available in the long term, we try to tackle the problem of software obsolescence in the field of DH. To interactively demonstrate our approach to the international DH community, we would like to give a workshop on the topic ""Modelling and Maintaining Research Applications in TOSCA"" in the run-up to the DH 2020 conference. Thereinwe will show how to model (DH) software systems with TOSCA and share experiences and best practices on how to work with the OpenTOSCA ecosystem, an open-source implementation of the TOSCA standard.The ProblemThe establishment of the DH as an independent scientific research area as well as the increasing usage of digital methods in the research process require adjustments to common result assurance practices. For example, the long-term archiving (LTA) of primary research data uses well-established practices such as employing standardized data formats and forwarding data to permanent repositories. However, the fact that digital artifacts generated in DH-oriented research do not only consist of primary data but also contain research software is mostly disregarded (Sahle and Kronenwett, 2013). Moreover, the variety of DH research outcomes includes so-called ""living systems"" in which the software to present, access or analyze the data represents an essential part of the actual research output (Bingert et al., 2016). In contrast to classical research results such as monographs or encyclopedias, living systems cannot be served long-term without maintenance as their instantiation, supervision, and permanent provisioning represent major technical, organizational, and financial challenges. Furthermore, the heterogeneity of the research software generated in the DH requires a highly flexible preservation strategy, i.e., a suitable technology that ensures standardization, reusability, and archiving of as many digital artifacts as possible (Barzen et al., 2018). In addition to the aforementioned challenges, i.e., heterogeneity, underfunding, and obsolescence of digital artifacts, scientific practice requires long-term interoperability and traceability of all research outcomes. With regard to digital systems, these requirements are (1) constant accessibility, (2) the possibility of error-free operation, and (3) the ability to reconstruct any stage of development of a research application at any time without major structural difficulties.Our ApproachThe TOSCA standard (OASIS, 2013 and 2019) allows software systems to be modelled, provisioned, and deployed in a standardized and provider-independent manner. Thus, it is suitable for long-term archiving and operation of research applications produced within the field of DH (Neuefeind et al., 2018 and 2019). Following the TOSCA standard, applications are modelled in “Topology Templates” by describing their components and their relations amongst each other: Components are represented as “Node Templates”, while relations are modeled as “Relationship Templates”. Moreover, the semantics of a Node Template or Relationship Template are dictated by reusable types, i.e., “Node Types” and “Relationship Types” respectively. For example, a Python web application can be modelled as a Node Template that is an instance of the ""Python Application"" Node Type. To express that the Python Application accesses a MySQL database, a second Node Template that is of type ""MySQL Database"" can be added to the Topology Template. Then, the connection between both components can be described by a Relationship Template that is an instance of the Relationship Type ""connectsTo"". Additionally, to specify that both components are running on an Ubuntu virtual machine (VM), a Node Template of type ""Ubuntu VM"" can be added, while Relationship Templates of type “hostedOn” between the Python Application Node Template and the VM Node Template, as well as between the MySQL Database and the VM describe their respective hosting relations.Hereby, TOSCA's type system enables the modelling of reusable component types, e.g., the ""Python Application"" Node Type, which can be reused in multiple Topology Templates describing different applications. Therefore, synergic effects emerge as existing Node Types can be reused in other Topology Templates, easing the modelling of new applications. In addition, the open-source TOSCA implementation OpenTOSCA (Breitenbücher et al., 2016) offers the possibility to graphically model applications using the TOSCA editor “Winery” (Kopp et al., 2013) which further simplifies the creation of new applications by providing drag-and-drop modeling capabilities.Workshop CurriculumDuring our four hour workshop, we will (1) give an overview to different solutions for long-term preservation of living systems and (2) describe the modeling language TOSCA. Based on these theoretical units, practical tasks will introduce (3) the modelling of an existing application using TOSCA and (4) how applications can be deployed using the OpenTOSCA ecosystem. Thus, by combining the theoretical foundations and the practical application of TOSCA, the participants will be able to model (research) software systems according to the standard and provision and deploy applications using the OpenTOSCA ecosystem.The practical tasks are structured as follows: (1) Identify the components of an application and (2) describe them and their relations among each other in an TOSCA-based application topology, i.e., in a Topology Template. By fragmenting an application into its components and mapping them to TOSCA Node Types, the Topology Template describing the application can then be modelled using the OpenTOSCA ecosystem. Afterwards (3), the modelled TOSCA application will be deployed by the OpenTOSCA runtime. Moreover, by sharing our experiences and best practices in using OpenTOSCA with the community, we will introduce concepts such as ""software stacks"" in a practical way.Target GroupThe workshop is primarily designed for data center employees, libraries and other institutions focusing on infrastructures for long-term archiving and operation of heterogeneous software systems. Previous experience in dealing with Linux and writing shell scripts as well as with software stacks and service orchestration are helpful but not necessary for a successful participation. To provide a productive context for communicating the described content and to enable individual consultation and support, we designed the workshop for about 20 participants but limit it to a maximum of 30 participants.Technical PrerequisitesFor a successful participation in the workshop, it is necessary that each participant brings his/her own laptop. Although a shared instance of the OpenTOSCA ecosystem will be provided, it is desirable that all participants set up an OpenTOSCA instance on their work equipment prior to the workshop in order to perform modelling and deployment tasks on their own devices. Therefore, registered participants will be provided with all necessary information about system requirements and how to setup OpenTOSCA prior to the workshop. Furthermore, relevant documentation, publications, and manuals will be provided both in advance and in the context of the workshop. In addition, a stable internet connection as well as a sufficient number of power outlets for all electronic devices are indispensable. About the InstructorsUwe Breitenbücher is a research staff member and postdoc at the Institute of Architecture of Application Systems (IAAS) at the University of Stuttgart, Germany. His research vision is to improve cloud application provisioning and application management by automating the application of management patterns. Uwe was part of the CloudCycle project, in which the OpenTOSCA Ecosystem was developed. His current research interests include cyber-physical systems, blockchains, and microservices.Anna Fischer is a research assistant at the Data Center for the Humanities (DCH) at the University of Cologne and joined the “SustainLife” Project in January 2020. Her recent research and working activities have focused on data management and software development for natural language processing tasks, e.g., in collaboration with one of the chairs for Romance linguistics at the University of Cologne. Lukas Harzenetter is a research associate at the Institute of Architecture of Application Systems (IAAS) at the University of Stuttgart, Germany. He received his Master of Science degree from the University of Stuttgart in Software Engineering in 2018. His research interests are in the field of cloud deployment and management models focusing on the development and change of such models over time. Lukas is part of the “SustainLife” project which is working on sustainable application deployments in the domain of digital humanities.Frank Leymann is a full professor of computer science and director of the Institute of Architecture of Application Systems (IAAS) at the University of Stuttgart, Germany. His research interests include service-oriented architectures and associated middleware, workflow- and business process management, cloud computing and associated systems management aspects, and patterns. Frank is co-author of more than 400 peer-reviewed papers, about 70 patents, and several industry standards. He is an elected member of the Academy of Europe.Brigitte Mathiak is chairwoman of the Data Center for the Humanities (DCH) and is particularly interested in data management and text mining. The idea for the ""SustainLife"" project arose after she had experienced again and again how living systems have to be abandoned or neglected. She is Junior Professor for Digital Humanities at the University of Cologne and Senior Scientist at the Leibniz Institute for the Social Sciences (GESIS).Claes Neuefeind is a postdoc at the Cologne Center for eHumanities (CCeH) at the University of Cologne. He worked with Philip Schildkamp and Lukas Harzenetter on the DFG-LIS project ""SustainLife"" until October 2019 and changed for a position that is responsible for coordinating the Digital Humanities of the North Rhine-Westphalian Academy of Sciences and the Arts office.Philip Schildkamp has been researching since 2015 and teaching since 2017 at the University of Cologne. He studied sociology, psychology, and Digital Humanities information processing. The main topics of his employment are technical infrastructure measures in the field of (Digital) Humanities and the orchestration of distributed software systems. Since March 2018, Philip has been part of the DFG-LIS project ""SustainLife"" at the Data Center for the Humanities (DCH). This poster is partially funded by the DFG-LIS project “SustainLife” (GEPRIS 379522012).","philip.schildkamp@uni-koeln.de, lukas.harzenetter@iaas.uni-stuttgart.de, frank.leymann@iaas.uni-stuttgart.de, bmathiak@uni-koeln.de, c.neuefeind@uni-koeln.de, uwe.breitenbuecher@iaas.uni-stuttgart.de",Workshop/Tutorial 4
"Chokshi, Crystal Nicole","University of Calgary, Canada",Gmail’s Smart Compose: A Critical Composit(ion),Gmail’s Smart Compose: A Critical Composit(ion),"algorithmically-mediated writing, Smart Compose, critical algorithm studies, critical media studies, linguistic capitalism","English, North America, Contemporary, information retrieval and querying algorithms and methods, text mining and analysis, Communication studies, Media studies",English,North America,Contemporary,"information retrieval and querying algorithms and methods, text mining and analysis","Communication studies, Media studies","While Google once merely monitored users’ words, today the company literally writes them. This is thanks to Smart Compose, a word-prediction algorithm that Google has launched in Gmail and Google Docs. The algorithm depends on Google’s meticulous recording and machine-reading of the personal data of an untold number of its 1.5 billion Gmail users, leveraging users’ words and writing for the development of the technology. In this context, language is particularly vulnerable to corporate intervention and manipulation. As such, this presentation carefully considers Fréderic Kaplan’s (2014) call to action: “through… the advent of algorithms as a new media, something is likely happen [sic] to language, and, although we are not yet sure what it will be, new tools must be built in order to understand this global linguistic evolution” (p. 62).Responding to Kaplan's call, I report on experiments with Smart Compose in which I am manually transcribing more than 50,000 words from published texts and, subsequently, annotating and visualizing input to and output from the algorithm. These experiments are part of my larger doctoral project that seeks to locate the shifting ""semantic coordinates"" (Striphas, 2015, p. 398) of ""language,"" ""words,"" and ""writing"" in an algorithmic culture. Applying the framework of data colonialism (Couldry & Mejias, 2019), I argue that word-prediction algorithms such as Smart Compose must necessarily shift our understanding of these terms when words become data (Thornton, 2019) and writing becomes a datafied practice.More broadly, I suggest that in place of the question posed by Siva Vaidhyanathan (2011) some years ago—“what do we gain and what do we lose by inviting Google to be the lens through which we see the world?” (p. 9)—we must, urgently and necessarily, ask this: what do we gain and what do we lose by allowing Google to offer the words through which we write the world?",crystal.chokshi1@ucalgary.ca,Short Presentation
"Horak, Laura","Carleton University, Canada",Transing DH: Adopting a Trans-Centric Approach to Building the Transgender Media Portal,Transing DH: Adopting a Trans-Centric Approach to Building the Transgender Media Portal,"transgender, cinema, database, ethics, feminist","English, North America, 20th Century, Contemporary, database creation, management, and analysis, public humanities collaborations and methods, Film and cinema arts studies, Transgender and non-binary studies",English,North America,"20th Century, Contemporary","database creation, management, and analysis, public humanities collaborations and methods","Film and cinema arts studies, Transgender and non-binary studies","Guided by the fields of intersectional feminist digital humanities and transgender studies, this talk explores our team’s efforts to adopt a trans-centric approach to building a collaborative online tool to investigate and publicize films made by trans, Two Spirit, nonbinary, intersex, and gender-nonconforming people.",laura.horak@carleton.ca,Short Presentation
"Satlow, Michael (1); Sperling, Michael (2)","1: Brown University, United States of America; 2: New York University, United States of America",The Rabbinic Social Network,The Rabbinic Social Network,"historical networks, gephi, pattern recognition","Asia, English, BCE-4th Century, 5th-14th Century, network analysis and graphs theory and application, History, Theology and religious studies",English,Asia,"BCE-4th Century, 5th-14th Century",network analysis and graphs theory and application,"History, Theology and religious studies","This project attempts to apply the techniques of social network analysis (SNA) and visualization to the representations of rabbinic interactions in the Babylonian Talmud, a sprawling text written in Hebrew and Aramaic and probably redacted in Babylonia (modern day Iraq) in the sixth century CE. Our goals are (1) to develop a workflow and methodology allowing us to visualize and analyze the interactions between rabbis as represented in the Babylonian Talmud; (2) to see if we could learn something new about the relationship between rabbis as represented in the Talmud and/or the process of its redaction; and (3) to present a public-facing interface allowing scholars to interact directly with our visualization.Many of the research questions that drive this project go back more than a century. Pioneering work in Jewish studies (especially Albeck (1969); Margolioth (1987)) has attempted to detail the relationships between some of these rabbis. This work remains valuable, although it sometimes uses outdated methodological assumptions. Some of the relationships, for example, are reconstructed on the basis of stories about rabbis that most scholars today would understand as late, fictional creations. So too, scholars have long tried to understand the process by which the Babylonian Talmud was redacted (for summary of the scholarship on this, see Rubenstein 2013). Historians have also tried to understand the rabbis as a “network”, although without applying the quantitative tools now available (Hezser (1997); Lapin (2012)).In this part of the project, we focused our attention on citation chains. Rabbis frequently say things in the name of other rabbis (e.g., “Rabbi X said in the name of Rabbi Y who said in the name of Rabbi Z” – these chains usually consist of two or three names but can go up to nine!). By focusing on simply the names in these chains (and not the content of what they reported), our work intersects with that of Zhitomirsky-Geffet and Prebor (2018) and Josh Waxman (2019). At the same time, both our workflow and the kinds of questions that we were asking of the network as a whole make it distinct.The first step in our workflow was to identify each instance of a citation chain in the Hebrew/Aramaic text. In order to do this, we compiled a list of the names of all rabbis mentioned in ancient rabbinic literature (along with any aliases that they had) and assigned each a unique numeric identifier. The list was created through both automated and manual processes. Then, we created and ran a pattern matching program on a digital version of the “standard” printed edition (Vilna) of the Babylonian Talmud text to identify instances of rabbinic names and citation chains. Once identified, the program split the citations into “source” and “target” rabbis so we could identify who was citing whom. The results of the automated process were highly accurate as we verified through manual review of a statistically significant sample.The program identified 5,245 citation instances. When grouped into unique interactions (e.g., Rabbi X may cite Rabbi Y twenty times, but we counted that as one unique interaction), we were left with 630 rabbis (our nodes) and 1217 unique interactions (our edges). We loaded our node and edges files into Gephi (Gephi) and UCINET. A visualization can be seen in Figure 1, which (using a Force Atlas 2 layout) groups the more connected rabbis toward the center.Figure 1: Graph of All Rabbis in Babylonian Talmud Who Appear in Citation ChainsWe have two major research findings. First, and less surprisingly, when separated into Modularity Classes through an unsupervised algorithm, the rabbis relatively cleanly separated into groups that clustered around rabbinic figures who themselves had many connections, which looks like a “school” structure (see Figure 2). Previous research has led us to expect this. Second, and more surprisingly, the rabbis at the centers of each of those circles were themselves densely and directly connected to each other. These ten or fifteen rabbis, over four centuries and two geographical locales, served as the backbone for the rabbinic network. It is still unclear to us whether these connections represent real social interactions or can better be explained as the result of later editing and redactional decisions and conventions.Figure 2: Rabbis in Citation Chains in Babylonian Talmud in Modularity ClassesThere are problems inherent in this data. The rabbis are themselves sometimes unsure about an attribution (and explicitly argue about it). Since rabbis sometimes shared names or nicknames, it is sometimes impossible to match with certainty a name with a distinct individual; in such cases we assigned the shared name to the more prominent rabbi. Moreover, we are just using one, easily available text. Manuscripts sometimes record these attributions differently. We feel that given the macro approach we took to this network, these problems become less significant. Nevertheless, they need to be better taken into account in future, more fine-grained analyses.By the time this is published, we should have our data and code freely available on Github. It may take us longer to develop a public-facing interface, perhaps along the lines of “The Six Degrees of Francis Bacon.” We will also extend our approach to other interactions in the Babylonian Talmud (e.g., when a rabbi asks a question of another rabbi); to other rabbinic texts from this period; and to other manuscript versions of these texts.","michael_satlow@brown.edu, mike.sperling@rocketmail.com",Lightning
"Bowker, Lynne","University of Ottawa, Canada",Improving machine translation literacy to facilitate and enhance scholarly communication,Improving machine translation literacy to facilitate and enhance scholarly communication,digital literacy; machine translation; machine translation literacy; human-computer-interaction; scholarly communication,"English, North America, Contemporary, artificial intelligence and machine learning, curricular and pedagogical development and analysis, Language acquisition, Translation studies",English,North America,Contemporary,"artificial intelligence and machine learning, curricular and pedagogical development and analysis","Language acquisition, Translation studies","English is the main language of scholarly communication, but, most researchers are not native English speakers. Contemporary machine translation approaches such as neural machine translation (NMT) are data-driven and use artificial-intelligence-based machine learning techniques; however, such tools rarely produce high quality output of specialized text without human intervention. There is an emerging need for machine translation (MT) literacy among non-Anglpohone students and faculty who must both read and write in English in order to participate fully in the scholarly communication process. We designed and pilot tested a machine translation literacy workshop to help researchers use MT more effectively for scholarly tasks such as: 1) search and discovery of scholarly texts; 2) reading and evaluating scholarly texts; 3) research communication in international teams; and 4) writing for scholarly publishing. Pre- and post-workshop surveys were used to evaluate the success of the workshop and recommend improvements for future iterations.",lbowker@uottawa.ca,Poster
"Barbot, Laure (1); Dombrowski, Quinn (2); Fischer, Frank (3); Rockwell, Geoffrey (4); Spiro, Lisa (5)","1: DARIAH; 2: Stanford University, United States of America; 3: Higher School of Economics, Moscow, Russia; 4: University of Alberta, Canada; 5: Rice University, United States of America",Who needs tool directories? A forum on sustaining discovery portals large and small,Who needs tool directories? A forum on sustaining discovery portals large and small,"directories, discovery, infrastructure, tools, sustainability","Global, English, Contemporary, digital research infrastructures development and analysis, meta-criticism (reflections on digital humanities and humanities computing), Humanities computing",English,Global,Contemporary,"digital research infrastructures development and analysis, meta-criticism (reflections on digital humanities and humanities computing)",Humanities computing,"Digital humanists broadly agree that tool directories are a good and valuable thing, worth building and maintaining, but there is no sustainability model. This forum aims to move beyond platitudes and interrogate the value of directories and possible models for sustaining them. For whom are tool directories valuable, and in what context? Tool directories require ongoing attention in order to remain relevant -- and more technically sophisticated directories face infrastructure maintenance costs as long as the directory remains online and functional. Some directories have adopted a crowdsourcing model to address content updates, translation, and other necessary functions once grant funding runs out. As unpaid labor increasingly becomes an area of attention and concern for digital humanities, one is left to ask, how ethical are directories, particularly when this volunteer labor is at particular risk of being lost through fragile infrastructure? At a certain point, time and funding are a zero-sum game: are directories actually worth it?This forum is organized by individuals representing a range of DH directories, spanning from 2002 (Geoffrey Rockwell’s TAPoR) to 2020 (Frank Fischer and Laure Barbot representing the Marketplace developed within the SSHOC project), along with the defunct DiRT (Lisa Spiro and Quinn Dombrowski). We will share in advance a brief white paper with case studies and provocative questions, and elicit discussion via Twitter and a mailing list. For the forum itself, we will briefly introduce the context for the discussion to accommodate participants who have not read the white paper (15 minutes). We will then organize the forum attendees into breakout discussion groups, each focused on 1–2 agreed-upon questions (15 minutes). These breakout discussions will last 30 minutes, and each group will have one participant report back to the larger group, which will lead into a general discussion (20 minutes). We will close the forum with a short synthesis and ask participants to suggest next steps (10 minutes).The forum will help the organizers grapple with the difficult decisions that fall out from the “directory paradox”, where the DH community’s praise of directories is wildly incommensurate with the interest or resources available for sustaining them. Similarly, we hope it will help others running directories at various scales (including in forms such as Libguides or lists on GitHub pages) return to their projects with a clearer sense of what they’re doing, for whom, why, and for how long. Questions: How should we review tools? Validity of tools for designers’ stated purpose. Usability in classroom. Applicability to certain research questions. Who are directories useful for? And to do what? Are directories the most cost-efficient way to do these things? Who should be responsible for creating and maintaining directories?What would a directory look like that you personally would be willing and able to contribute to? Does that even exist? How can they be sustained when the grant runs out? How can students use them? Do we need standardized ontologies? How can we involve volunteers? What are the different models for tool directories? Why shouldn’t we just let Google do it?","laure.barbot@dariah.eu, qad@stanford.edu, frank.fischer@dariah.eu, grockwel@ualberta.ca, lspiro@rice.edu",Forum
"Fischer, Frank (1); Busch, Anna (2); Heyden, Linda-Rabea (3,4); Schwindt, Mark (5)","1: Higher School of Economics, Moscow; 2: University of Potsdam; 3: Wikimedia Deutschland e.V.; 4: University of Jena; 5: Ruhr University Bochum",Faust Times Eighteen: A Network Analysis of Theatre Plays Around the Myth of Faust,Faust Times Eighteen: A Network Analysis of Theatre Plays Around the Myth of Faust,"Faust, drama, plays, literature","Europe, English, 18th Century, 19th Century, 20th Century, network analysis and graphs theory and application, Literary studies",English,Europe,"18th Century, 19th Century, 20th Century",network analysis and graphs theory and application,Literary studies,"Alongside the comparative network analysis of larger literary corpora (Algee-Hewitt 2017, Trilcke/Fischer 2018), there has recently been a trend towards focusing on author-centred subcorpora, such as the oeuvres of Jane Austen (Wade 2017) or Anton Chekhov (Faynberg et al. 2018).Instead of picking out individual authors, we can instead focus on productive literary topoi instead and examine them with the means of network analysis, something that has not been undertaken yet as far as we can see. The abundance of dramas revolving around the Faust myth will serve as an example.The German Drama Corpus (https://dracor.org/ger) currently holds 18 TEI-encoded plays that centre around a Faust character. They range from early plays like Weidmann's ""Johann Faust"" (1775) to the two parts of Goethe's ""Faust"" (1808, 1832) and Friedrich Theodor Vischer's ""Faust, part III"" (1862), but also feature mash-ups like Grabbe's ""Don Juan und Faust"" (1829) and a version with a female Faust character, Wilhelm Schäfer's ""Faustine"" (1898).Network analysis enables us to take a comparative macroscopic look at the different structures of these plays. For example, the roles of the devil/sub-devil Mephistopheles and the famulus Wagner can be examined more closely, shedding new light on the structural development of the sujet. It will become clear when and where these characters appear – but also where they are missing. Other types of characters also come to the fore, such as Gretchen or even Faust's parents, who play no role in Goethe's version but who do appear in other plays.Our poster first offers network visualisations that concentrate on the centres, i.e., the constellation directly around the Faust character, especially in the larger plays (Soden's ""Doctor Faust"" has 62 characters, Julius von Voss's ""Faust"" has 72 characters, Avenarius' ""Faust"" has 95 characters, Goethe's first part of ""Faust"" has 115 and the second part even 189 distinguishable characters/voices). The visualisations are supported by statistical network measures, such as the Betweenness Centrality, Degree and Weighted Degree. In addition to visual evidence, these measures can also address the positioning of the characters surrounding Faust in a new way.The poster will demonstrate how network analytical and quantitative aspects complement existing literary research on the topic (for an overview of the wealth of literary works around Faust cf. Hucke 1992).","frank.fischer@dariah.eu, annabusch@uni-potsdam.de, heyden.linda@gmail.com, info@markschwindt.com",Poster
"Weenink, Maartje","Manchester Metropolitan University, United Kingdom",Who's Afraid of the Big Bad ?: Researching Trends in the Early Gothic Novel using Word Embeddings,Who's Afraid of the Big Bad ?: Researching Trends in the Early Gothic Novel using Word Embeddings,"word embeddings, gothic, genre formation, sentiment analysis","Europe, English, 18th Century, 19th Century, database creation, management, and analysis, natural language processing, Linguistics, Literary studies",English,Europe,"18th Century, 19th Century","database creation, management, and analysis, natural language processing","Linguistics, Literary studies","This poster will present the preliminary results of my PhD project which researches trends in the early Gothic novel using computational methods. A corpus of over 2500 British early Gothic texts has been created and explored using a combination of the analysis of word embeddings and sub-sections of the corpus defined by annotated meta-data. Variations in the embeddings for such sub-corpora demonstrate that various established theories such as the assumed British preoccupation with European national identity in Gothic fiction, the tendency to categorise all Gothic novels as filled with negative sentiment, or the ambiguously defined 'female Gothic', warrant re-evaluation and further exploration assisted by quantitative methods. Notable changes in embeddings for specific datasets such those comprised of texts written at the onset of the Gothic's popularity, or by female authors, are visualised in this poster.",maartje.weenink@stu.mmu.ac.uk,Poster
"Ciotti, Fabio","Università di Roma Tor Vergata, Italy","Theoretical intersections: cognitive poetics, cultural evolution, and distant reading in literary studies","Theoretical intersections: cognitive poetics, cultural evolution, and distant reading in literary studies","distant reading, cognitive poetics, computational literary studies, cultural evolution, cultural analytics","Europe, English, 20th Century, Contemporary, text encoding and markup language creation, deployment, and analysis, text mining and analysis, Cognitive sciences and psychology, Literary studies",English,Europe,"20th Century, Contemporary","text encoding and markup language creation, deployment, and analysis, text mining and analysis","Cognitive sciences and psychology, Literary studies","Since Franco Moretti coined the widely successful term “distant reading” (Moretti, 2000) quantitative/computational text analysis methods have gained a wide circulation in literary studies. We can even speak of a distant reading school, nowadays. The diffusion of distant reading approaches has raised a lively debate (mostly in the North American context), and has attracted various criticisms, both from “traditional literary scholars” and self-critical adopters that can be subsumed into a threefold typology:Theoretical/Ideological: the intentional and qualitative nature of the literary domain is in principle irreducible to quantitative and computational methods; literature is not data (Marche, 2012) and literary criticism is not data analytics (Fish, 2012);Methodological: the (statistical/computational) models and methods adopted for literary analysis are wrong, inaccurate, and ultimately inadequate (Da, 2019);Pragmatical: the limits in the representativeness of the textual data set used in the analysis and the problems in defining the adequacy of its selection criteria (Bode, 2018).Each of these kinds of criticisms would need a deep discussion and are strictly interconnected. For instance, in the well-known Da’s articles, allegedly oriented presented as a replication failure study, the following excerpts makes apparent that the author has an a priori skepticism about the epistemological possibility of what she calls “computational literary studies:There is a fundamental mismatch between the statistical tools that are used and the objects to which they are applied. (Da, 2019: 601)[…] It may be the case that computational textual analysis has a threshold of optimal utility, and literature—in particular, reading literature well—is that cut-off point. (2019: 639)I think that one of the main reasons underlying these more or less critical positions toward distant reading is the fact that it lacks sound and coherent rationales from the point of view of the theory: in fact, we can say that distant reading is the first methodology in literary studies that does not come with a theory of literature embedded in it, as it was for all of its predecessors. Consequently, all distant reading studies derive their theoretical frameworks and terms from theories in the literary domain that generally relies on the fundamental idea that literary texts can be explained only by the way of interpretation or if we prefer of hermeneutics.The problem is that any literary interpretation based on quantitative, immanent, and purely formalist approach is subject to the theoretical criticism that was expressed by Stanley Fish in his harsh and seemingly ultimate criticism to stylistics in “What Is Stylistics, and Why Are They Saying Such Terrible Things About It?” (Fish, 1980). The point for Fish was not to criticize the methods per se, but the possibility to extract meaningful literary interpretations directly from the simple linguistic facts, the idea of an “algorithmic interpretation” (Fish’s words!), since interpretation always starts from a contextual and situated point of view that pre-defines the very objects of its actuation.Many important scholars active in the field do believe that there is the possibility to reconcile traditional theories (of literature) with computational/quantitative methods. Just to make a couple of examples of these consilience theses, we can cite Andrew Piper (Piper, 2018) and Michael Gavin (Gavin, 2018). Piper proposes a sort of computational hermeneutics, that integrates distant and close, and quantitative and qualitative readings. Michael Gavin argues that “vector semantics share a set of assumptions with literary critic William Empson, who devoted his career to explaining how poets played with words’ many meanings”. What is suspicious in these (and similar but often less intriguing and thought-provoking) calls to the reconciliation of the two poles is that their outcomes are either literary critically unsatisfactory or are self-contradictory, in that the hermeneutical and critical part of the discourse is self-standing, the critical arguments are logically independent from the results of the computational analysis.On the base of these considerations, I think that distant reading simply cannot be considered a methodological innovation to be applied to our pre-existing theories of literary texts (in all their rhizomatic variants): it is necessary to find a suitable theory or framework where these methods can yield to interesting results. To make a substantial step in this direction we should first of all take seriously the notion of distant reading and abandon the idea of literature as either made of singular special individuals (the great or the small texts, amenable to interpretations by literary critics, or the big or small writers) or reduced to abstract ideal type, under the scrutiny of literary theoreticians with no clue with empirical evidence. This move would import also taking seriously the move from (or renounce to) interpretation to (embrace) explanation as the real aim of the scholarly inquiry.On what theoretical basis, then, can we construct a notion of literature amenable to distant reading methods?One possible direction to be explored as some scholars like Ted Underwood suggests (English and Underwood, 2016; Underwood, 2017), is that distant reading should fall inside the tradition of sociology of literature or history of ideas a la Nouvelle Histoire. Although there are many reasons to lean toward a sociological vision of literature as an optimal base for distant reading, I think that an even better theoretical framework is that of the cognitive and bio-evolutionistic approaches to literature and the cultural evolution studies. Cognitive poetics/narratology, and bio/evolutionary literary studies have been two of the most interesting waves of innovation in the literary field of the last 30 years and are now established field of inquiry. With different graduation depending on the authors, they have advocated the introduction of a more scientific methodology in the study of literature, looking for methodological and theoretical insights into cognitive science and evolutionary psychology.What is more interesting for my thesis is that, not surprisingly, the debate around the legitimacy and acceptability of the cognitive approaches in literary studies has determined a discussion on the problem of literary interpretation that has many similarities with the discourse I have proposed in this paper. Recently, a young and brilliant scholar active in the field, Marco Caracciolo, has re-opened the debate, and in doing this he has explicitly stated that “In order to contribute to cognitive science, literary scholarship has to complement—and in some cases even supplant—interpretation with a different set of goals and methods”. (Caracciolo, 2016: 193).The other scientific field where literary studies can find a theoretical framework that takes great advantage of distant reading methodology is that of cultural evolution. This field of study that as of now has no application in literary studies, aims at providing a naturalist and empirical explanation of the nature and evolution of culture, adopting widely mathematical/statistical and computational modeling.One of the theoretical underpinnings of cultural evolution is the adoption of the population thinking framework, taken from evolutionary biology (after Ernest Mayr interpretation of Darwin’s theory) and population genetics, and its application to cultural phenomena as pointed out recently by Dan Sperber and his collaborators (Claidière et al., 2014):Literature is part of the cultural sphere, so it can be considered a population of individual items (the texts) whose members are defined by a set of measurable features. The description of the population at a given state (synchronic, in our beloved Sausurrian terms) and its evolution (diachronic) is feasible by the way of statistical and data-driven analysis.To conclude, I think that in order to take full advantage of the most advanced methods and analytical techniques encompassed by the label Distant reading, like text mining and machine learning, in literary and cultural studies, we need to find a proper theoretical framework that gives sense to the hypothesis experiments, data sets and explanations we can generate. The attempt to justify and anchor this approach in the context of the traditional literary theories and methodologies has proven a limitation that undermines the interesting analytical results, and it is easily amenable to the ‘so what’ criticism, or ideological attacks. Maybe it is time to change the framework, and to abandon the classical hermeneutical literary studies environment.",fabio.ciotti@uniroma2.it,Short Presentation
"Satlow, Michael; Mylonas, Elli","Brown University, United States of America",Inscriptions of Israel/Palestine,Inscriptions of Israel/Palestine,"epidoc, inscriptions, non-roman alphabets, epigraphy","Asia, English, BCE-4th Century, 5th-14th Century, scholarly editing and editions development, analysis, and methods, text encoding and markup language creation, deployment, and analysis, History, Theology and religious studies",English,Asia,"BCE-4th Century, 5th-14th Century","scholarly editing and editions development, analysis, and methods, text encoding and markup language creation, deployment, and analysis","History, Theology and religious studies","The “Inscriptions of Israel/Palestine” (IIP) project (www.brown.edu/iip) seeks to create a corpus of inscriptions (texts written on durable materials, other than coins) from the geographical location of present-day Israel/Palestine, that date from around the sixth century BCE to the seventh century CE. The inscriptions are in Greek, Latin, Hebrew, and Aramaic. The purpose of the project is not only to allow for access and robust (and ultimately, federated) searching but also for scholarly analyses. As one of the longest running active digital epigraphy projects (with over 4,000 inscriptions entered to date), IIP provides several use cases of working with a complex and challenging multi-lingual corpus. This abstract will focus on our data modeling and approach to Linked Open Data.IIP was an early adopter of the Epidoc schema, a customization of TEI developed especially for those working with ancient texts preserved on durable materials, such as inscriptions and coins (Elliott, Bodard, Cayless). Many users of Epidoc are in contact with each other through epigraphy.info, and the schema is continually being modified in response to user requests. The general principle, however, is that each material object on which a text is inscribed or written is treated as a discrete XML file. IIP thus gives each inscription a unique, findable ID that also serves as its document name (e.g., ash0001.xml). Each file has an extensive teiHeader, in which we encode the metadata, where possible using controlled vocabularies as attributes of elements, linked to authority files. This kind of robust encoding allows for the database-style, faceted indexing and searching powered by SOLR that we provide through our interface. We also include images (although we have much more work to do collecting them) and geographical information. We have a geographical interface that allows for mapping (see Figure 1).Figure 1: Screenshot of Search Page of ""Inscriptions of Israel/Palestine""Our data has always been open. All of our XML files can be seen and downloaded individually directly from our site, or downloaded in bulk from our open Github site (https://github.com/Brown-University-Library/iip-texts) or through our API (for which we give detailed instructions on the site itself). We also encode our permission license (CC BY-NC 4.0) into our files.Since participating in the conference, “The Big Ancient Mediterranean” (BAM) we have sought to create links in our data in three ways. The primary geographical data within each inscription is linked to its corresponding Pleiades Gazetteer id (Pleiades). The primary chronological data within each inscription is linked to its corresponding PeriodO id (PeriodO).   And each of the types of object upon which the inscription is written is linked to the Getty Art and Architecture Thesaurus (Getty). For example:                        <origin>                     <date period=""http://n2t.net/ark:/99152/p0m63njbxb9""                           notBefore=""0001""                           notAfter=""0100"">First century CE</date>                     <placeName>                                <region>Judaea</region>                                <settlement ref = ""http://pleiades.stoa.org/places/687928""> Jerusalem</settlement>                                <geogName type=""site"">Akeldama Caves</geogName>                                <geogFeat type=""locus"">Cave 2 Chamber B</geogFeat>                            </placeName>                                                      <p/>                        </origin>The use of these ids allows our data to be scraped live by different projects, such as Pleiades and the Pelagios Network (Pelagios Network). The community is just now beginning to develop an ecosystem that allows for the fruitful exchange of data between sites using Linked Open Data and we hope that through this expansion the usefulness of our data will expand.We added these links to existing data, which was a costly process. We first developed an XSLT script to extract the place, time, and object values into a spreadsheet. We then manually found and added the links, in the process having to submit new geographical and chronological values for inclusion in the other authorities (and then, in return, adding the new id numbers). We then used another XSLT script to insert the new links into the XML files. Along the way there was a great deal of checking and testing. For new data files, the additions are added at the time of creation.One of our active projects involves the lexicographical tagging of the texts in a way that could similarly be linked and shared. This entails performing word segmentation on the existing XML files, and then assigning part of speech tags using natural language parsing. This will enable new interface features and better forms of analysis. The Global Philology Project is an exploratory project that began to lay the infrastructure for compiling and analyzing lexicographical data in many different languages across multiple sites (Global Philology). We want to further explore how our tagging of individual words could make our data – on the level of the individual words of our texts – accessible and more useful to researchers in different fields.For a broader description of IIP and our goals, see Satlow (forthcoming) and Lembi (forthcoming). We describe our approach to bibliographical management at Lembi, Mylonas and Satlow (2016) and our approach to the FAIR principles (FAIR) in Mylonas, Lembi, Creamer, and Satlow.","michael_satlow@brown.edu, elli_mylonas@brown.edu",Short Presentation
"Siemens, Lynne",University of Victoria,University-Industry Partnerships in the Humanities: View from the partner and academic perspective,University-Industry Partnerships in the Humanities: View from the partner and academic perspective,"collaboration, academic teams, project management","English, North America, Contemporary, project design, organization, management, Humanities computing",English,North America,Contemporary,"project design, organization, management",Humanities computing,"While university-industry partnerships are common in the sciences, they are more rare on the humanities side. This provides an opportunity to explore one such collaboration based in the humanities. In this case, exploring open scholarship, the partnership involves libraries, academic-adjacent organizations and academics. Through a series of interviews, these parties express that they find the experience positive and see both benefits and challenges, albeit from different perspectives. For example, libraries and academic-adjacent organizations focus on learning while the academics are interested in moving research to production. The challenges include a focus on cultural differences and the partners' ability to navigate these. In terms of measures of success and desired outcomes, they are both in agreement that these measures and outcomes are soft in nature, though focused on influencing government policy on social scholarship. The partnership members will continue to invest time, resources, and intellectual capacity to the endeavor.",siemensl@uvic.ca,Lightning
"Houston, Natalie M. (1); Plecháč, Petr (2); Ruiz Fabo, Pablo (3); Bermúdez Sabel, Helena (4); Birnbaum, David (5); Thorsen, Elise (6)","1: University of Massachusetts Lowell; 2: Institute of Czech Literature, Czech Academy of Sciences; 3: LiLPa, Université de Strasbourg; 4: Université de Lausanne; 5: University of Pittsburgh; 6: Novetta",Understanding Rhyme Through Network Analysis,Understanding Rhyme Through Network Analysis,"rhyme, network analysis, machine learning, stylometry, intertextuality","South America, Europe, English, North America, 15th-17th Century, 19th Century, 20th Century, network analysis and graphs theory and application, text mining and analysis, Linguistics, Literary studies",English,"South America, Europe, North America","15th-17th Century, 19th Century, 20th Century","network analysis and graphs theory and application, text mining and analysis","Linguistics, Literary studies","Understanding Rhyme Through Network AnalysisRhyme enacts numerous relationships in poetic texts: relationships between words that share similar sounds; relationships between lines of verse that end in rhyming words; and relationships between the sound and semantic meaning of words that are linked together through rhyme. This panel brings together different approaches to using network analysis to understand the relationships that rhyme enacts in poetry in different language traditions. As the papers in this panel suggest, as one moves from considering rhyme’s function within a single poem to examining larger datasets, one can also consider how rhyme connects words, documents, and/or authors within a corpus. By juxtaposing papers focusing on poetry in Czech, English, Russian, and Spanish, this panel highlights the fact that rhyme is defined differently in different linguistic and poetic contexts, due to the levels of inflection present in different languages and to the development of different poetic traditions. Some rhyme definitions focus on the shared stressed vowel of single syllables, others encompass polysyllabic rhyme, and others focus on component phonemes, rather than whole words. Thus computational approaches to rhyme must be tailored to the particular languages of the texts under study. The papers on this panel use different network analysis and graph visualization methods to examine rhyme at the level of the corpus or dataset, rather than the individual poem, in order to understand how rhyme practice changes over time, across languages, and in relation to literary canon formation. It thus contributes both to computational poetics and distant reading methodologies within the digital humanities.   Distant Reading Nineteenth-Century British Poetry With Rhyme NetworksNatalie M. HoustonUniversity of Massachusetts Lowell1. IntroductionThe expectations and assumptions that nineteenth-century English readers brought to their reading of poetry was necessarily different from that which readers today bring to the same texts. One feature of that historical difference was their familiarity with poetic rhyme, and the assumptions about poetic language that it created. By examining rhyme words and sounds in a large dataset of English poems, we can better understand how rhyme shaped poetic discourse in the nineteenth century. This paper suggests that network analysis methods are useful for understanding the semantic networks created by the relatively limited set of rhymes available in English; for examining chronological and aesthetic explanations for changes in rhyme practice; and for exploring the relationships between poems that use the same rhyme pairs. Such analyses reveal the semantic and sonic features of conventional nineteenth-century poetry, and can thereby also distinguish unconventional or distinctive uses of rhyme.2. Context The vast majority of English poems published in the nineteenth century were rhymed (95% of the 108,842 poems in the Chadwyck-Healey English Poetry corpus used for this study). Both poets and readers thus expected poetry to be rhymed (McDonald 2012, 7) and many of the rhyme sounds and rhyme words used in nineteenth-century poetry were so frequently used as to create a set of implicit conventions of poetic discourse. Uncovering such implicit conventions can help reveal the structures of the field of poetry at the large scale (Bourdieu 1993).3. MethodThis project identifies rhyme words, groups, and syllables through a method that operationalizes the historical rules for rhyme found in nineteenth-century British rhyme dictionaries, in order to match words according to historical pronunciation and poetics (Houston 2016, 2019). Three different kinds of network analyses are then performed: a rhyme word co-occurrence network, a rhyme pair co-printing network, and a textual coupling network (Houston 2017).4. Rhyme Networks Because rhyme word frequency in British poetry follows a power law distribution, in which a small number of rhyme words are very frequently used, followed by a long tail of additional words, the rhyme word co-occurrence network can reveal the relationship between those frequencies and the clusters of rhyme words that are most likely to occur within the same poem. The rhyme pair co-printing network, based on co-citation analysis, links specific rhyme pairs if they appear in the same poem. Together these two networks reveal the semantic and sonic patterns that structured nineteenth-century poetic discourse. The textual coupling network, based on bibliographic coupling, links two poems if they use the same rhyme pairs. This network reveals chronological and aesthetic subgroups, suggesting how rhyme practice changed through the century. Works Cited    Bourdieu, P. (1993). “The Field of Cultural Production, or: The Economic World Reversed.” In The Field of Cultural Production: Essays on Art and Literature. Ed. Randal Johnson. New York: Columbia University Press. 29-73.      Houston, N. (2019). “An Evaluation of Rhyme Detection Using Historical Dictionaries.” Digital Humanities 2019, Utrecht University, Netherlands.   Houston, N. (2016). “Exploring the Rules of Rhyme: Operationalizing Historical Poetics.” Digital Humanities 2016, Krakow, Poland.   Houston, N. (2017). “Measuring Canonicity: a Network Analysis Approach to Poetry Anthologies.” Digital Humanities 2017, McGill University, Montreal Canada.   Jauss, H.R. (1970). “Literary History as a Challenge to Literary Theory.” Trans. Elizabeth Benzinger. New Literary History 2.1: 7-37.    McDonald, P. (2012). Sound Intentions: the Workings of Rhyme in Nineteenth-Century Poetry. Oxford: Oxford University Press.Recurrences of rhymes in 19th century Czech poetry as compared to English, German, Spanish, and RussianPetr PlecháčInstitute of Czech Literature, Czech Academy of SciencesIn any large enough body of poetic texts rhyme pairs inevitably reoccur (cf. Reddy–Knight 2011). The talk employs network analysis in order to explore to what extent rhyme pairs are shared across works by different authors in nineteenth-century Czech poetry. We show that the degree of recurrence is (1) comparable to Russian poetry, (2) noticeably weaker as compared to German and Spanish poetry and (3) strikingly weaker as compared to English poetry. Following the hypothesis formulated in Plecháč 2018, we offer a linguistic explanation for such findings: the size of rhyme repertory in a given language depends upon its inflection, the changes in word forms used to mark grammatical aspects such as voice, case, tense, mood, number, or gender. Roughly speaking—the more suffixes the language employs, the richer rhyme repertory gets as completely different words may rhyme only due to being followed by the same grammatical endings.As one may expect, we find that in all the languages examined the tendency to share rhyme pairs is stronger between works that come roughly from the same period than between those distant in time. We thus try to represent each work as a vector defined by the relative frequencies of rhyme pairs and use machine learning techniques (Random Forest, Support Vector Machine) in order to classify them into time periods or poetic movements. Cross-validations of the models show that depending on what classes are being used, the accuracy varies between 0.6 to 0.9 and always outperforms the random baseline.The datasets come from: ∙   The Corpus of Czech Verse (Plecháč, P. – Kolár, R. 2015)∙   The Gutenberg English Poetry Corpus (Jacobs 2018)∙   Metricalizer corpus of German poetry (Bobenhausen–Hammerich 2015), ∙   Corpus of Spanish Golden-Age Sonnets (Navarro-Colorado 2017) & Diachronic Spanish Sonnet Corpus (Ruiz Fabo et al. 2017)∙   Russian National Corpus (http://www.rusropora.ru) References   Bobenhausen, K. – Hammerich, B. (2015). Métrique littéraire, métrique linguistique et métrique algorithmique de l'allemand mises en jeu dans le programme Metricalizer. Langages 199, 67–87.      Jacobs, A. M. (2018). The Gutenberg English Poetry Corpus: Exemplary quantitative narrative analyses. Frontiers in Digital Humanities 5:5.   Navarro-Colorado, B. (2017). A metrical scansion system for fixed-metre Spanish poetry. Digital Scholarship in the Humanities 33(1), 112–127.   Plecháč, P. – Kolár, R. (2015). The Corpus of Czech Verse. Studia Metrica et Poetica 2(1), 107–118.   Plecháč, P. (2018). A collocation-driven method of discovering rhymes (in Czech, English, and French poetry). In M. Fidler – V. Cvrček (eds.), Taming the Corpus. From Inflection and Lexis to Interpretation. Cham: Springer, 79–95.   Reddy, S. – Knight, K. (2011). Unsupervised discovery of rhyme schemes. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics. ACL, Portland, 77–82.   Ruiz Fabo, P. – Bermúdez Sabel, H. – Martínez Cantón, C. – Calvo Tello, J. (2017). Diachronic Spanish Sonnet Corpus (DISCO). Madrid, UNED. https://zenodo.org/record/1069844 Network analysis and Russian rhymeDavid J. BirnbaumElise ThorsenConventions about the nature of phonetic similarity in Russian rhyme have varied over time. At the end of the explosively innovative “Silver age of Russian literature”, Viktor Žirmunskij (1923) and Valerij Brjusov (1923) both recognized a prevailing tendency in poetic composition of the time to test the limits of rhyme, but with subtle differences: either as a shedding of an artificial apparatus to the bare essentials of rhyme or as an elaboration on the rule set defining the rhyme zone. Žirmunskij’s and Brjusov’s approaches to rhyme were formal, but not quantitative, nor, of course, computational. Mixail Gasparov (1984) later argued that rhyme is often contingent on outside conditions, including prosodic innovations of one era becoming marked archaisms to avoid in another. Gasparov’s approach was explicitly quantitative, although also not computational. As in other dimensions of his quantitative studies of poetry, Gasparov produced a well-defined set of categories describing common deviations from exact phonetic repetition, and his pioneering work informs our computational exploration of imperfect end-rhyme.               Rhyme studies often operate at a corpus level, constructing large sets of rhyme pairs or nests that serve as training material for supervised machine-learning approaches to recognizing rhyme (see, for example, Plecháč 2018). This type of approach yields structured data that can be used for varied explorations of rhyme in verse, including network visualization, but a force-directed graph of word rhyme in a large corpus quickly becomes illegibly cluttered. Perhaps more importantly, where the object of study is the fluidity of rhyme over time, as is the case for us, an inventory of rhymes over a large corpus elides the instability and contingency of rhyming practice. Network graphs of word rhyme in individual poems (e.g., Lombardi 2013) are well suited to the visualization of rhyme, including imperfect rhyme, within a coherent domain, but because the number of rhyming words in a single poem is small, it is difficult to compare the network graphs of poems in a way that benefits meaningfully from network visualization.Our approach to rhyme, and especially our focus on the fluid conventions of imperfect rhyme, relies on structural levels smaller than the word, such as phonetic segments and phonetic distinctive features (Birnbaum and Thorsen 2017, 2018, and 2019), and these close perspectives can be seen as complementing word-level approaches to rhyme. Segments and distinctive features lack lexical semantics, and therefore do not provide access to the types of meaning that are encoded or created by rhyme associations. But because the inventory of segments and distinctive features is (compared to lists of rhyming words) relatively small and relatively stable, they create a means of profiling rhyme that can be visualized with less clutter than network graphs of words, and that can be compared more easily. Our contribution to this panel, then, explores the opportunity for the graphic visualization of network rhyme relations below the word level and in the context of the mutability of rhyme conventions over the history of Russian verse.Works cited   Birnbaum, David J. and Elise Thorsen. 2017. “Exploring inexact rhyme in Russian verse.” Presented at “Plotting poetry Machiner la poésie”, 5–7 October 2017, Universität Basel.   Birnbaum, David J. and Elise Thorsen. 2018. “The automatic detection of Russian rhyme.” Presented at “Plotting poetry 2: bringing deep learning to computational poetry analysis”, 12–14 September 2018, Freie Universität Berlin.   Birnbaum, David J. and Elise Thorsen. 2019. “Rules-based and machine-learning approaches to identifying Russian rhyme.” Presented at “Plotting poetry (and poetics) 3”, 26–27 September 2019, ATILF, Nancy.   Brjusov, Valerij Jakovlevič. 1923. “O rifme.” Review of: Viktor Maksimovič Žirmunskij, Rifma, ee istorija i teorija. Voprosy poètiki (Rossijskij institut istorii iskusstv), vyp. 2. Petrograd: Academia.   Gasparov, Mixail Leonovič. 1984. Očerk istorii russkogo stixa. Metrika, ritmika, rifma, strofika. Moscow: Nauka.    Lombardi, Thomas. 2013. “Network models of rhyme: ‘The raven’.” https://telombardi.wordpress.com/2013/08/08/network-models-of-rhyme-the-raven/   Plecháč, Petr. 2018. “A collocation-driven method of discovering rhymes (in Czech, English, and French poetry).” In M. Fidler, V. Cvrček, eds., Taming the corpus. From inflection and lexis to interpretation. Cham: Springer, 79–95.   Žirmunskij, Viktor Maksimovič. 1923. Rifma—ee istorija i teorija. Petrograd.Rhyme network analysis in a non-canonical corpus of sonnets in SpanishPablo Ruiz and Helena BermúdezRhyme constitutes a structural element for specific poetic forms, like sonnets. Given its relevance in versification, computational studies of rhyme can provide insight about style and intertextuality. We propose an exploration based on network analysis. Seeking representativity, we analyzed a corpus that draws from online anthologies, including non-canonical authors. We discuss challenges involved with this material and how networks can help analyze it.Previous work employing network analysis of rhyme includes Houston (2017) for Victorian poetry, List (2016) for Old Chinese, Osinova (2016) for Russian, or Sonderegger (2011) for English poetry ca. 1900. Network nodes in these works are mainly rhyme-words, with edges indicating rhyme-pairs. Our contribution here is creating author networks, where rhyme-pairs are the edges between authors, which can be exploited to assess intertextuality between specific authors.We analyzed the DISCO dataset of sonnets in Spanish (Ruiz et al., 2018), which contains rhyme annotations, besides author metadata for birthplace and period. The corpus sources are online sonnet anthologies (García, 2005; 2006a; 2006b) which cover lesser-known authors besides some canonical ones, from Spain and Latin America, between the 15th and the 19th centuries. The corpus can be found at https://github.com/pruizf/disco/tree/v3Considerable rhyme-pair recurrence across authors was attested. We first discuss findings regarding central authors in the network, then some peripheral authors, and finally how peripheral patterns revealed textual errors in the corpus.The most central author (Fig. 1) is 16th-century Spanish author Juan de Arguijo, patron of a canonical author, Lope de Vega (Asensio, 1883: 26). Arguijo’s centrality is not surprising; early scholars already noted that he had contemporary followers like Rodrigo Caro (Colon i Colon, 1841: 9). While not surprising, Arguijo’s position can be seen as an indication of the network’s sanity. Figure 1. Most central author, Juan de Arguijo, and first neighbours grouped together (central node)Peripheral areas of the network may also show interesting facts. In the top-right area of Fig. 1, two little-known authors strongly connected to each other appear. From discussions in López Cruces (2000: notes 95-96), we deduce that both wrote for a periodical that challenged authors to create sonnets using pre-established unlikely rhyme-words; the network seems to reflect this. To explore rhyme richness, a bipartite network with authors and rhyme-pairs as nodes was created. Non-central areas of this second network revealed data problems. Several disconnected components appeared (Fig. 3). These correspond to poems anthologized twice, but under two different author name variants The transmission of peripheral literature, via non-canonical sources, can pose “noise” problems. Finding these errors via networks highlights how they can help data curation: eccentricities led us to a close reading of some of the authors, then spotting textual problems. In summary, the paper shows how networks based on a heterogeneous, largely non-canonical corpus can confirm trends in it, besides showing interesting peripheral patterns and helping address textual problems inherent to the transmission of lesser-known literature. Future work includes integrating rhyme-data from a canonical sonnet corpus (Navarro-Colorado et al., 2016), for comparison. Also, evaluating rhyme variety per author considering the commonness of rhymes in different subnetworks, to assess originality vs. variety.Figure 2. The bipartite network, filtered to isolate two strongly connected authors, also seen at the top-right of the author network on Fig. 1. Both authors (round nodes) share unusual rhymes (diamond nodes); from the literature we deduce they took part in a writing challenge involving those rhyme-pairsFigure 3. Disconnected components revealed textual problems in the dataset’s sources.","Natalie_Houston@uml.edu, plechac@ucl.cas.cz, ruizfabo@unistra.fr, helena.bermudezsabel@unil.ch, djbpitt@gmail.com, enthorsen@gmail.com",Panel
"Wolff, Mark","Hartwick College, United States of America",Computation and Rhetorical Invention: Finding Things To Say With <em>word2vec</em>,Computation and Rhetorical Invention: Finding Things To Say With word2vec,"rhetoric, invention, word2vec, machine learning","Europe, English, North America, Contemporary, artificial intelligence and machine learning, electronic literature production and analysis, Literacy, composition, and creative writing, Literary studies",English,"Europe, North America",Contemporary,"artificial intelligence and machine learning, electronic literature production and analysis","Literacy, composition, and creative writing, Literary studies","In his recent book Friending the Past, Alan Liu laments the waning of a rhetorical regime that until recently had held sway in literary studies as a means of making sense of the past. Instead of using what Liu names as rhetoric-representation-interpretation to convey an understanding of history, we are now stuck in an ambiguous regime of communication-information-media where it is not clear how we reach an understanding of anything (2-3). The shift from rhetoric-representation-interpretation to communication-information-media is not unique in the history of literary studies, however. It follows another shift that occurred over 100 years ago in how rhetoric was deployed. Gérard Genette observed in 1966 that literary studies had not always emphasized representations. Before the end of the nineteenth century, literary studies revolved around the art of writing. Texts were not objects to interpret but models to imitate: students demonstrated their understanding of literature by mastering elocution and reproducing figures of style in the works they read. With the institution of literary history as a nationalist project at the end of the nineteenth century, academic reading approached texts as objects to be explained according to prescribed methods for documenting how literature represented a national identity. This new way of studying literature stressed disposition, or the arrangement of ideas in the service of ideology.The methods of literary history would eventually be used by literary scholars in the twentieth century to turn narratives about literature away from nationalism toward other priorities, most notably poststructuralism and the critique of cultural hegemonies.Recent developments in information technology have further challenged paradigms for reading literature. Digital tools for text analysis allow for the study of large corpora using quantitative methods. As Ted Underwood, Andrew Piper and others have shown, large-scale computational text analysis has called into question fundamental concepts in literary history such as periodization, nationality, and genre. Using computational methods can enable us to develop models for literary studies, but these models are not limited to interpretation. Computational techniques such as topic modeling and word vector spaces can facilitate investigations into the possibilities for literary creation. Technology has the potential for exploring invention, or the finding of ideas to express through language given a context that can be parameterized.If, in literary studies, an emphasis on elocution or style served the perpetuation of social hierarchies, and if an emphasis on disposition or argumentation challenged these hierarchies by promoting forms of knowledge and ideologies, a new rhetorical emphasis is needed to respond to the ontological condition of the communication-information-media era. We are surrounded by data with no clear way to make sense of it, and we need to explore inventional methods of finding things to say within this state of being. Digital environments today constitute in part the material context for suasive activity, and as Thomas Rickert argues, contemporary rhetoric must attend to how humans and the world are in this context (xv). The affordances of networked access to texts and computational processing contribute to a rhetorical ambience that grants a degree of agency to the environment in what is said about the world, which includes literature.I will consider two examples of how tools for computational literary studies lend themselves to inventional practices. The first is ReRites, a year-long project by David (Jhave) Johnston who used a neural net trained on various corpora to produce poetry (“Why A.I.?”, 172). The raw text of the poetry was generated by computation but Johnston edited the output. In terms that emphasize the materiality of computation, Johnston describes his role as “carving the text.” Neither the computer nor Johnston writes these poems in the sense we usually give to writing: they emerge from the world in which a machine and a human find themselves. After performing complex analyses on very large corpora, the machine produces something the human takes to find something to say with language.The second is SonGenApp, a web application I developed that enables a user to select verses from a large corpus of sonnets to assemble a new poem. From all the verses in the corpus a word embedding is modeled with word2vec, and from the model the user selects verses semantically with an analogy based on a pair of words. The user can modify a selected verse as long as it follows the rules of scansion and rhyme for sonnets. With the application attending to formal constraints, the task of the user is to find verses that are meaningful in some way at the moment of using the application. The user can always read the source texts for selected verses and base the construction of the generated poem on a knowledge of literary themes and history. But this prior knowledge is not necessary. The user can encounter verses in the corpus by changing the analogy as if it were a knob on a black box.The quantity of digital texts at our disposal opens possibilities for discovery in rhetorical invention. Stephen Ramsay has described a “hermeneutics of screwing around” where browsing resources leads serendipitously to the pleasure of finding things one had not anticipated. Computation has the potential to afford the same discovery in finding things to express through writing.",wolff.mark.b@gmail.com,Lightning
"Swanstrom, Elizabeth Anne",University of Utah,Coding Literary Ecologies,Coding Literary Ecologies,"environmental humanities, digital humanities, literary studies, natural language processing, media ecologies","English, North America, 19th Century, 20th Century, Contemporary, eco-criticism and environmental analysis, natural language processing, Literary studies, Environmental, ocean, and waterway studies",English,North America,"19th Century, 20th Century, Contemporary","eco-criticism and environmental analysis, natural language processing","Literary studies, Environmental, ocean, and waterway studies","Literary scholarship tends to treat the natural world as the stage upon which narrative unfolds, rather than the essential pre-condition for existence. “Coding Literary Ecologies” brings the environmental features of literary texts into detailed relief in a playful yet strategic manner. It is comprised of four web-based applications, each accompanied by an essay that situates it within literary history and contextualizes it within contemporary discourse about the digital and environmental humanities.",swanstro@gmail.com,Lightning
"van Wissen, Leon (1); Latronico, Chiara (1); van Ginhoven, Sandra (2); Zamborlini, Veruska (1)",1: University of Amsterdam; 2: Getty Research Institute,The Montias Case: an experiment with data reconciliation and provenance between research and cultural heritage institutions,The Montias Case: an experiment with data reconciliation and provenance between research and cultural heritage institutions,"Reconciliation and Disambiguation of Linked Open Data, Provenance, Named Entity Recognition in Archival Sources, Cooperation public and research institutions, Dutch Golden Age","Europe, English, North America, 15th-17th Century, 18th Century, digital archiving, linked (open) data, Art history, History",English,"Europe, North America","15th-17th Century, 18th Century","digital archiving, linked (open) data","Art history, History","This paper discusses the complex process of reconciliation of data coming from research and public cultural heritage institutions with their own selection criteria that have shaped the provenance of their collections. We demonstrate how the Golden Agents digital humanities research infrastructure [1] can play an intersecting role as intermediary data provider between these distributed collections in the reconciliation, disambiguation and deduplication of data by taking their provenance into account. To this end we analyse an art historical case we call ‘The Montias Case’, with data from three different sources: the Getty Provenance Index, [2] the Frick Collection, [3] and the notarial acts of the Amsterdam City Archives. [4]In the 1980s, John Michael Montias (1928-2005) began to compile a database containing records of ownership of works of art of the Dutch Golden Age. To this end he also selected records from the Amsterdam City Archives using inventories dated 1597-1681. In 1985 and 1987, Montias was invited to the Getty Research Institute to automate his work and incorporate it into the Getty Provenance Index. When his collaboration with the Getty concluded around 1990, Montias continued his project and donated his work to the Frick Art Reference Library. Both datasets were enriched by others. The Getty Provenance Index and the Frick Collection have now two partially overlapping datasets with a shared provenance that are in need of deduplication/disambiguation to fully exploit them for art-historical research. The Getty Research Institute and the Golden Agents consortium set up an experiment in 2019 to reconcile the Montias data with their provenance and enrichments to their original source: the notarial acts. This reconciliation via the Golden Agents infrastructure is necessary because the Amsterdam City Archives due to their public mission only index person and geographical names, but not the art objects that Montias listed that we need as researchers.We first link the inventories from the Getty Provenance Index and the Frick Collection to their corresponding notarial deeds. Additionally, we deduplicate the names indexed in these data sources and identify people utilizing the Lenticular Lenses II tool (Idrissou et al., 2018, 2019) in development within the Golden Agents project. Then we link the individual items indexed in the Montias database to the transcription of the historical archival source. The Web Annotation Model is used as a data model for the text and serves both modelling and presentational purposes. This allows us to link and to validate the exact piece of text indicated by coordinates on the scan where a certain object is listed and to present it for instance in a IIIF-environment. The thesauri AAT and TGN, [5] and ICONCLASS [6] help in describing these items in a standardized way. Finally, we demonstrate how, in line with the goals of the Golden Agents and the remodel of the Getty Provenance Index as Linked Open Data projects, [7] the results of the Montias Case can be relevant for data reconciliation and for modeling the provenance of distributed heterogeneous cultural heritage collections for (re-)use in digital humanities research projects in general.[1] https://www.goldenagents.org[2] https://www.getty.edu/research/tools/provenance/search.html[3] https://research.frick.org/montias/[4] http://archief.amsterdam/archief/5075/[5] https://www.getty.edu/research/tools/vocabularies/[6] http://www.iconclass.org/[7] https://www.getty.edu/research/tools/provenance/provenance_remodel/index.html","l.vanwissen@uva.nl, c.latronico@uva.nl, svanginhoven@getty.edu, v.carrettazamborlini@uva.nl",Lightning
"Antonini, Alessio; Benatti, Francesca; King, Edmund","The Open University, United Kingdom",Restoration and Repurposing of DH Legacy Projects,Restoration and Repurposing of DH Legacy Projects,"Information Systems, Legacy Systems","Global, English, Contemporary, meta-criticism (reflections on digital humanities and humanities computing), sustainable procedures, systems, and methods, Informatics",English,Global,Contemporary,"meta-criticism (reflections on digital humanities and humanities computing), sustainable procedures, systems, and methods",Informatics,"The institutional funding system of Digital Humanities (DH) is usually devoted to the creation of new projects, creating a recurring problem of unsupported legacy projects whose material cost of upkeep depends on the voluntary contributions of institutions and individuals. The lack of resources to invest in “remedial” actions [10] pushes DH projects towards outdatedness. Additional funding success delays this process by introducing extra resources but, simultaneously, it fast-forwards obsolescence by advancing the field.Indeed, the impact of a DH project can be considered as the ability to establish as common knowledge what was once innovative and cutting-edge by fulfilling its research questions. In this scenario, managing successful DH projects requires addressing competing issues related to the preservation of their integrity [1] (i.e. consistency of data, questions and vision) and of their role and purpose (i.e. their use in the field).The management of legacy systems has been widely studied from a technical perspective [2,3,4,5,6,7,8], e.g. cost/value [9], approach to integration [1,9], change of use [1] and archiving [3,4]. Rather than presenting technical solutions, this contribution focuses on the rationale for defining an approach through human, financial and political perspectives [2].The issue of legacy is not one of data formats but principally a cultural one that we analyse from two distinct approaches:The “restoration” approach, implementing remedial actions [10] that “update” the project to new contexts to preserve its function and role (e.g. extending its data structure to address new questions)The repurposing approach [1], implementing actions that rethink the value of the project by finding it new purposes, functions and roles in new contexts (e.g. defining new questions to be addressed with its existing data).At stake in the two approaches are the integrity and identity of the project. A project’s integrity is the logical and historical connection between its origin, output and outcomes. A project’s identity is the meaning or role it has within the community of people involved. In this scenario, we argue that addressing a project’s legacy should take into account:The project’s vision, research questions and target “knowledge gap”; the project creators’ motivations and aims; the funding bodies’ goals and prioritiesThe project’s practices, orchestration of people, organisations and tools, operational limits and constraintsThe project’s knowledge, research data and outputs, correlated research activities, answers produced and outcomes (e.g. new projects, scholarly research, education, impact on the field).The contribution then discusses a real case, the UK Reading Experience Database (RED), as emblematic of the challenges of managing legacy DH projects. RED has had a long history, repeated funding successes and significant visibility in Book History scholarly literature. It was devised by Simon Eliot in 1993, first implemented in 1996, published on the web in 2007 and finally closed to new submissions in 2018 [11]. RED’s vision was to advance research in the history of reading by establishing a new methodology based on empirical evidence [12]. RED’s practice established strong synergies between researchers, students and volunteers for the distributed acquisition and curation of evidence of reading. The RED contribution form’s structured approach to knowledge encouraged data inputters to pay close attention to the contexts and agents involved in the reading experience: who was reading, what was read, and when and where the act of reading took place [13].RED data has been successfully converted from a legacy custom relational database to linked open data. Still, RED is a legacy project because of what its data expresses about reading experiences: a now-outdated vision established more than twenty years ago, which has now become embedded in the DH community through successful activities, follow-on projects, publications, research and training initiatives [14,15,16,17]. RED is both a vast database and the centre a wide network of collaborations; therefore addressing its legacy is not a trivial decision.1. The Repurposing of RED: Repurposing RED’s vision means, for instance, rethinking its role from research infrastructure to an educational resource. Consequently, RED’s practices could be reframed as a playground for DH students, providing an environment for training and annotation evaluation. The knowledge produced and encompassed in RED could document the history of DH methods or become a training set for machine-learning algorithms.2. The Restoration of RED. Restoring RED’s vision means, for instance, incorporating in RED new approaches currently required by funding bodies (e.g. collaboration with data science) and current research priorities within Book History, changed considerably since 1993. Consequently, the study of sources could be combined with machine-learning and natural-language processing tools not included in the original structure of RED. Finally, new research questions such as the effects of reading [18] and the multi-modality of reading on new media [19] could be addressed.With a repurposing approach, the integrity of the dataset could be preserved by relinquishing RED’s role as a research project. With a restoration approach, RED’s role as a research project could be preserved through the entire re-curation of its data, the complete re-development of the tool ecosystem to include automatic steps and the entire reassessment of its value as a research resource in light of current DH and Book History research agendas. Unsurprisingly, to keep a project’s role we must face the cost of adapting to the new context, while to keep its form, we must search for a new purpose.On a more general level, there is a question about how to preserve the “human legacy” of RED, e.g. the network of collaborations, student volunteers and contributors engaged. A DH project is a Cultural Artefact, and therefore its historical context can guide the re-tuning of its role as the context changes or the search for new purposes compatible with the values and vision of the social system of the project.As a final remark, we hope these questions can elicit a broader discussion about new and future DH projects and how we could design for their legacy, e.g. the new Reading Europe Advanced Data Investigation Tool.","alessio.antonini@open.ac.uk, francesca.benatti@open.ac.uk, edmund.king@open.ac.uk",Long Presentation
"Page, Kevin (1); Delmas-Glass, Emmanuelle (2); Beaudet, David (3); Norling, Samantha (4); Rother, Lynn (5,6); Hänsli, Thomas (7,8)","1: Oxford e-Research Centre, University of Oxford, United Kingdom; 2: The Yale Center for British Art, Yale University, USA; 3: The National Gallery of Art, Washington D.C., USA; 4: Indianapolis Museum of Art at Newfields, Indianapolis, USA; 5: Museum of Modern Art, New York, USA; 6: Leuphana Universität, Germany; 7: University of Zurich, Switzerland; 8: ETH Zurich, Switzerland",Linked Art: Networking Digital Collections and Scholarship,Linked Art: Networking Digital Collections and Scholarship,"Linked Data, Art, Art History, Art Provenance, Cultural Heritage Institutions","Global, English, 18th Century, 19th Century, 20th Century, data modeling, linked (open) data, Art history, Humanities computing",English,Global,"18th Century, 19th Century, 20th Century","data modeling, linked (open) data","Art history, Humanities computing","Panel Overview: Linked Art - Networking Digital Collections and ScholarshipLinked Art[1] is a major new initiative by which art museums will publish information about their collections as interconnected open data. Building upon long standing interdisciplinary thinking from the digital humanities and information engineering, Linked Art is an international collaboration across twenty-four institutions identifying focussed, practical models which meet their requirements on a sustainable basis.The central aim of Linked Art is the development and application of Linked Data to cultural heritage collections, with an emphasis on works of art and their provenance. Linked Data will provide the foundation for multi-modal digital scholarship across these rich collections; as an open data standard, Linked Art provides consistent, structured ways for arts institutions to publish art-related data where, in many cases, there has not been a consistent shared model to date.This panel presents a range of perspectives representative of the collaborative intersections found in the Linked Art community: with speakers from universities and art museums; who are practitioners and academics; on topics ranging from implementation, to curation, and research.The panel will takes the form of six position papers, outlined below, followed by questions and answers between the audience and panel. In doing so, Linked Art seeks to engage with the Digital Humanities community, building capacity for future collaborative implementations and research investigations.1. Linked Data and Open Data in Cultural HeritageEmmanuelle Delmas-Glass, The Yale Center for British Art, Yale University, USA.Cultural heritage institutions have a great deal to gain from deeply engaging in the networked environment. They have poured many resources in the digitization of their collections for the benefit of their audiences, from students to experts, who want to have access to more online material of a higher quality. The current landscape of cultural heritage knowledge on the Web, however, is very much siloed, which both harms the relevance of individual institutions as well as the overall state of scholarship which might use that knowledge.This paper reflects on the main challenges that cultural heritage institutions face when it comes to publishing their collections descriptions as Linked Open Data resources. Some challenges can be due to friction between a declared digital mission and the resources allocated, which might seem to be in competition with other institutional priorities. Other memory institutions are still in the process of understanding that managing their knowledge and data – so it can then be leveraged for the Semantic Web – needs to be a core data curation activity. It is also partly due to the lack of entry-level technology and ontology resources that has prevented museums from engaging more deeply with Linked Open Data.This talk will give an overview of previous initiatives and technologies intended to open up access to cultural heritage institutions, particularly art museums, including the International Council of Museums Committee for Documentation Conceptual Reference Model (CIDOC CRM, also an ISO standard); The American Art Collaborative (AAC); PHAROS, the International Consortium of Photo Archives; the Art and Architecture Thesaurus (AAT); the Union List of Artist Names (ULAN); and the International Image Interoperability Framework (IIIF).In the context of the successes and limitations of these efforts, this paper will outline the strategy taken by Linked Art, which is both a standard-based data model and a community, which has emerged from earlier work at the Getty Research Institute. This presentation will challenge the traditional paradigm which has large and wealthy institutions succeed in the face of structural challenges. In this era of hyper connectedness, the solution to museums’ relevance in the Web cannot be developed by a lone institution, and indeed the model that Linked Art promotes is instead based on inclusion with the goal to create institutional and individual partnerships. The other precept that the Linked Art data model advocates for is usability over absolute data completeness, and this talk will go over some specific data modeling principles that allow balance between the requirements of the institution, domain knowledge experts, technologists who will implement the standard, and scholars and other users of Linked Art.Emmanuelle Delmas-Glass is the Collections Data Manager at the Yale Center for British Art, a member of the IIIF Operating Committee, and co-chair of the Linked Art working group of the International Council of Museums’ Committee for Documentation (ICOM CIDOC). She oversees the creation, access to and distribution of the museum’s collections information and metadata, playing the lead role in ensuring the intellectual and technical integrity of the collections data and metadata.2. Conceptual models meet practice and scholarship: conventions, standards, and technologyKevin Page, Oxford e-Research Centre, University of Oxford, UK.The utility of a data model is dependent on its usage, and the context in which this use occurs. While conceptual models play an important role in providing a framework within which different data sets can be consistently represented and combined, in practice the resulting information structures can be perceived as complex or unwieldy, which can stifle adoption in cultural heritage institutions.Specialisms in scholarship and practice bring differing requirements and benefits to the structuring of collections data and its analysis, which are similarly defined by use – the operational needs of a collection or library are different from that of an exhibition, or of the intellectual needs of academic study. The challenge, then, is in respecting and encouraging these different ‘information perspectives’, whilst benefiting from their intersections where they occur, and managing the complexity of the systems and organisational implications. In addition, we recognise scholarship within cataloguing and curation activities, whilst appreciating these have different – but complementary – information needs and outputs from academic study of the collections. Digital tools and methods should reflect these activities, roles and specialisms, rather than constrain them. Doing so can ease adoption of digital approaches alongside existing established practice, increasing the sources of compatible structured information, and achieving overall progress through the combination of multiple information intersections.In considering the above, this paper reflects upon more nuanced notions of authority, standards, and how these are realised technologically; moving from necessary ‘on the wire’ interoperability to a progression from local practice, through emergent community conventions, to international standards bodies. It can be beneficial for different stages of maturity to exist simultaneously across distinct but complementary information structures, reflecting the communities of specialist practice and scholarship who are using the data. The technologies of Linked Open Data, including RDF and ontologies, provide a flexible foundation through which we can realise such an iterative and incremental approach to standardisation, and in which alternative information perspectives can co-exist.Linked Art recognises a further first-class perspective: that of the software developer writing (potentially for, or with scholars) applications which consume collection data provided by cultural heritage institutions. It adopts the principles of Linked Open Usable Data, as proposed by Rob Sanderson of the Getty Research Institute, to create a profile of the CIDOC CRM tailored to this information perspective, and in which established practice – for example, in the use of AAT – can be respected.Kevin Page is Associate Faculty at the University of Oxford e-Research Centre. He was Technical Director of the Oxford Linked Open Data (OXLOD) project, a prototype for the use of CIDOC CRM across the Gardens, Libraries, and Museums of the University of Oxford. Kevin is Principal Investigator of the Linked Art Research Network and Linked Art II project, both funded by the UK’s Arts and Humanities Research Council, and a member of the Linked Art Editorial Board.3. Practicing Linked Art: Evolving Art Data at the National Gallery of ArtDavid Beaudet, National Gallery of Art, Washington D.C., USA.The National Gallery of Art (NGA) seeks to serve the United States in a national role by preserving, collecting, exhibiting, and fostering the understanding of works of art, at the highest possible museum and scholarly standards. As such, the institution is frequently engaged in activities that require production of the structured data describing our collection, the associated media, and writings related to the collection’s historical significance. These demands for data manifest in a variety of ways. Images are requested en masse, data sets are requested in support of research, and both data and media are increasingly requested in support of on-site and remote visitor experiences as well as for analytical purposes.In recognition of the need to automate the accurate publication of data about art, artists, depictions of art, and associated media, the NGA’s department of Analytics and Enterprise Architecture has been seeking data modeling and dissemination standards that are accepted by the cultural heritage art community in order to ensure the greatest reach possible from its automated art data services. The NGA has selected Linked Art as a strategic data standard.  Standardizing formats and exchanges of data through automated means has paid dividends for many industries in the past and continues to do so. For example, the IIIF standards[2] for image sharing have expanded the reach of deep zoom technologies and image collections across the cultural heritage sector and it is expanding into other communities[3]. Whilst data standards are not in themselves novel, no prevailing broadly adopted standard for modeling art data exists. Linked Art seeks to fill that gap.As part of its participation in the Linked Art community, the NGA is evolving an existing art data interface, one that currently provides collection data and images to its Conservation Space system, to use the Linked Art standards as a proof of concept. This paper will give details of that implementation, alongside the basics of the Linked Art model[4] - for representing artworks, people, and depiction. The evolving, open source[5], public-facing NGA interface already provides art data to a location-aware mobile app[6] available for visitors to download on their iOS devices, which will be demonstrated.A solution architect with the National Gallery of Art since 2005, David Beaudet designs and builds technical solutions for authoring and publishing rich art imagery, content, and metadata. David is a member of the editorial board of Linked Art and collaborates on the IIIF Discovery API.4. The Linked Art of Georgia O’Keeffe: Collections Across Institutional BoundariesSamantha Norling, Indianapolis Museum of Art at Newfields, Indianapolis, USA.The creation and publication of linked open data allows for previously siloed data to be connected with other related data on the Web, breaking down institutional barriers and facilitating research and new scholarship – in traditional and digital formats – that may not have previously been possible. With members and community participants representing over 20 different arts-related organizations, the Editorial Board for the developing Linked Art data model for describing art collections reflects this barrier-breaking nature of linked data.In order to showcase the connections that can be made when multiple institutions publish their collections data in a consistent format and utilize shared vocabularies, members of the Linked Art community collaborated to create a cross-institutional sample data set. The artist Georgia O’Keeffe was selected to serve as the common thread for the data to be contributed by participating institutions. With an emphasis on relationships, the linked data collected for the showcase naturally expanded to include not just O’Keeffe and her artworks, but also the works of her contemporaries, the exhibitions in which the artworks were exhibited, and the various organizations and individuals that had participated in provenance events in the lifecycle of the artworks.The development of the Linked Art O’Keeffe data set serendipitously coincided with the launch of the Georgia O’Keeffe Museum’s (GOKM) beta version of the GOKM’s Collections Online, which was built on a linked data foundation. The GOKM expressed the importance of linked data to their digital strategy to make it possible for “meaningful connections to be expressed across different types of collections (artworks, archival items, books, etc.) to establish a more complete understanding of Georgia O’Keeffe’s life, work, and contexts.”Drawing on both the GOKM’s Collections Online[7] and the co-constructed O’Keeffe showcase data set that is now available publicly within the Linked Art GitHub repository[8], this paper and panel presentation will explore the network of Linked Art data with Georgia O’Keeffe at the center. Following intersections between and connections across data sets, the exploration will follow a path through the many relationships identified that link artworks, archives, exhibitions, and people within O’Keefe’s linked data network. In discussing some of the specific relationships identified between institutional collections, the exploration will also highlight key patterns within the Linked Art data model. The Georgia O’Keeffe showcase data set, while small in scale, demonstrates the potential for the Linked Art data model to facilitate the creation of new connections between collections of all types – connections that cross institutional boundaries and facilitate scholarship at the intersections between GLAM collections.As Digital Collections Manager at the Indianapolis Museum of Art at Newfields, Samantha Norling manages digital assets and data related to the museum's art, archival, and horticultural collections. A trained librarian and professional archivist working within an art museum, Samantha is particularly interested in digital projects that break down barriers between GLAM institutions. She is a member of the Linked Art Editorial Board.5. The History of Art is Linked but the Data Is Not: Georgia O’Keeffe, Provenance and ScholarshipLynn Rother, Leuphana Universität, Germany.The history of artworks is linked. They were produced by the same artists, traded by the same dealers, collected by the same people, transferred, looted or confiscated by the same entities while eventually finding their permanent home in the same museums – or not. To date, these links across museum collections are only visible to the few scholars or experts studying the artworks’ history of ownership. But the field of provenance research has matured enough to enable and support structuring and aggregating provenance records as Linked Data.Though shaped by complex and diverse contexts, an artwork’s provenance record can be broken down into empirical data consisting of objects, protagonists, dates, locations and types of transactions. To this day, however, the majority of museums record the valuable information harvested through time-consuming and resource-intensive provenance research within their collection management systems without machine-readable structure, hindering the analysis and linking of the data across institutions on a larger scale. Digital humanities tools offer the potential to standardize, aggregate, and consider the museum accumulated provenance data broadly to reveal new stories about the global circulation and displacement of artworks and nuance the existing histories of collecting and art market practices.As museum objects and their movements through time and space tell stories beyond object-based art historical research and collection cataloguing, this paper will elaborate on the potential of Linked Art for provenance research and for scholarship in related fields. The intertwined histories of selected works by the American Modern artist Georgia O’Keeffe – from different museum collections including the Georgia O’Keeffe Museum in Santa Fe, New Mexico and The Museum of Modern Art (MoMA) in New York – will serve as an example.In particular, MoMA’s acquisition and deaccession of O’Keeffe’s Kachinas – representations of Pueblo and Hopi spirits used in ceremonies and rituals and therefore considered culturally sensitive objects in museum collections – will show how structured provenance data of museums using Linked Art can benefit related research fields such as the histories of collecting and art market practices but also museum, Native American, and Indigenous studies.Lynn Rother is the Lichtenberg-Professor for Provenance Studies at Leuphana Universität, Lüneburg, Germany, and a member of the Linked Art Editorial Board. Previously, Lynn was Senior Provenance Specialist at The Museum of Modern Art, New York, where she oversaw provenance research, procedures, documentation, digital strategies and funding in conjunction with all curatorial departments regarding works in the Collection, loans, acquisitions, and deaccessions.6. Topographia Helvetiae: Linked Art in SwitzerlandThomas Hänsli, University of Zurich / ETH Zurich, Switzerland.The historical view of Switzerland’s nature has been defined by artworks describing a ‘visual topography’ of the Alpine country long before the emergence of a broader touristic interest for Switzerland across Europe. Paintings, drawings, prints, and photographs depicting alpine landscapes, natural monuments, picturesque villages and much more shaped the perception of Swiss landscapes both nationwide and beyond.The Swiss Art Research Infrastructure (SARI) provides unified and mutual access to Swiss collection data, research data, and digitised visual resources from museums, archives, collections, as well as academic and public research institutes, based on a Linked Open Data network. Being part of a national research infrastructure programme, its mission is to combine the unique scholarly expertise from specialised research institutions beyond technical, linguistic, and institutional borders and to enhance the visibility and accessibility of Switzerland’s valuable collections and research resources.The aggregation and access of these visual resources is fundamental for browsing and understanding the evolution of the framing of Switzerland. The project »Bilder der Schweiz« (Views of Switzerland), developed under the aegis of SARI, provides a unique access point to topographic artworks and photographs from the eighteenth to the early twentieth century from major Swiss libraries, museums, and private collections. Starting from mass-printed, but hand-coloured vedute of the so-called ‘Schweizer Kleinmeister’ -- an affordable visual medium to propagate a canonised view of Switzerland that gained increased popularity over time -- the research portal includes related materials such as landscape paintings, drawings, printed textual sources, travel guides, travel journals, and further materials related to artist production, printing and marketing of printed ‘vedute’.The project will provide unified access to these heterogeneous materials from comparable, but technically different, institutional repositories following the Linked Art data model; and in doing so enhance public visibility of these little known, but widely consumed ‘Schweizer Kleinmeister’, and make them accessible to scholars.The project also provides an excellent test of the Linked Art framework’s flexibility, assessing the model’s ability to describe specific non-mainstream subject-based collections. This paper will reflect upon the overall transformation of a diverse selection of data sources into a Linked Art compliant format; with a specific focus on the advantages and drawbacks of the framework when describing tight semantic integration between the expression (the depicted visual apparatus) and the content (the perspective over the object). Finally, a reflection over the possible coexistence of multiple levels of description of an object, each level addressing specific communities, will be presented.Head of gta Digital (ETH Zurich, 2011) and director of the Swiss Art Research Infrastructure (University of Zurich, 2017), Thomas Hänsli has authored the strategy for a national research network and co-authored the development and implementation of comprehensive reference data models based on CIDOC-CRM. He is responsible for the implementation of several Linked Open Data projects in Switzerland and a member of the Linked Art Editorial Board.","kevin.page@oerc.ox.ac.uk, emmanuelle.delmas-glass@yale.edu, d-beaudet@nga.gov, snorling@discovernewfields.org, lynn.rother@leuphana.de, thomas.haensli@uzh.ch",Panel
"Viola, Lorella; Verheul, Jaap","Utrecht University, Netherlands, The",The GeoNewsMiner: An interactive spatial humanities tool to visualize geographical references in historical newspapers,The GeoNewsMiner: An interactive spatial humanities tool to visualize geographical references in historical newspapers,"Spatial Humanities, diasporic newspapers, transatlantic migration, NER, machine learning","Comparative (2 or more geographical areas), Europe, English, North America, 19th Century, 20th Century, artificial intelligence and machine learning, text mining and analysis, Geography and geo-humanities, History",English,"Comparative (2 or more geographical areas), Europe, North America","19th Century, 20th Century","artificial intelligence and machine learning, text mining and analysis","Geography and geo-humanities, History","The GeoNewsMiner[1]: An interactive spatial humanities tool to visualize geographical references in historical newspapers[2]Lorella Violaa and Jaap VerheulbaLuxembourg Centre for Contemporary and Digital History (C2DH), University of LuxembourgbDepartment of History and Art History, Utrecht University, Utrecht, the NetherlandsThe GeoNewsMiner (GNM) is an interactive tool that maps and visualizes geographical references in historical newspapers. As a use case, we analysed Italian immigrant newspapers published in the United States from 1898 to 1920, as collected in the corpus ChroniclItaly (Viola 2018). Immigrant newspapers form a rich source that adds a historical dimension to the study of both the migration of the past century and the migratory experiences of migrant communities (Viola and Verheul 2019). They for instance enable researchers to compare references to the homeland and the host land (Vellon 2010; Forlenza and Thomassen 2016), thus offering an indication of the way diasporic media negotiate processes of assimilation and ethnic identification (Park 1922; Rhodes 2010; Viola and Musolff 2019, Viola and Verheul 2019), a topic that bears great relevance in the global age of satellite dishes and internet connectivity (Dhoest et al. 2012; Hickerson and Gustafson 2016; Parks 2005; Matsaganis, Katz, and Ball-Rokeach 2011; Appadurai 2008).In order to offer new perspectives on the geographies of the past, we employed a state-of-the-art deep learning method to extract and disambiguate place names from historical newspapers. Deep learning outperforms the state-of-the-art of place name extraction and disambiguation based on static lists in gazetteers or ensembles of NER-tools (Canale, Lisena, and Troncy 2018; Won, Murrieta-Flores, and Martins 2018; Mariona Coll Ardanuy and Sporleder 2017; Maria Coll Ardanuy 2017, Yadav & Bethard 2019). The two major advantages lie in its potential for text enriching: 1) they may be based on the historical context of a historical corpus; 2) they are able to recognize toponyms in a dynamic way, for instance as as a geographical concept (Viola and Verheul 2020). For the development of the GNM, we the deep learning sequence tagging tool developed by Riedl and Padó (2018). The sequence tagging retrieved 1,369 unique locations which occurred 214,110 times throughout the whole corpus. Because each individual document is timestamped, it was possible to quantify the number of references to each location was at any given time within the timeframe of ChroniclItaly, that is 1898-1920. Afterwards, locations were geocoded by using the Google API which identifies a place as it is stored in the Google Places database and in Google Maps. The tagged version of ChroniclItaly is available as an open access resource (ChroniclItaly 2.0, Viola 2019).Finally, to visualise and explore the data, we developed the GNM App (Figure 1). Unique to this tool is the possibility to aggregate the data according to a wide range of parameters (time; newspaper’s title; least/most mentioned places; absolute or relative frequency; aggregation on national, regional or city level). It is also possible to overlay historical maps that show the borders of selected years (1880, 1914, 1920, 1994), and download and share the data/results (Figure 2). This offers users the possibility to analyse the results in an intuitive, interactive, and reproduceable way as well as providing great flexibility to researchers working in spatial humanities, particularly from a historical perspective.One potential application of GNM is for example the possibility to reconstruct the “geographical agenda” of historical newspapers by analysing the changing geographical bias of the press, an issue urgent to fields such as media studies, cultural history and international relations (McCombs 2014; Craine 2014; Reese and Lee 2012; Wanta, Golan, and Lee 2004; Gans 2004; Beaudoin and Thorson 2001; Ginneken 1998; Gitlin 2003). As a preliminary data exploration, for instance, the tool shows that references to geographical locations in both Italy and the United States stay remarkably stable over the period that includes the First World War.[3] The full documentation of GNM is made available to the research community to facilitate transparency, reproducibility and replicability (Viola 2020).[4] The app has much to recommend particularly to humanities scholars who are more and more confronted with the challenge of exploring collections larger than before and in a digital format.[1] GeoNewsMiner is a project by Lorella Viola and Jaap Verheul. This project was funded by the Utrecht University Innovation Fund for Research in IT and received support from the Research Engineering team of Utrecht University. The technical implementation was provided by Jonathan de Bruin and Casper Caandorp. The Shiny app was developed by Kees van Eijden[2] We would like to thank DH2020 reviewers for their helpful comments[3] GNM is available as an open access app at https://utrecht-university.shinyapps.io/GeoNewsMiner/[4] https://github.com/lorellav/GeoNewsMiner","lorella.viola@uni.lu, j.verheul@uu.nl",Short Presentation
"Burkette, Allison (1); Kretzschmar, William (2)","1: University of Kentucky, United States of America; 2: University of Georgia, United States of America",Web 2.0 for an Authoritative Web Site,Web 2.0 for an Authoritative Web Site,"Linguistic Atlas Project, Web 2.0, digital collaboration","English, North America, Contemporary, curricular and pedagogical development and analysis, public humanities collaborations and methods, Linguistics",English,North America,Contemporary,"curricular and pedagogical development and analysis, public humanities collaborations and methods",Linguistics,"As we prepare to move into the next generation of the Linguistic Atlas Project website, we now must engage in theoretical discussion about a critical issue in DH: how to democratize online information in accordance with what has become known as Web 2.0. The LAP has long-standing authority within academic discussions of the development and characteristics of the different varieties of American English. One of the goals of the LAP editors is the expansion of the use of LAP data by non-linguists. In order to reach a new, wider audience we plan to create a ""Teaching and Sharing"" extension of the LAP website, which raises the question: how do we, as sponsors of the LAP website, negotiate authority with participation; in short, how do we let people participate without letting go of the authoritative nature of the website? This presentation addresses this question along with some possible answers.","allison.burkette@uky.edu, kretzsch@uga.edu",Short Presentation
"Desjardins, Renée","Université de Saint-Boniface, Canada","How can science and knowledge be created for all and by all without #linguisticjustice?: Findings from a two-year study on the intersections between citizen science, social media, crowdsourcing, and Translation Studies.","How can science and knowledge be created for all and by all without #linguisticjustice?: Findings from a two-year study on the intersections between citizen science, social media, crowdsourcing, and Translation Studies.","Translation Studies, linguistic justice, knowledge dissemination, Citizen Science","Global, English, Contemporary, meta-criticism (reflections on digital humanities and humanities computing), social media analysis and methods, Humanities computing, Translation studies",English,Global,Contemporary,"meta-criticism (reflections on digital humanities and humanities computing), social media analysis and methods","Humanities computing, Translation studies","This paper presents the findings of a 2-year study examining the role of translation and language diversity in online citizen science initiatives. In the last 10 years, the proliferation of mobile technologies, the popularity of participatory culture and social media, as well as the uptick in crowdsourced models for conducting large-scale tasks has impacted the academic landscape. Specifically, this project considers two platforms, Zooniverse and the Canadian Citizen Science Portal. The overarching framework is an amalgamation of methods and theories, situating the project in the transdisciplinary Digital Humanities. Because the project’s mandate is, in part, to informmore equitable exchange/dissemination of citizen science capitals in online and digital spaces, the underpinning philosophical worldview is one that is transformative. Within the purview of Translation Studies, this study falls under the umbrella of descriptive product-oriented research and context-oriented research. Social network analysis (data visualization) and social media analysis (qualitative) further supplements this framework.",rdesjardins@ustboniface.ca,Long Presentation
"Gorman Jr., Daniel James","University of Rochester, United States of America",“Digitizing Rochester’s Religions: Piloting a Community–University Partnership in the Digital Humanities.”,“Digitizing Rochester’s Religions: Piloting a Community–University Partnership in the Digital Humanities.”,"Religion, Urban, Public, History, Archive","English, North America, 19th Century, 20th Century, Contemporary, digital archiving, public humanities collaborations and methods, History, Theology and religious studies",English,North America,"19th Century, 20th Century, Contemporary","digital archiving, public humanities collaborations and methods","History, Theology and religious studies","Launched by Dr. Margarita Guillory at the University of Rochester in fall 2016, Digitizing Rochester’s Religions documents the evolution of religion in Western New York after the Second Great Awakening (1800–1850) ended. Western New York and the city of Rochester were renowned for revivals and new religious movements during the Second Great Awakening, so that the region became known as the “Burned-over District.” However, Western New York’s religious history after 1850 has not received equal scholarly attention. Dr. Guillory and the graduate and undergraduate students who worked on DRR sought to fill this gap. (I served as the lead graduate student researcher.) The team wrote essays about past and present religious sites, visited religious sites and archives, and digitized sources from community archives. We sought to collaborate with local religious communities, so that DRR would build a meaningful relationship between the University and surrounding neighborhoods.Bruce Lincoln’s definition of religion, namely that a religion consists of a “discourse,” a “set of practices,” a “community,” and “an institution,” guides DRR. [Source: Bruce Lincoln, Holy Terrors: Thinking about Religion after September 11 (Chicago: University of Chicago Press, 2003), 5–7; see 5 for “discourse,” 6 for “set of practices” and “community,” and 7 for the full quote of “an institution.”] In its attention to religious spaces and its collaborative approach to scholarship, DRR draws inspiration from Dr. Courtney Bender’s Sacred Gotham, which tasked students with mapping religious spaces in New York City. It also builds on Dr. David H. Day’s 2003 web project “Encountering Old Faiths in New Places: Mapping Religious Diversity in the Rochester, New York Area,” based at Monroe Community College (https://web.archive.org/web/20071102133941/http://www.monroecc.edu/depts/sociology/pluralism/overview.htm). “Encountering Old Faiths” featured students’ ethnographic observations of current religious sites. DRR, by contrast, profiles past as well as present religious sites, so that it is more historical than anthropological in its orientation. Overall, DRR contributes to a growing field of public-facing projects about lived religion in U.S. cities; notable examples include Boston’s Hidden Sacred Spaces (http://www.hiddensacredspaces.org/) and the American Religious Sounds Project (https://religioussounds.osu.edu/).When Dr. Guillory moved to Boston University in 2018, I took over DRR and completed it as a pilot project, documenting religious communities in Rochester’s southwester quadrant, instead of the whole city as originally planned. The website (http://digrocreligions.org/) provides a template for historians, religionists, and students to pursue this work on a larger scale. Taken together, the essays featured on DRR detail how, beginning in the 1960s, the loss of Rochester’s industrial base exacerbated racial and economic segregation. Religious organizations in the economically distressed southwestern neighborhoods filled the gap left by the withdrawal of tax dollars and government services. By launching job programs, soup kitchens, and clinics, religious groups in southwest Rochester tried to meet the physical and material as well as spiritual needs of residents.Had COVID-19 not required the cancellation of DH2020, my lightning talk would have reviewed DRR’s public and digital history aspects and provided a tour of the website. I would have discussed the importance of developing reciprocal relationships with community partners (in our case, religious congregations), although we were not fully successful. After I took over the project, the priority became finishing it, so that I only used publicly available documents instead of archival sources to finish several essays. As for digital humanities technology, I would have discussed the workflow of scanning, formatting, cataloguing, and compressing 80 gigabytes’ worth of primary sources for the website. Finally, I would have presented DRR as an example of successful project-based learning, since the students who worked on DRR gained hands-on archival, ethnographic, and digital humanities experience.",dgormanj@ur.rochester.edu,Lightning
"Tobón Restrepo, Irene","Banco de la República de Colombia, Colombia","La Enciclopedia de Banrepcultural: Una enciclopedia digital de patrimonio cultural colombiano, que nace de la colaboración de los visitantes de las Bibliotecas del Banco de la República ","La Enciclopedia de Banrepcultural: Una enciclopedia digital de patrimonio cultural colombiano, que nace de la colaboración de los visitantes de las Bibliotecas del Banco de la República ","Colombia, cultural heritage, crowdsourcing","South America, English, Spanish, 19th Century, 20th Century, Contemporary, crowdsourcing, digital biography, personography, and prosopography, Art history, History","English, Spanish",South America,"19th Century, 20th Century, Contemporary","crowdsourcing, digital biography, personography, and prosopography","Art history, History","La Enciclopedia de banrepcultural.org es una publicación digital de conocimiento sobre el patrimonio cultural colombiano y en español. Es de libre acceso, libre uso y gran parte de su contenido nace de una creación colaborativa a través de campañas de crowdsourcing.Inició en el año 2000 con respuestas a las consultas académicas de los visitantes a la Biblioteca Luis Ángel Arango y se creó un listado de cerca de 700 artículos que respondían a estas preguntas. En 2017 se decidió publicar estos contenidos en un gestor MediaWiki y se crearon nuevas convocatorias para la generación de información.En esta nueva fase queremos aprender a integrar La Enciclopedia con Wikipedia y Wikidata, así como ampliar los proyectos colaborativos para traducir los artículos a otros idiomas y a las lenguas indígenas del país. Nos preocupa la poca presencia de conocimiento sobre Colombia y Suramérica que se encuentra disponible en internet.",itobonre@banrep.gov.co,Lightning
"Palladino, Chiara (2); Karimi, Farimah (3); Mathiak, Brigitte (1)","1: University of Cologne, Institute of Digital Humanities; 2: Furman University, Classics Department; 3: GESIS Leibniz Institute for the Social Sciences",NER on Ancient Greek texts with minimal annotation,NER on Ancient Greek texts with minimal annotation,"Named Entity Recognition, Herodot, Conditional Random Fields","Europe, English, BCE-4th Century, natural language processing, text mining and analysis, Computer science, Philology",English,Europe,BCE-4th Century,"natural language processing, text mining and analysis","Computer science, Philology","This paper presents the results in the adaptation of a new workflow of Named Entity Recognition and classification applied to primary sources in Ancient Greek. We used a model of language-independent data extraction and pattern discovery based on machine learning algorithms, which allowed the extraction of a dataset of automatically classified place-names and ethnonyms starting from a small manually annotated dataset. The idea is that we should be able to train the machine to recognize an entity from recurring elements in the context, without providing a long annotated training dataset in advance, working on the assumption that premodern textual sources display a recognized systematicity in their linguistic encoding of space, which provides a test-case for automatic and semi-automatic methods of pattern discovery and extraction.","chiara.palladino@furman.edu, karimi.farimah@gmail.com, bmathiak@uni-koeln.de",Short Presentation
"Yamada, Taizo; Inoue, Satoshi","The University of Tokyo, Japan",A Flow for Digitizing Japanese Historical Materials and their Long-Term Use,A Flow for Digitizing Japanese Historical Materials and their Long-Term Use,"Digitalization, Japanese History, Digital Preservation, OAIS","Asia, English, 5th-14th Century, 15th-17th Century, data, object, and artefact preservation, digital archiving, Asian studies, History",English,Asia,"5th-14th Century, 15th-17th Century","data, object, and artefact preservation, digital archiving","Asian studies, History","We formulated a flow and a rule for digitizing historical documents as organizations, not individuals or projects. It is the rule for supplying data, not going to disappear even when projects that create data will end. The flow consists of following processes: the flow has investigating materials and shooting by digital camera, screening and sorting of images, data registration into the database, image registration, security setting. Each process has a responsible person or department in our institution. As a result, images of historical materials of pre-modern Japanese history are acceleratingly concentrated in our storage, and it is growing as ""image cloud for Japanese historical material"". In order to further develop it, now we have been embedding function of the digital preservation[2] like an Open Archival Information System (OAIS)[3] into our system.","t_yamada@hi.u-tokyo.ac.jp, inoue@hi.u-tokyo.ac.jp",Poster
"Vignale, Francois (1); Antonini, Alessio (2); Gravier, Guillaume (3)","1: Le Mans Université, France; 2: Open University, UK; 3: CNRS, France", THE READING EXPERIENCES ONTOLOGY (REO): REUSING AND EXTENDING CIDOC CRM , THE READING EXPERIENCES ONTOLOGY (REO): REUSING AND EXTENDING CIDOC CRM ,"Ontology management, CIDOC-CRM, digital heritage, history of reading","Global, Europe, French, 19th Century, 20th Century, Contemporary, data modeling, linked (open) data, Book and print history, Cultural studies",French,"Global, Europe","19th Century, 20th Century, Contemporary","data modeling, linked (open) data","Book and print history, Cultural studies","This paper aims to present the development strategy of the ontology proposed in the READ-IT project (https://readit-project.eu) and the contributions it makes to the conceptual description of reading experiences and, more broadly, to the description of intangible heritage and other ""experiential"" phenomena. Its development relied on a data-driven approach with the active participation of a representative panel of reading experts from HSS disciplines. The process was iterative to converge towards a consensus balancing ICT requirements and HSS scholar needs. In this regard, we will focus on the general framework and the way in which both alignments of CIDOC CRM (and some of its extensions) with READ-IT’s data model and creations of classes have been carried out, as well as the benefits derived from a pragmatic and cost-efficient approach, allowing us to offer REO as an extension of CIDOC CRM, which will guarantee its reusability, improvement and maintenance over time.","francois.vignale@univ-lemans.fr, alessio.antonini@open.ac.uk, guig@irisa.fr",Short Presentation
"Watson, Jada Emily","University of Ottawa, Canada",From Public Humanities to Social Remembering: Big Data and the Digital Redlining of Women in Country Music Culture ,From Public Humanities to Social Remembering: Big Data and the Digital Redlining of Women in Country Music Culture ,"social remembering, country music, gender representation, big data, digital redlining","English, North America, Contemporary, digital biography, personography, and prosopography, public humanities collaborations and methods, Feminist studies, Musicology",English,North America,Contemporary,"digital biography, personography, and prosopography, public humanities collaborations and methods","Feminist studies, Musicology","Theories of social remembering (Misztal 2003; Strong 2011) and digital redlining (Noble 2018) offer a critical framework for considering the credibility of big data within cultures that disadvantage and systematically ignore women. Reflecting on results of a data-driven analysis of Mediabase’s country airplay reports from 2000 to 2018, this paper considers the role of data in the process of shaping country music culture, and reframes our understanding of these reports as an instrument that systematically “remembers” some artists, while “casting away” or “forgetting” others. Over the course of this period, the number of songs by women played on country format radio declined 41.3% (Fig. 1). These reports map the evolving terrain of country music’s cultural space and have resulted in a system that pushes women to the margins: songs by women are played infrequently on country format radio (Fig. 2), with the majority of their airplay occurring in the overnights (Fig. 3) (Watson 2019a/b). As a result, songs by women are charting in declining numbers, peaking in the bottom positions of the weekly charts, and barely heard in radio’s peak daytime hours. Such practices impact black women more significantly (Watson 2020). In this way, the reports reveal a digital redlining of women, wherein programming practices are perpetuating inequalities by refusing high traffic times of day to already marginalized artists. More critically, this paper addresses the challenges of critiquing social remembering through public scholarship and reflects (as Marcia Chatelain [2016] does in her work) on my experiences of thinking and working in public digital spaces.Gender has been a central dynamic of country music history and culture (Pecknold & McCusker 2016), wherein masculinity and femininity are invoked to define class boundaries, cultural tastes, institutional hierarchies, performance styles, and the evolution socially prescribed roles. With strong ties to conservatism and religion, country’s first female artists often appeared on stage with their husband or male family members, a constant reassurance for record-buyers that the social order in which females performed familial roles and habits of constancy and tradition endured in the genre (McCusker 2017). Following WWII, as female artists began taking a more prominent role on stage and behind the scenes, cultural institutions actively sought to censor lyrics in women’s songs if they were too “suggestive”, aggressive, or politically charged—a trend that has continued throughout the genre’s history (Bufwack & Oermann 2004; Keel 2004; Watson & Burns 2010). Behind the scenes, the country music industry has employed a strict quota system for female artists on radio playlists and label rosters (Penuell 2015), which has, in turn, limited their opportunities for participating within the mainstream of the industry as performer and songwriters.Adopting methods for data-driven studies of popular music charts (Wells 2001; Lafrance et al 2011) and influenced by the concept of prosopography (Keats-Rohan 2007; Crompton & Schwartz 2018), this project has developed an approach for collecting and organizing music industry data in order to study how the biography of individuals shapes and is shaped by the genre’s cultural constructs. In order to address complex socio-cultural issues of equity and diversity in country music, it has developed a comprehensive dataset of all of the singles played on country format radio between 2000 and 2018, enhanced with biographic information about the lead and featured artists involved in performing the recorded tracks played on country radio to facilitate a vast range of queries about programming practices and their impact on weekly charts. In so doing, this project deconstructs the gender politics that have governed the genre and shows how big data has created and perpetuated gender inequalities and contributed to the continued marginalization and “forgetting” of female narrative voices within country music culture.  Figure 1. Distribution of unique songs by men, women and male-female ensembles played on country format radio between 2002 and 2018.Figure 2. Distribution of spins for songs by men, women and male-female ensembles between 2002 and 2018 reveals a 40.2% increase in spins for male artists against a 44.8% decline in spins for songs by women.Figure 3. Distribution of spins for songs by men, women and male-female ensembles across the five dayparts in country format radio programming in 2002 (left) and 2018 (right).",jwatso4@uottawa.ca,Short Presentation
"Homburg, Timo",Mainz University Of Applied Sciences,Mind the gap: Filling gaps in cuneiform tablets using Machine Learning Algorithms,Mind the gap: Filling gaps in cuneiform tablets using Machine Learning Algorithms,"Cuneiform, Text","Asia, English, BCE-4th Century, artificial intelligence and machine learning, semantic analysis, Computer science, Linguistics",English,Asia,BCE-4th Century,"artificial intelligence and machine learning, semantic analysis","Computer science, Linguistics","IntroductionA presisting problem in near eastern studies is the existence of broken cuneiformtablets (listing 1.1). In recent years efforts have been undertaken to 3DScan Maraet al. (2010), to paleographically describe Homburg (2019) and to digitally recon-struct broken fragments Collins et al. (2014, 2017) of cuneiform tablets. However,not always broken fragments can complement each other and often parts of thecuneiform tablet remain destroyed. These fractures or gaps in the cuneiformtablet are not always easy for scholars to fill and take a considerable amount ofinterpretation time on their part. With the emergence of more digitally availablecuneiform text resources, this publication sees an opportunity to investigate ifauto-complete algorithms, based on machine learning and linguistic linked opendata (LLOD) resources Homburg (2017) can be useful in the reconstructionof cuneiform texts. The classification results are to be used to create a epochand language specific recommendation system to fill gaps on cuneiform tablets,therefore assisting cuneiform scholars.Related WorkRelated work has been done in autocompletion systems which face the similarchallenge of anticipating the users input derived from context and other featuresLeung & Zhang (2008), Gikandi (2006), Hyvönen & Mäkelä (2006). Those tech-nologies are heavily relied on in input method engines1 which are powered withdifferent dictionary-based algorithms, but recently Chen et al. (2015), Huanget al. (2018) also with machine learning approaches and neural networks. Inputmethod engines for cuneiform have been developed by Homburg et al. (2015).MethodologyFollowing Homburg & Chiarcos (2016) machine learning methods applied areeither based on grammatical rules (POSTagging), dictionary-based methods ex-ploiting (third-party) dictionary resources or statistical approaches using thefollowing types of machine learning features:– Context-dependent features: e.g. for Hidden Markov Model Classifications– Grammatical features derived from POSTaggers– Semantic Features derived from the semantic meaning of surrounding words– Metadata Features e.g. text categorizations– Paleographic Features using PaleoCodage for a subset of manually annotatedtexts Homburg (2019)Experimental SetupThe effectiveness of the algorithms and features is tested on a corpus of all CDLItexts in ATF which is split in a training and test set. Texts are prepared withrandom gaps for classification and evaluated using the original texts (the goldstandard) on unicode cuneiform and on the respective cuneiform transliterationfor different cuneiform languages (Sumerian, Hittite, Akkadian) and epochs. Theposter features selected peliminary results of the classification and a significanceanalysis of the features for further discussion for improvement. A possible futuregoal could be a shared task to improve classification accuracy similar to thecuneiform language identification challenge Jauhiainen et al. (2019)ApplicationLastly, the poster presents a prototypical application (fig. 1)displaying the results of the machine learning process which is currently in devel-opment. The implementation builds up on the concept of input method enginesHomburg et al. (2015) and will provide a self-learning component.Caption Figure 1: (prototype.jpg)Text Completion Prototype: ”If...Enlil”. The dictionary knows, that Enlilis a gods name (NE) and is commonly preceded by a determinative character for god𒀭(an), which is suggested in first place to fill the gap. Next likely options are aperson named Enlil (male or female), the people (tribe) of Enlil, or a location.",timo.homburg@gmx.de,Poster
"Esten, Emily (1); Blickhan, Samantha (2); Noel, Will (1); Rustow, Marina (3)","1: University of Pennsylvania Libraries, United States of America; 2: Adler Planetarium; 3: Princeton University","Scribes, Scholars, & Scripts: creating a Digital Humanities community through crowdsourcing","Scribes, Scholars, & Scripts: creating a Digital Humanities community through crowdsourcing","open data, crowdsourcing, multilingual DH, text analysis, project management","Global, English, 5th-14th Century, 15th-17th Century, 18th Century, crowdsourcing, public humanities collaborations and methods, History, Library & information science",English,Global,"5th-14th Century, 15th-17th Century, 18th Century","crowdsourcing, public humanities collaborations and methods","History, Library & information science","How do you organize a research project around manuscript fragments? Digital collections can allow teams to create a shared online space in which images can be hosted, but the types of research questions that individual scholars may want to ask can be as fragmented as the objects of interest themselves. In this panel, we will discuss Scribes of the Cairo Geniza (https://www.scribesofthecairogeniza.org), a collaboration between the University of Pennsylvania Libraries and the Zooniverse (https://www.zooniverse.org), the world’s largest platform for online crowdsourced research. The project invites the public to help classify and transcribe fragments from the Cairo Geniza, a corpus of 350,000 fragments primarily from the 10th-13th centuries, found in a storeroom (or ‘geniza’) of the Ben Ezra synagogue in Fustat. The panelists will discuss the process of designing, developing, and running a large, multi-institution online crowdsourcing project from the following perspectives: project manager, web developer, content specialist, and data specialist.","estenemily@gmail.com, samantha@zooniverse.org, wgnoel@gmail.com, mrustow@princeton.edu",Panel
"Howard-Sukhil, Christian","Bucknell University, United States of America","<em>Project Twitter Literature</em>: Scraping, Analyzing, and Archiving Twitter Data in Literary Research","Project Twitter Literature: Scraping, Analyzing, and Archiving Twitter Data in Literary Research","social media, data curation, data preservation, globality","Global, English, Contemporary, digital archiving, social media analysis and methods, Literary studies, Media studies",English,Global,Contemporary,"digital archiving, social media analysis and methods","Literary studies, Media studies","Project Twitter Literature (TwitLit), seeks to address a growing gap in the literary-historical record[1] by establishing a consistent, rigorous, and ethical method for scraping and cleaning up Twitter data for the use of humanities scholars. In particular, my project explores the growing community of amateur writers who are using Twitter as a means of publication and dissemination for their literary output. There are three parts to my project: the research findings related to the global literary community on Twitter, the tools and resources developed as part of the project and made openly available to other scholars, and partnership with a university library to ensure the long-term preservation of the collected data.The data that I have collected shows that social media is altering literary practices by providing a space for amateur writers to publish, disseminate, and receive feedback from a global community of writers. Preliminary figures put the number of active Anglophone writers using Twitter as a publication platform for their literary output at over 1 million users per year since 2015, and writers working in non-English languages on Twitter raise these numbers even higher. This practice is changing how literature is produced, published, and shared. Readerships too are changing, for rather than being tied to print subscriptions or access to physical books, audiences of social media literature are based on online communities and tied to the costs of physical devices and internet access.My presentation will showcase these research findings in order to highlight the importance and necessity of social media archival work. In so doing, I will discuss how I collected the data using a Python script (co-developed by myself and several other scholars), challenges of cleaning up and visualizing this data (using ArcGIS and tools developed by Documenting the Now), and ethical best-practices for using social media data in research. Information relating to this process – including detailed instructions, the Python scripts used to collect Twitter data, and a list of resources – are free and openly accessible on the project’s website (www.twit-lit.com) and GitHub repository (https://github.com/TwitLit/TwitLitSource). Other scholars are invited to use these scripts and other resources to collect their own social media data.Additionally, my project has attempted to plan for the long-term preservation of the over eight million tweets that I have collected. This preservation has been made difficult by Twitter’s strict Developer Policy and Agreement, which prevents individuals from keeping or disseminating large data sets for more than 30 days. The only exception to this policy is made on behalf of academic institutions, which may store Twitter data for unlimited amounts of time on behalf of academic research.[2] The Project TwitLit project thus presents best-practices for establishing a working relationship with university libraries for storing and disseminating Twitter data in a way that is both in accord with Twitter’s legal restrictions and responsive to the needs of scholars. In short, Project TwitLit provides a case-study of a growing community on Twitter while simultaneously developing a set of tools and guidelines for other scholars seeking to engage in similar work.[1] In December 2017, the Library of Congress, which began archiving Twitter in 2010, announced that it would no longer collect all Tweets; instead, Tweets produced after December 2017 would only be collected on a selective basis. There are no other ongoing, systematic efforts to collect and preserve this digital material. See Library of Congress, “Update on the Twitter Archive at the Library of Congress” (December 2017).[2] For more information related to the challenges of collecting and storing Twitter data, please see Christian Howard, “Studying and Preserving the Global Networks of Twitter Literature,” in Post-45.",cfh008@bucknell.edu,Short Presentation
"BLILID, Abdelaziz","University of Tours, France",«La planète numérique» d'un peuple autochtone transnational: Une analyse des liens hypertextes des sites web amazighs,«La planète numérique» d'un peuple autochtone transnational: Une analyse des liens hypertextes des sites web amazighs,peuples autochtones - culture amazighe - activisme culturel - communauté imaginée,"Africa, French, Contemporary, cultural analytics, linked (open) data, African and African American Studies, First nations and indigenous studies",French,Africa,Contemporary,"cultural analytics, linked (open) data","African and African American Studies, First nations and indigenous studies","This research processing on the Internet using the Aboriginal Nations in the North America: les Berbères ou les Amazighs. Ce peuple a Investi Internet Internet dès les années 1990, a déclaré que le texte était représenté dans la toile numérique afin de permettre la transmission de la culture entre eux et d'associations et de faire en sorte que leurs revendications politiques (Almasude, 1994). Ils ont créé des sites Web pour codifier et transmettre leur patrimoine culturel. Ainsi, le Web leur a offert un moyen singulier de transmission: en transposant dans l'univers numérique, l'identité culturelle s'est redéfinie. This is this subject of this study at the people on the amazigh for cultural protection and activism is one one jou.",a.blilid@gmail.com,Lightning
"Okuda, Nozomu (1); Kinnison, Jeffery (2); Coffee, Neil (1); Scheirer, Walter (2)","1: Department of Classics, The University at Buffalo, SUNY; 2: Department of Computer Science & Engineering, University of Notre Dame",Integrating Intertextual Search into Your Web Application: The Tesserae Intertext Service API,Integrating Intertextual Search into Your Web Application: The Tesserae Intertext Service API,"intertextuality, API, web services, digital classics","English, North America, BCE-4th Century, 5th-14th Century, Contemporary, software development, systems, analysis and methods, text mining and analysis, Literary studies, Philology",English,North America,"BCE-4th Century, 5th-14th Century, Contemporary","software development, systems, analysis and methods, text mining and analysis","Literary studies, Philology","The Tesserae Project presents the TIS API, a REST-based web service that allows partner collections to integrate Tesserae intertext search directly into their web applications.","nozomuok@buffalo.edu, jkinniso@nd.edu, ncoffee@buffalo.edu, walter.scheirer@nd.edu",Poster
"Scheirer, Walter (1); Forstall, Christopher (2)","1: University of Notre Dame, United States of America; 2: Mount Allison University, Canada",Quantitative Intertextuality: Analyzing the Markers of Information Reuse,Quantitative Intertextuality: Analyzing the Markers of Information Reuse,"Intertextuality, Text Analysis, Cultural Analytics, Computer Vision, Memes","English, North America, Contemporary, artificial intelligence and machine learning, text mining and analysis, Cultural studies, Literary studies",English,North America,Contemporary,"artificial intelligence and machine learning, text mining and analysis","Cultural studies, Literary studies","A remarkable amount of information crosses our eyes and ears each day, yet we adeptly identify what is familiar with seemingly no effort at all. In many cases, what we see or hear has been shaped by recognizable prior sources. Such instances of intertextuality reveal a wealth of data about authorship, influence, and style, making them attractive targets for automatic identification. This lightning talk introduces quantitative intertextuality [Forstall-and-Scheirer-2019], a new approach for the algorithmic study of information reuse in text, sound and images. Using a variety of tools drawn from machine learning, natural language processing, and computer vision, we will describe how to trace patterns of reuse across diverse sources for scholarly work and practical applications.","walter.scheirer@nd.edu, cforstall@mta.ca",Lightning
"Day, Kevin",University of British Columbia,“Beyond the logic of commensurability: a cultural analysis of media artworks and digital media in information capitalism”,“Beyond the logic of commensurability: a cultural analysis of media artworks and digital media in information capitalism”,"media art, philosophy of technology, media studies, information capitalism, big data","Global, English, Contemporary, digital art production and analysis, Art history, Media studies",English,Global,Contemporary,digital art production and analysis,"Art history, Media studies","The proposed presentation will examine the pedagogical potential of media artworks that interrogate the big data economy within contemporary information society. The study begins by establishing the socio-political landscape within which it is situated, one that recognizes the pervasive utopic myth and exploitative algorithmic activities of informatics, and asserts that media art needs to address digital media by examining the underpinning logic of information within the wider landscape of information capitalism. Guided by a framework that pulls together theories of media and art, the paper argues that media art has the capacity to subvert normalized and entrenched ways of knowing through its potential to foster ways of 'knowing differently' in relation to information and communication technology. To substantiate the argument, the paper will position information as an epistemic model through which one comes to make sense of the world – and precisely that which visual/media art should tackle and question.",kevin.t.day@gmail.com,Short Presentation
"Kelber, Nathan",ITHAKA,<em>The Plant Humanities Workbench</em>: Using Linked Open Data to Discover Early Modern Plant History,The Plant Humanities Workbench: Using Linked Open Data to Discover Early Modern Plant History,"plants, linked open data, early modern, book history","Comparative (2 or more geographical areas), Europe, English, North America, 15th-17th Century, Contemporary, linked (open) data, software development, systems, analysis and methods, Book and print history, Environmental, ocean, and waterway studies",English,"Comparative (2 or more geographical areas), Europe, North America","15th-17th Century, Contemporary","linked (open) data, software development, systems, analysis and methods","Book and print history, Environmental, ocean, and waterway studies","In 2017 with the support of the Mellon Foundation, the experimental wing of JSTOR known as JSTOR Labs began a three year partnership with Dumbarton Oaks, a Harvard-owned research library and collection in Washington DC, to create a new digital humanities tool focused on the study of plants from a humanities perspective. The Plant Humanities Workbench will enable new research by intergenerational teams of students, advanced researchers, and professionals. Drawing on the technical and design expertise of the JSTOR Labs team, the digital tool will be interactive, iterative, and scalable. This presentation will demonstrate the tool after its first year of development, summarizing key findings in the project that may be relevant to a variety of digital humanists.",NKELBER@GMAIL.COM,Short Presentation
"Kelber, Nathan","JSTOR Labs, United States of America",Algorithms of Resistance: Using OCR and AI for Social Justice,Algorithms of Resistance: Using OCR and AI for Social Justice,"jim crow, algorithmic bias, ocr, tdm","English, North America, 19th Century, 20th Century, optical character recognition and handwriting recognition, text mining and analysis, African and African American Studies, Law and legal studies",English,North America,"19th Century, 20th Century","optical character recognition and handwriting recognition, text mining and analysis","African and African American Studies, Law and legal studies","This long presentation will share key findings from the final report of On the Books: Jim Crow and Algorithms of Resistance, a Mellon-funded Collections as Data project using machine learning to systematically discover racism within North Carolina laws. The project will make North Carolina legal history accessible to researchers by creating a corpus of over one hundred years of North Carolina public, private, and local session laws and resolutions from the end of civil war through the civil rights movement (1865-1968). This project represents the intersection of many subject areas including critical race theory, American history, text and data mining, machine learning, and public humanities. It will be of interest to subject specialists, data science researchers, educators, librarians, and other information professionals.",NKELBER@GMAIL.COM,Long Presentation
"Hunter, Elizabeth",San Francisco State University,<em>Bitter Wind</em>: Adapting Greek Tragedy for Spatial Computing,Bitter Wind: Adapting Greek Tragedy for Spatial Computing,"spatial computing, theatre studies, Greek tragedy, mixed reality, interactivity","English, North America, BCE-4th Century, Contemporary, meta-criticism (reflections on digital humanities and humanities computing), virtual and augmented reality creation, systems, and analysis, Literary studies, Performance Studies: Dance, Theatre",English,North America,"BCE-4th Century, Contemporary","meta-criticism (reflections on digital humanities and humanities computing), virtual and augmented reality creation, systems, and analysis","Literary studies, Performance Studies: Dance, Theatre","This presentation will report on the completed research project Bitter Wind, my adaptation of an ancient Greek tragedy for Microsoft’s spatial computing HoloLens headset. The presentation will demonstrate how emerging embodied technologies like spatial computing (1) offer new possibilities for qualitative interpretation and (2) promote greater theatre studies engagement with digital humanities. In addition to outlining Bitter Wind’s technological elements, the presentation will explain how I integrated the affordances of spatial computing with theories of audience participation to mount a new dramaturgical and historiographic analysis of a canonical source. This presentation will also argue that a stronger theatre studies presence in DH has become urgent, as DH inquiry begins to expand from two-dimensional screens to the immersive, 360° environment created by embodied technologies like augmented/mixed/virtual reality, wearables, and the Internet of Things.",ebh@sfsu.edu,Short Presentation
"Blackwell, Christopher William; Palladino, Chiara; Greico, MacKense; Bolton, Allie","Furman University, United States of America",DUCAT: Passage/Translation Alignment with the CITE Architecture,DUCAT: Passage/Translation Alignment with the CITE Architecture,"citation, translation, prose, poetry, alignment","Comparative (2 or more geographical areas), Europe, English, North America, Contemporary, linked (open) data, semantic analysis, Literary studies, Translation studies",English,"Comparative (2 or more geographical areas), Europe, North America",Contemporary,"linked (open) data, semantic analysis","Literary studies, Translation studies","DUCAT is a tool that uses the CITE Architecture, allowing alignments among texts. It is a zero-infrastructure HTML app that exports data in a plain-text CEX format. It allows two kinds of alignments among any number of texts, in any language:**Citation Alignment** A poem, cited line-by-line, is translated into prose, cited by paragraph. A paragraph of the translation might correspond to several lines of poetry; a range of lines of poetry might overlap with seveal paragraphs of the prose translation.**Translation Alignment** A text and its translation will not align word-by-word, but will inevitably consist of one-to-one, one-to-zero, many-to-one, one-to-many, many-to-many, many-to-zero, or zero-to-many alignments. In the case of ""many"" the words may be discontiguous. When we align more than one translation with an original text, these problems multiply.While DUCAT is a standalone tool, the data it produces is generic, plain-text, and can be re-imported, shared, and reused.","cwblackwell@gmail.com, chiara.palladino@furman.edu, mackense.greico@furman.edu, allie.bolton@furman.edu",Short Presentation
"Bermúdez Sabel, Helena; Dell'Oro, Francesca; Marongiu, Paola",Université de Lausanne,Visualisation of semantic shifts: the case of modal markers,Visualisation of semantic shifts: the case of modal markers,"Latin, Diachronic linguistics, Modality, Semantic maps, Visualization","Global, Europe, English, BCE-4th Century, Contemporary, data modeling, semantic analysis, Humanities computing, Linguistics",English,"Global, Europe","BCE-4th Century, Contemporary","data modeling, semantic analysis","Humanities computing, Linguistics","This proposal examines the importance of visual representations to convey semantic shifts while presenting a work in progress concerning the diachronic study of modality in Latin.1A visualisation is meant to aid the comprehension of data by taking advantage of our visual perception and its capacity to discern patterns, trends and atypical values (Heer et al. 2010: 59). A visual representation is more accessible, attractive and it can replace complex cognitive calculations. However, selecting the most efficient visualisation can be challenging, especially when conceiving it as a scholarly resource (see Jessop 2008) for the representation of abstract concepts. In our case we would need to:condensate pages and pages of dictionaries and historical grammars;add information to previous models to better convey the multidimensionality of modal semantic shifts;  update the traditional visualisation models incorporating motion, color and user interactivity.The semantic map visualisation method was introduced by Haspelmath (2003)2 to describe and illustrate the multifunctionality patterns of linguistic elements. The semantic map appears as a geometric representation of functions connected together in a semantic space. Semantic maps were employed in various ways, cross-linguistically or on individual languages,3 and synchronically or diachronically.Van der Auwera and Plungian (1998) apply this resource to visually represent and predict universal patterns of modalisation.4 They build on the single patterns of modalisation for possibility and necessity of the cross-linguistic study by Bybee et al. (1994) (see Fig. 1), complementing them with lexical information from other languages. An overview of the modalisation and grammaticalisation paths is achieved by including pre- and post-modal meanings (Fig. 2).5Fig. 1: “To possibility and beyond” (van der Auwera and Plungian 1998: 91).Fig. 2: “Unifying the possibility and necessity paths”: Example of a semantic map representing the shifts of possibility and necessity (van der Auwera and Plungian 1998: 98).Our proposal follows this model but our aim is to produce a digital visualisation with these additional features:Diachrony: Addition of a timeline to visualise when new meanings appear.Synchrony: Visualisation of coexisting meanings based on position and shape.Chronology: Addition of the etymological information available for each marker;6 enrichment of each meaning with its first attestation that is displayed by hovering the mouse over that concept.Polyfunctionality: Working with empirical data does not imply unambiguous findings, therefore our proposal codifies multiple modal values of the same marker.Multilingual versions: A map in the source language together with the English translation is available: users can change the language at will.Legibility: Points 1-5 extend the contents of previous models. To guarantee legibility, certain information is color-coded and other pieces of information will appear by user demand. Colors, for instance, are employed to identify and distinguish pre-, post- and modal meanings. Users can bring out a specific path of modal shift just by clicking on one of the affected senses/steps. A combination of shape and position can render the coexistence of meanings.We are currently working on the development of the semantic modal maps of some Latin modal markers. These maps were drafted based on the Thesaurus Linguae Latinae (Thesaurusbüro München Internationale Thesaurus-Kommission 1900–), and are susceptible of change against corpus-based evidence. This work in progress is available at our website (http://woposs.unil.ch/semantic-modal-maps.php). Updates refining the visualization and/or adding more semantic maps are frequent. As part of the future work, we envisage the implementation of additional features: we plan to employ motion to visualise the modal path of a specific marker interacting with its chronological attestations. Also, we are conscious of the challenges that discernity of color entails for people with visual disabilities, thus we want to avoid the use of color as the only visual means of conveying certain information. Therefore, we plan to combine it with other visual cues, like texture. In addition, the size will be customizable and color contrast will be checked.7The visualisations currently available were developed using Inkscape8 to create the basic SVG that will be enhanced by manually including the animation elements with a combination of JavaScript and CSS. The selection of open-source software guarantees an open development. Therefore, not only the results, but all the data and methods will be made publicly available, thus contributing to both the Open Data movement and Public Digital Humanities.Even if the core of our work concerns Latin modality, the principles and techniques presented can be easily applied to any other language. We claim this model to be versatile: it would be useful to researchers, but also appropriate for vehiculating complex semantic concepts in an educational environment thanks to its readability and immediacy. Besides its possible implementation with languages other than Latin, any type of semantic shift could be visualised following our template. Semantic maps aid in the understanding of meaning so any fields working with natural language, like history or philology, would benefit from our results.Notes1 This study is part of the SNSF-funded project (SNSF n° 176778) A world of possibilities: Modal pathways over an extra-long period of time: the diachrony of modality in the Latin language (WoPoss). See more information in <http://woposs.unil.ch>. All the data and code of this project will be made available during the project lifespan (February 2019–January 2023) as open data.2 For earlier conceptualizations of semantic maps see Hjelmslev (1963), Lazard (1981), Anderson (1982).3 For both a single-language approach and a cross-linguistic one see François (2008).4 Semantic maps had been already applied (amongst others) to tense and aspect (Anderson 1982), evidentiality (Anderson 1986), conditionals (Traugott 1985), voice (Croft et al. 1987).5 The model by van der Auwera and Plungian (1998) and Bybee et al. (1994) has been applied by Magni to modal markers in Latin (2005).6 The etymological information for Latin is taken from the Thesaurus Linguae Latinae (1900–), de Vaan (2011), Ernout-Meillet (1932) and Meiser (1998).7 See <https://webaim.org/resources/contrastchecker> (consulted on 05/06/2020).8 Available at <https://inkscape.org/> (consulted on 05/06/2020).","helena.bermudezsabel@unil.ch, francesca.delloro@unil.ch, paola.marongiu@unil.ch",Poster
"Smithies, James (1); Balaawi, Fadi (2); Flohr, Pascal (3); Rababeh, Shaher (2); Idwan, Sahar (2); Palmer, Carol (4); Mubaideen, Shatha (4); Esposito, Alessandra (1); Ciula, Arianna (1)",1: King's College London; 2: Hashemite University; 3: University of Oxford; 4: Council for British Research in the Levant,MaDiH (مديح): Mapping Digital Cultural Heritage in Jordan,MaDiH (مديح): Mapping Digital Cultural Heritage in Jordan,"Jordan, archaeology, infrastructure, data, analysis","Asia, English, BCE-4th Century, 5th-14th Century, 15th-17th Century, data publishing projects, systems, and methods, database creation, management, and analysis, Archaeology, Library & information science",English,Asia,"BCE-4th Century, 5th-14th Century, 15th-17th Century","data publishing projects, systems, and methods, database creation, management, and analysis","Archaeology, Library & information science","MaDiH (مديح): Mapping Digital Cultural Heritage in Jordan, is a collaborative project between King’s Digital Lab (KDL), the Hashemite University, the Council for British Research in the Levant (CBRL), the Department of Antiquities of Jordan, the Jordanian Open Source Association, and the Endangered Archaeology in the Middle East and North Africa (EAMENA) project. It is scheduled to run for two years, from February 2019 - February 2021. The project will contribute to the long-term sustainable development of Jordan’s digital cultural heritage, identifying key systems, datasets, standards, and policies, and aligning them to government digital infrastructure capabilities and strategies. Defining a robust technical and operational architecture for digital cultural heritage will assist the Department of Antiquities in their planning processes, help product development teams develop their systems, facilitate the aggregation of valuable datasets held in disparate repositories, and ensure data generated from research activity is properly stored and widely accessible.","james.smithies@kcl.ac.uk, fadi.balaawi@hu.edu.jo, pascal.flohr@arch.ox.ac.uk, srababeh@hu.edu.jo, sahar@hu.edu.jo, director@bi-amman.org.uk, shathamubaideen@bi-amman.org.uk, alessandra.g.esposito@kcl.ac.uk, arianna.ciula@kcl.ac.uk",Short Presentation
"Helling, Patrick","University of Cologne, Germany",Modelling Consultation Workflows for Research Data Management in the Humanities ,Modelling Consultation Workflows for Research Data Management in the Humanities ,"Research Data, Research Data Management, RDM, Data Center, Workflow","Europe, English, Contemporary, digital research infrastructures development and analysis, sustainable procedures, systems, and methods, Library & information science",English,Europe,Contemporary,"digital research infrastructures development and analysis, sustainable procedures, systems, and methods",Library & information science,"AbstractResearch data management (RDM) is becoming more important for researchers and research institutions like universities and non-university centers. Due to the heterogeneity of methods in the Humanities, RDM in the Humanities is very complex. Several data centers and contact points have been established to handle this complexity, support researchers and conduct active RDM. Nevertheless, there are few best practices and policies on how RDM is done in a sustainable and goal-oriented way. In this paper I present a consultation-protocol-template we developed at the Data Center for the Humanities (DCH) at the Faculty of Arts and Humanities at the University of Cologne (UoC) to structure and measure RDM in a comprehensive way. I illustrate generic RDM-consultation-workflows developed on the basis of our consultations.1.   Introduction and Background Research data is an essential ingredient and facilitator of scientific progress across all disciplines and includes heterogenous data (Bryant, Lavoie, Malpas, 2017). The developments of the European Open Science Cloud (EOSC) and OpenAIRE, and the plans to establish a national research data infrastructure (NFDI) for the whole German research landscape (RfII, 2016) illustrate that the relevance of RDM has reached a high political level, nationally and internationally.1 However, this is a rather new development and practical implementation has not yet reached maturity.The Data Center for the Humanities (DCH) is the first contact point for researchers in the Humanities at the University of Cologne (UoC) on questions concerning we (Blumtritt, Helling, Mathiak et. al., 2018).2 Through open consultation hours the DCH advise researchers at the Faculty and beyond, attend to their projects and support them as far as RDM is concerned. In addition, we run the Language Archive Cologne (LAC), a repository for audio-visual data, we do active data management and curate, archive and publish research data.4In this paper, I seek to further the professionalization of RDM, by looking closer at the consultation process and the experiences made with it at our institution in 6 years of consultation practice.2.   RDM consultation protocols In order to identify goal-oriented solutions for RDM-needs and as a basis for active RDM, we developed a consultation-workflow (see Figure 1). Through semi-structured interviews and structured consulting protocols, we are offering sustainable RDM-services and -solutions (Helling, Blumtritt and Mathiak, 2018). With more than 70 consulting protocols, the DCH has a wealth of knowledge.Figure 1: The core consulting-workflow at the DCH.On the basis of a comparative analysis of the structured consulting protocols, I was able to identify the core information required for active RDM. The information can be subsumed under the categories (red) context information of a process, (yellow) RDM request-categories with a concrete description of the categories in the context of a consultation and (green) recommendations (see Figure 2).Figure 2: anonymous example of the consulting protocol template: (A) consists of meta information about a process, (B) and (C) are both consulting protocols of a process.3.   Modelling of RDM-consultation workflowsIn 2019, I identified 28 demand-categories in 36 consultations. I started modelling generic workflows for each demand-category fitting with every consultation, where the demand was given with the help of the Business Process Model and Notation (BPMN).4 I will only illustrate on two examples in this abstract, but plan to show more in the presentation.3.1.   Support with a proposalMany funding institutions ask for statements on how researchers will deal with the digital research data representing the output of an applied project. If a consultation is needed in such a case, we conduct an interview with the researchers and try to receive as much information as possible about the project, which we translate into our consultation-protocol-template.Figure 3: Consultation workflow for support with a proposal.Based on the information in the protocol, we then follow the steps as detailed in Fig. 3, always taking the context information into account. Based on similar cases, we suggest a text to describe possible strategies for data management and define fundamental steps of data handling, archiving and publishing.These formulations get integrated into the overall application by the researchers and thus become part of the proposal.3.2.   Data archivingAnother common request is the archiving (see Figure 4) of data at the end of a project. After the generic steps of a workflow described already, we start to search for a fitting repository with respect to the conditions of the actual data and the whole project.Figure 4: Consultation workflow for data archiving.We developed a system of priorities that we work through step by step: at first, we check if we can identify a (1) domain-specific repository that fits with the given needs and conditions. In our case, this can be the LAC, or an external repository. If there is no solution, we check if there is a fitting (2) generic repository. In our case, this can only be an external repository, but, in the context of another data center, it could be a generic repository driven by the data center.In any case the results of the investigation need to be passed to the researchers: either we mediate a contact to an external repository, or we present information about the conditions of archiving data in the LAC.Conclusion The formal description of our workflows optimizes both consultations and active RDM. The workflows follow acknowledged standards as well as know-how and experiences made by data managers since 2013. Thanks to these procedures, we are able to deal with the diversity of RDM-requirements in the Humanities in an effective and standardized way and we can handle different consultations with the same or a similar request equally. This makes RDM measurable and comprehensible. Because of the abstract level of modelling, the workflows are generic and can be adapted in different contexts.In my talk, I will present more workflow models and in a more detailed way. Contextually, I will define RDM-requirements and -services based on active data management in a comprehensive way and show synergies between different needs and service structures. This will lead me to a plea for the necessity of measurable RDM-structures based on RDM done in expert centers by data manager, as opposed to the common approach of building and establishing contact points and training a mediating data manager based on recommendations of political and scientific committees and umbrella organizations that are not closely related to daily research and RDM.",patrick.helling@uni-koeln.de,Short Presentation
"Meneses, Luis (1); Martin, Jonathan (2); Furuta, Richard (1); Siemens, Ray (3)","1: Electronic Textual Cultures Lab, University of Victoria; 2: King’s College London; 3: Center for the Study of Digital Libraries, Texas A&M University",Analyzing Link Topology to Quantify the Degree of Planned Obsolesce in Online Digital Humanities Projects,Analyzing Link Topology to Quantify the Degree of Planned Obsolesce in Online Digital Humanities Projects,"degradation, abandonment, preservation, online digital humanities projects","Global, English, Contemporary, data, object, and artefact preservation, digital archiving, Computer science, Humanities computing",English,Global,Contemporary,"data, object, and artefact preservation, digital archiving","Computer science, Humanities computing","Many of the online projects in the digital humanities have an implied planned obsolesce –which means that they will degrade over time once they cease to receive updates in their content and software libraries (Fitzpatrick 2011). We presented papers at Digital Humanities 2017, 2018, and 2019 that explored the abandonment and the average lifespan of online projects in the digital humanities (Meneses and Furuta 2017), contrasted how things have changed over the course of a year (Meneses et al. 2018), and introduced a strategy for preservation by creating standalone software executables (Meneses et al. 2019). However, managing and characterizing the degradation of online digital humanities projects is a complex and pressing problem that demands further analysis.In this sense “planned obsolescence” is a nuanced designation —as there are many cases of successful projects in digital humanities that are shifting their focus from active development to data management (for example: http://cervantes.dh.tamu.edu). These are cases where a project’s online presence has not received updates for some time but its online tools are stable and continue to be accessed by its users. However, if updates are not applied to the infrastructure or content of a project over time web requests will eventually start generating errors on the server or the client —affecting the overall user experience (Nowviskie and Porter 2010). These are examples of why the rules for traditional resources do not fully apply and new metrics are needed to identify issues concerning online projects in the digital humanities.In this study we dive deeper into exploring the distinctive signs of abandonment to quantify the planned obsolesce of online digital humanities projects. In our workflow, we use each project included in the Book of Abstracts that is published after each Digital Humanities conference from 2006 to 2019. We then proceed to periodically create a set of WARC files for each project, which are processed using Python (van Rossum 1995) and Apache Spark (Apache Software Foundation 2017) to statistically analyze the retrieved HTTP response codes, number of redirects, DNS metadata and detailed examination of the contents and links returned by traversing the base node. This combination of metrics and techniques has allowed us to assess the degree of change of a project over time. As one of the results from our 2019 presentation, we claimed that the most important signature for degradation comes from the assessing the validity and overall health of the topology of links in a project. Thus, the focus of our study is analyzing this key signature.We acknowledge that research on the preservation of projects in the digital humanities is also carried out by other groups (Larrousse and Marchand 2019) (Arneil, Holmes, and Newton 2019). However, our study is different as it focuses on two points: first, identifying the signals of abandoned projects using computational methods; and second, quantifying their degree of abandonment. In the end, we intend this study to be a step forward towards better preservation strategies for the planned obsolesce of online digital humanities projects.","ldmm@uvic.ca, jonathan.d.martin@kcl.ac.uk, furuta@cse.tamu.edu, siemens@uvic.ca",Poster
"Miller, Yitzchak; Prebor, Gila","Bar Ilan University, Israel",From Metadata to linked Open Data and Wikidata : Yemenite Hebrew Manuscripts and Wikidata,From Metadata to linked Open Data and Wikidata : Yemenite Hebrew Manuscripts and Wikidata,"linked Open Data, Wikidata, Hebrew Manuscripts, Metadata","Asia, English, Contemporary, linked (open) data, manuscripts description, representation, and analysis, Humanities computing, Library & information science",English,Asia,Contemporary,"linked (open) data, manuscripts description, representation, and analysis","Humanities computing, Library & information science","The Hebrew Manuscripts catalogue in the National Library of Israel provides metadata for most Hebrew manuscripts in the world. Traditionally, library catalogues have served as a tool to manage library collections causing library catalogues to be data silos. In order to break down these metadata silos, information must be accessible and free. The semantic web, and in particular, linked open data, are initiatives that can turn library catalogues into a real part of the Internet. In order to explore the potential of linked data to this type of data we plan to convert a small part of the catalogue’s metadata of Hebrew Manuscripts in the NLI to Wikidata and create Wikidata items for each of the chosen manuscripts. We hope this will lead to an enrichment of the data and easy access to tools for querying and visualizing the collection. As a case study we have chosen Yemenite script manuscripts.","Isaac.Miller@biu.ac.il, gila.prebor@biu.ac.il",Poster
"Ciula, Arianna; Caton, Paul; Ferraro, Ginestra; Maher, Brian; Noël, Geoffroy; Vieira, Miguel","King's College London, United Kingdom",The place of models and modelling in Digital Humanities: Intersections with a Research Software Engineering perspective,The place of models and modelling in Digital Humanities: Intersections with a Research Software Engineering perspective,"Research Software Engineering, models, critical modelling","Comparative (2 or more geographical areas), English, Contemporary, data modeling, project design, organization, management, Humanities computing",English,Comparative (2 or more geographical areas),Contemporary,"data modeling, project design, organization, management",Humanities computing,"This paper1 aims to bridge Digital Humanities (DH) and Research Software Engineering (RSE) communities. It argues that the production of models is the core contribution of RSE to the epistemology of DH. We adopt an inclusive definition of models and modelling (see Ciula et al. 2018) which spans the whole range from ‘deformative’ to empirical modelling (see Smithies 2017: 168), including formal or predictive modelling (Joslyn and Turchin 1993), and the technical solutions produced in the process as well as the know-how, languages and documentation which accompany this production. From this wide perspective, models are also artefacts which can be studied across the history of science and of the humanities tradition (see Bod 2018) and in comparison with other modelling practices in science. RSE practice is grounded on a strong conscience of the experimental apparatus and the iterative critique of models built (and often deflated) for a purpose. The challenge is to recognise the idiosyncrasy and situatedness of modelling practices and artefacts while devising methods to expose the scalability of the underlying workflows and modelling processes.We reflect on the epistemology of DH from the practical perspective of our RSE lab - King’s Digital Lab (KDL)2 - and the research processes embedded in our Software Development Lifecycle (fig.1). The human element is at the core of the technical ecosystem we research and operate in. We acknowledge that KDL models and modelling are co-constitutive of human expertise, technical systems and operational methods, all aspiring to an environment conducive of open knowledge. This is not only in terms of development and management approaches via adoption of open standards, open source and exposure of open data but also, more fundamentally, in sharing (achievements and struggles around) our processes and in promoting open models. We will use project-specific examples, including data modelling and knowledge representation practices, to demonstrate how some of our research into model-making processes challenge the perception of the technical work of RSE within DH as a stale, mechanistic and uncritical procedural activity.Figure 1 KDL Software Development Lifecycle (SDLC) (King’s Digital Lab 2019) mapped to Agile DSDM project phases (Agile Business Consortium 2014).KDL operates within a rather unique context. It claims its origins in the pioneering work of colleagues at King’s College London working in applied computing in the Humanities already in the 1970s (see Short et al. 2012). However, the crossings between RSE and DH communities at King’s and internationally are only recently being highlighted and explored (e.g. Gold 2009; Smithies 2019; DHTech Group 2019). We argue that the study and building of models is one of the dimensions via which these crossings emerge more vividly with substantial epistemological implications and innovative ramifications for the field of DH as a whole.The core practice of research in DH is modelling (e.g. cf. McCarty 2005: 20–72; Buzzetti 2002; Beynon and McCarty 2006; Flanders and Jannidis 2015, 2018), which implies the translation of complex systems of knowledge or conceptual frameworks into computationally processable models or operational frameworks.Gooding (2003) claims that in experimental settings computational approaches are analogues to other processes of abstraction, measurement and contextual interpretation, whereby reduction of complexity is followed by expansion in the guise of a double funnel-shaped process (Gooding 2003: fig 13.4). We can trace these processes of reduction and expansion also in the RSE context, where, for example, operationalisation makes models formalised into snippets of code or software components. Other languages than translation into code play a role in the process, however.3The paper will reflect on models as artefacts of different kind expressed via a variety of languages, including but not limited to computational models, produced during several phases of the SDLC (see Ciula and Smithies, forthcoming) such as:negotiations around the meaning of the project units of analysis documented in diagrams and definitions which shape requirements and an agreed project language;paper or whiteboard sketches used to draft the solution architecture for a project in its feasibility assessment;wireframes and static mockups of user journeys;data models implementing the logical structure of a database;statistical models implemented with ad hoc algorithms and code or relying on tested formulas and existing libraries.While specific instantiations of models (for example in relational databases) can have a rather short life, in the RSE context and projects where they were designed and developed, they often represented innovative solutions which have a longstanding effect. Indeed, while models are temporary pragmatic solutions to address specific project challenges yet, at the scale KDL operates, they are the backbone of the team's tacit knowledge as well as the building blocks towards more generalisable and re-usable approaches. They mediate and bridge the layers of the Lab’s socio-technical system: team expertise, data and technical systems (fig. 2).Figure 2 Multilayered socio-technical system of the Lab where concentric circles denote co-constitution of team expertise, data and technical systems (Ciula and Smithies, forthcoming: fig. 5).While we can refine existing approaches to sustain and expose modelling efforts in SDLC cycles which rely on RSE best practices such as attention to documentation and re-use,4 there is also room for more innovative approaches, including a design first culture, workflow integration across RSE roles, the assessment and potential adoption of a set of modelling notation languages for the exposure mechanisms for our models.In alignment with a “critical modelling” approach (see Bode 2020), but also with a material culture and media literacy perspective, in this paper we aim to reflect upon models by looking at the wide epistemological implications of their production and use, at the responsibilities of modellers,5 at how models come to be and what effect they have in the resources they contribute to instantiate and hence in interpretative processes of expansion as well as reduction.Notes[1] A preliminary version of this paper was presented at the symposium Computational Text Analysis and Historical Change, held at Humlab, Umeå University (Sweden), 4-6 September 2019.[2] The authors currently cover different roles at King’s Digital Lab (KDL), a Research Software Engineering (RSE) team hosted by the Faculty of Arts & Humanities at King’s College London, which provides software development and infrastructure to departments in the Faculty of Arts & Humanities while also collaborating with Social Science & Public Policy as well as a range of external partners in the higher education and cultural heritage sectors.[3] Data modelling, for example, is informed if not driven by communication and collaborative reasoning around more or less standardised graphical representations and notations in phases of reverse engineering as well as design methods. Note that KDL intend and use design methods in a wide sense ranging from techniques of requirements elicitation in pre-project analysis to data modelling and wireframing in evolutionary development (see Bennet et al. 2005). Equally, the re-integration or expansion of modelling efforts into interpretative frameworks usually rely on verbal and visual language to document code, or to explain the results of an experiment (Ciula and Marras 2019: 39).[4] With this respect see, for example, the KDL checklist for assessment of digital outputs within the UK Research Framework Exercise (Ciula 2019).[5] Models contribute to define and redefine objects of study which come charged with layers of scholarship and analysis, with previous selections, bias and political as well as ethical responsibilities. As the creators of new memory regimes and intermediaries to the past engaged in modelling efforts which interact and affect the materiality of our objects of study (Ciula 2017a), we bear responsibilities (Ciula 2017b). In line with ongoing discussions around the representativeness and constraints of the digital archive (e.g. Dahlström 2010; Hitchcock 2013; Prescott 2015), Bode (2018, 2020) presented some lucid analysis around modellers’ responsibilities in digital literary studies by exposing the gaps that propagate from produced literary works we know of to material preserved in the analogue archive, to selections of works that make it into digital archives to further reductions in the creation of a corpus of analysis and, last but not least, in the application of statistical modelling techniques which dictate additional powerful yet limiting constraints if not contextualised critically within an interlocked chain of bias.","arianna.ciula@kcl.ac.uk, paul.caton@kcl.ac.uk, ginestra.ferraro@kcl.ac.uk, brian.maher@kcl.ac.uk, geoffroy.noel@kcl.ac.uk, jose.m.vieira@kcl.ac.uk",Long Presentation
"Hörmann, Richard; Schlager, Daniel","University of Salzburg, Austria",Enhanced Stand-off TEI Annotation with StoReCo: A generic approach with the use of RDF.,Enhanced Stand-off TEI Annotation with StoReCo: A generic approach with the use of RDF.,Stand off markup RDF Annotation,"Europe, English, Contemporary, data modeling, text encoding and markup language creation, deployment, and analysis, Humanities computing, Informatics",English,Europe,Contemporary,"data modeling, text encoding and markup language creation, deployment, and analysis","Humanities computing, Informatics","TEI-XML is the standard in digital text editing. The classic inline annotation with TEI quickly reaches its limits when it comes to complex annotations. In order to realize DH projects with such requirements on the dhPLUS platform currently developed at the University of Salzburg, a special form of stand-off markup is presented: It saves the original TEI file, converts the markup into RDF instructions, keeps the text and links the triples to an unlimited number of annotation levels. A unique identifier connects the different stages and facilitates presentation and search processes.","richard.hoermann@sbg.ac.at, daniel.schlager@sbg.ac.at",Poster
"van Zaanen, Menno; Trollip, Benito; Ramukhadi, Phathuthsedzo; Mlambo, Respect","South African Centre for Digital Language Resources, South Africa","Identifying relations between characters in Afrikaans, Tshivenḓa, and Xitsonga book ","Identifying relations between characters in Afrikaans, Tshivenḓa, and Xitsonga book ","African languages, literary analysis, named entity recognition, network analysis","Africa, English, Contemporary, electronic literature production and analysis, natural language processing, Literary studies",English,Africa,Contemporary,"electronic literature production and analysis, natural language processing",Literary studies,"The usefulness of computational linguistic tools, such as named entity recognition (NER) systems, in linguistic or literary studies of under-resourced languages is an area that is still relatively unexplored. We applied NER systems to one Afrikaans novel and two scanned dramas, one in Tshivenḓa and one in Xitsonga. Personal relations are identified through character name co-occurence in sentences and these relationships are visualized using Gephi, following the approach by Van de Ven et al. (2018). The research identified several practical problems: low quality OCR, low quality NER, limited amounts of NE and language specific issues.","menno.vanzaanen@nwu.ac.za, benito.trollip@nwu.ac.za, Phathutshedzo.Ramukhadi@nwu.ac.za, respect.mlambo@nwu.ac.za",Short Presentation
"Rominger, Gian Duri; O'Leary, John; Budak, Nick","Princeton University, United States of America",<em>DIRECT</em> - Digital Intertextual Resonances in Early Chinese Texts,DIRECT - Digital Intertextual Resonances in Early Chinese Texts,"china, phonology, text, tool","Asia, English, BCE-4th Century, concordancing and indexing, natural language processing, Asian studies, Linguistics",English,Asia,BCE-4th Century,"concordancing and indexing, natural language processing","Asian studies, Linguistics","In this talk, the creators will provide an introduction and demonstration of DIRECT (Digital Intertextual Resonances in Early Chinese Texts), a new digital tool designed to simplify phonological analysis of early Chinese texts. DIRECT enables users to quickly and easily identify homophonous and near-homophonous passages in ancient Chinese texts that may be obscured by graphic variation, uncovering hidden resonances and patterns.","gianr@princeton.edu, jo10@princeton.edu, nbudak@princeton.edu",Lightning
"Winter, Caroline; El Khatib, Randa; Arbuckle, Alyssa; Siemens, Ray","Electronic Textual Cultures Lab, University of Victoria","The Open Knowledge Program: Creating Space for Digital, Public Scholarship","The Open Knowledge Program: Creating Space for Digital, Public Scholarship","open scholarship, open knowledge, social knowledge creation, digital humanities, public humanities","English, North America, Contemporary, digital research infrastructures development and analysis, public humanities collaborations and methods, Humanities computing",English,North America,Contemporary,"digital research infrastructures development and analysis, public humanities collaborations and methods",Humanities computing,"DH2020 ProposalCaroline WinterRanda El KhatibAlyssa ArbuckleRay Siemens The Open Knowledge Program: Creating Space for Digital, Public ScholarshipIn Generous Thinking: A Radical Approach to Saving the University (2019), Kathleen Fitzpatrick argues that academics must reconsider themselves within the “larger ‘us’ that we together form,” rather than holding themselves apart from the wider community (8). How to enact more open, public work is not always obvious, however. Researchers often face barriers to engaging in open and public-facing scholarship, including lack of training, infrastructure, and technical and community support. The Open Knowledge Program at the University of Victoria’s Electronic Textual Cultures Lab (ETCL) facilitates intersections between the scholarly and public communities by supporting university and community researchers in creating open knowledge: “what open data becomes when it’s useful, usable and used” (Open Knowledge Foundation n.d.). In this paper, we discuss the trajectory of the program so far, share examples of participants’ contributions, and invite feedback and discussion about adapting the program for other contexts and its next steps.The ProgramThe Open Knowledge Program is based in the ETCL, a collaborative humanities research lab with a focus on open scholarship and a mandate that includes research, training, and service.The Program comprises three initiatives: the Open Knowledge Practicum (OKP), the Open Knowledge Practicum at the Digital Humanities Summer Institute (OKP@DHSI), and the Open Knowledge Residency (OKR). The OKP is a term-long program that welcomes researchers from UVic and the wider community into the lab to work on an open knowledge project, including contributing to Wikipedia. The OKP@DHSI supports the ETCL’s global community by inviting DHSI students and instructors into the lab for a condensed, three-day version of the OKP. The OKR offers graduate students in any discipline at UVic an intensive, week-long residency in the lab to conduct thesis or doctoral research and share findings in an open venue.The InterventionJust as digital scholarship has moved from the periphery of the Humanities towards its centre, open scholarship is increasingly recognized as the new scholarly mode. This transition from closed to open is driven partly by necessity — the costs of the current subscription model of scholarly communication being unsustainable for many research libraries — and partly by researchers driven to engage with the broader community and universities striving to fulfill their public missions (CARL–ABRC 2010; O’Gara 2019; Suber 2019). It is also driven by opportunity, since digital technologies have made it possible to share scholarly work widely with academic and public readers, greatly extending its “reach” (Maxwell 2015, 2).Digital tools allow researchers to engage with new types of research materials, new tools and methodologies, and new modes of communication, but require skills that are not part of traditional Humanities curricula, including collaboration. These digital skills are increasingly recognized as essential for scholarly work and beyond, particularly for emerging scholars (Brier 2012; El Khatib, Arbuckle, and Siemens 2019; IPLAI 2013; Jakacki and Faull 2016; Lewis et al. 2015; MLA 2014; NEH 2016; Reid 2012). The Open Knowledge Practicum draws on the pedagogical model of the practicum, in which skills and knowledge are applied in practice. Practicums are common in Education and professional fields including nursing and clinical psychology, but much less so in the Humanities and Social Sciences. Although practicums are usually part of a larger curriculum, the OKP employs this model as a standalone program, in which researchers plan, develop, and create their own research projects. In doing so, participants put their subject matter expertise and digital skills into action by creating open knowledge resources while gaining experience working in a DH lab as part of a research team. Participants develop their own projects in consultation with the lab team and our colleagues in the Library and across campus as needed. The lab team also provides just-in-time collaborative learning as needed in the use of specific software applications, digital tools, and project management. Honorariums for participants comprise registration in a DHSI course, providing further opportunity for learning, putting learning into practice, and building community.In its focus on open scholarship as praxis, the Open Knowledge Program complements the “apprenticeship model that dominates graduate training and socialization in the humanities and elsewhere across the university,” which focuses tightly on skills necessary to institutional frameworks (Bartha and Burgett 2015, 33­–4). These frameworks, however, leave little space for digital and public scholarship: review, promotion, and tenure guidelines, for example, tend to discourage open scholarship in favour of more traditional forms of scholarly communication (Alperin et al. 2018). In their study of best practices for supporting digital scholarship, Lewis, Spiro, Wang, and Cawthorne note that physical space in which to work, collaborate, and learn is key (2015, 2). Lewis et al. also find that “collaborative competencies” and “learning mindsets,” comprising “creativity, curiosity, and an enthusiasm for learning” are as important for digital scholarly work as technical skills (2015, 2). Building regular lab hours into the Program’s structure provides participants with a shared physical workspace as well as a highly collaborative community of practice that facilitates social knowledge creation (Burke 2000). Participants in the Program determine the subject and scope of their project, pursuing interests that intersect with their academic work or study, or not. In this way, the Program creates intellectual space and scheduled time for curiosity, exploration, and creativity, space that is often difficult to find within a university’s institutional structure (Bartha and Burgett 2015). Many projects focus on social justice issues such as wealth inequality, Indigenous knowledge systems, international LGBTQ rights, and histories of oppressed groups (ETCL 2020; El Khatib et al. 2019).The Open Knowledge Program has evolved over the past few years to support standalone projects as well as multi-term endeavours and components of large, grant-funded research initiatives with faculty partners and their student research assistants. We anticipate that describing the structure and goals of the Open Knowledge Program and highlighting the work of its participants will open a discussion of the Program’s future directions and provide a model for other digital scholarship centres that are invested in public scholarship and open knowledge production.","winterc@uvic.ca, khatib@uvic.ca, alyssaa@uvic.ca, siemens@uvic.ca",Long Presentation
"Olive, Jenn","Georgia State University, United States of America","<em>Never Alone</em>, Never Finished: Defining <em>Never Alone’s</em> World Games Genre as Ethical Alternative to Empathy Games","Never Alone, Never Finished: Defining Never Alone’s World Games Genre as Ethical Alternative to Empathy Games","Never Alone, world games, survivance, empathy games","Global, English, North America, Contemporary, electronic literature production and analysis, mixed-media analysis, Games studies, Literary studies",English,"Global, North America",Contemporary,"electronic literature production and analysis, mixed-media analysis","Games studies, Literary studies","In this paper, I will investigate the genre of world games through the game that launched the genre into the spotlight, Never Alone. This debut title for the first indigenous-owned game company in the United States, Upper One Games, uses this genre to categorize itself within the rest of game experiences, which is significant given the genre's lack of use in games at large as well as what it signifies in its difference from established genres. Based on my investigation, I will argue that the potential importance of the genre as defined through Never Alone comes from its opportunity to answer to the ethical concerns of the empathy games genre.",jolive1@gsu.edu,Short Presentation
"Schöch, Christof (1); van Dalen-Oskam, Karina (3); Antoniak, Maria (4); Jannidis, Fotis (2); Mimno, David (4)","1: University of Trier, Germany; 2: University of Würzburg, Germany; 3: Huyghens ING and University of Amsterdam, The Netherlands; 4: Cornell University, USA",Replication and Computational Literary Studies,Replication and Computational Literary Studies,"replication, reproduction, computational literary studies","Global, English, Contemporary, meta-criticism (reflections on digital humanities and humanities computing), text mining and analysis, Literary studies",English,Global,Contemporary,"meta-criticism (reflections on digital humanities and humanities computing), text mining and analysis",Literary studies,"Technical Note A PDF version of this contribution is available at Humanities Commons: https://hcommons.org/deposits/item/hc:30439 Several versions of this contribution in structured formats, including figures and bibliographical data, are available from Zenodo: http://doi.org/10.5281/zenodo.3893428 Panel OverviewThe ""replication crisis"" that has been raging in fields like Psychology (Open Science Collaboration 2015) or Medicine (Ioannidis 2005) for years has recently reached the field of Artificial Intelligence (Barber 2019). One of the key conferences in the field, NeurIPS, has reacted by appointing 'reproducibility chairs' in their organizing committee 1. In the Digital Humanities, and particularly in Computational Literary Studies (CLS), there is an increasing awareness of the crucial role played by replication in evidence-based research. Relevant disciplinary developments include the increased importance of evaluation in text analysis and the increased interest in making research transparent through publicly accessible data and code (open source, open data). Specific impulses include Geoffrey Rockwell and Stéfan Sinclair's re-enactments of pre-digital studies (Sinclair and Rockwell 2015) or the recent replication study by Nan Z. Da (Da 2019). The paper has been met by an avalanche of responses that pushed back several of its key claims, including its rather sweeping condemnation of the replicated papers. However, an important point got buried in the process: that replication is indeed a valuable goal and practice. 2As stated in the Open Science Collaboration paper: ""Replication can increase certainty when findings are reproduced and promote innovation when they are not"" (Open Science Collaboration 2015, 943).As a consequence, the panel aims to raise a number of issues regarding the place, types, challenges and affordances, both on a practical and on a policy or community level, of replication in CLS. Several impulse papers will address key aspects of the issue: recent experience with attempts at replication of specific papers; policies dealing with replication in fields with more experience in the issue; conceptual and terminological clarification with regard to replication studies; and proposals for a way forward with replication as a community task or a policy issue.Contribution 1: ""A typology of replication studies"", by Christof SchöchThis contribution aims to provide orientation about the range of existing replication studies, based on a simple typology. The typology describes the relationship between an earlier study and its replication in terms of four key variables: the research question, the method of analysis (including the implementation of that method) and the dataset used. 3 For each of these variables, a replication study can attempt to operate either in the same way as the previous study, or in a different way. Note that the typology is not meant to establish these distinctions in a purely binary fashion: rather, as data or methods are never entirely identical or completely different from the earlier study, the extreme points in the typology are meant to open up a gradient of pratices. In addition (and this aspect might be unique to the Digital Humanities), replication studies can be described as involving crossing the boundary between non-digital and digital research or between qualitative and quantitative research.[Figure 1: Typology of repeating research]At the most fundamental level, such a typology structures the field and provides a clearly-defined terminology and systematic relations between the various types. For example, replication vs. reproduction or re-analysis vs. follow-up research. Such a shared understanding is useful because each type of replication study comes with its own objectives, requirements and challenges as well as their own place and function in the research process. A replication study strictly repeating key aspects of an earlier study will be most useful in reviewing and quality assessment, while follow-up research more loosely modeled after an earlier study may instead have important methodological implications or lead to knew domain knowledge. This is true despite the fact that in reality, there is more to consider than a few binary categories when describing a given replication study.But understanding replication through such a typology can have an impact on the field of DH in a number of additional ways. It can provide guidance when publishing research and help clarify what needs to be provided (in terms of data, code and contextualization in prose) in order for a study to be amenable to a specific type of replication. It can help assess the merits and limitations of a given replication study to assess whether, given the stated objectives of the authors, they have employed a suitable type of replication strategy. And it can support designing a replication study and clarify what data, code and contextual information needs to be obtained or reconstructed in order to perform a specific type of replication. 4Beyond this, such a typology can contribute to better define the relationship between replication in the strict sense and related efforts like benchmarking and evaluation studies. Finally, because such a typology makes it easier to identify similar studies across disciplinary boundaries, it may help us as a field learn more quickly from other fields with a longer tradition in (specific types of) replication studies. In this way, such a typology of replication studies can contribute to establishing replication as a well-understood part of Computational Literary Studies.Contribution 2: ""Replication to the Rescue: Funding Strategies"", by Karina Van Dalen-OskamThe Dutch Research Council (NWO) is the first funding agency to take the initiative for a pilot programme for Replication Studies. Their aim is ""to encourage researchers to carry out replication research. NWO wants to gain experience that can lead to insights into how replication research can be effectively included in research programmes. That experience should also lead to insights into and a reflection on the requirements that NWO sets for research in terms of methodology and transparency."" 5The first two rounds of funding were in 2017 and 2018, and were aimed at the Social Sciences and Medical Sciences. In the third round in 2019, the Humanities were included.This was done after a heated discussion in Nature between Rik Peels and Lex Bouter (chair of the Replication Studies Programme Committee) on the one hand, and Bart Pender, Sarah de Rijcke and J. Britt Holbrook on the other. Peels and Bouter (2018) started of with a note titled ""Humanities need a replication drive too"". De Rijke and Penders (2018), both scholars from Science and Technology Studies, countered with the call to ""Resist calls for replicability in the humanities"". They argue that quality criteria are crucially different in the humanities and the sciences. 6NWO went ahead with including the humanities in the call for replication studies, stating they are aware that not all humanities research is suitable for replication. NWO ""expresses no preference or opinion about the value of various methods of research. Where possible it wants to encourage and facilitate the replication of humanities research: this should certainly be possible in the empirical humanities."" 7In March 2020, seven proposals were awarded funding, but none of these can be called typical Humanities projects. 8How many submissions were received from humanities applicants - did scholars indeed resist, as Pender and De Rijcke advised? And in a wider context: How do Dutch Humanities scholars evaluate the new possibility? And does this agree with the reception in the growing and very active Dutch Digital Humanities community?In my short impulse paper, I will reflect on what we can learn from the explicit invitation to the humanties to apply for funding for replication studies. What does this tell us about the status of humanities research in the Netherlands, and more specifically about the role of the Digital Humanities? I will pay special attention to the opportunities these developments may have for Computational Literary Studies. Should we consider the situation as ""Funding Strategies to the Rescue: Replication"", so a turning around of the title of my talk?Contribution 3: ""Replication of quantitative and qualitative research - a case study"", by Fotis JannidisLiterary studies always had an empirical side - 'empirical' in the broader sense, that claims and counterclaims are substantiated by referring to specific parts of texts. These text segments are regarded as indicators which in their sum make a more general point plausible, for example the use of specific terms to validate a hypothesis about a text. Therefore, the concept of replication warrants a wider understanding in Computational Literary Studies. The prototypical center is the quantitative replication of quantitative research, but it also includes quantitative replication of qualitative philological research: Using the same indicators to validate the same hypothesis but moving the research into an empirical framework. Seen in the context of the discussion of mixed methods, this is a specific case of ‘triangulation’. Triangulation refers to “the application of different data analysis methods, different data sets, or different researchers’ perspective to examine the same research question or theme” (Bergin 2018, 29). But here, data sets and data analysis methods overlap strongly, while the research framework is changed from hermeneutic to quantitative.Our case study is an attempt to replicate research on the complexity of language in German dime novels, published by Peter Nusser (Nusser 1981), and it demands both kinds of replication. Nusser describes the language of dime novels on three levels: vocabulary, syntax, and phrases. The work on vocabulary and syntax is quantitative, while the analysis of phrases is qualitative. The replication of the quantitative parts is made more difficult by the fact that the results which Nusser reports have actually been produced by another author in the context of an unprinted exam thesis which seems to be lost for the moment. So a lot of information is missing, and we can only make educated guesses: the exact corpus design (for high literature only the authors are given and for dime novels only the series), the strategies of tokenization and sentence splitting, the exact formula for calculating specific values, etc. The qualitative research is enumerating many phrases which are seen as examples of clichés and there is no explicit comparison with high literature. So a quantification must try to operationalize the concept of cliché and then compare retrieval results between dime novels and high literature.As is well known in Computational Literary Studies, operationalization as an instance of formal modeling usually covers some aspects that are part of the intuitive notion, while others are excluded for the time being and it is one goal to reduce the loss (Moretti (2013); for a counterposition see Underwood (2019, 181)). In a replication, the loss may be responsible for the difference in outcome. In view of all these difficulties it could seem an unnecessary endeavor to replicate the research, but Nusser’s study had a huge influence on the assessment and evaluation of popular literature in German studies for almost four decades.Contribution 4: ""Reliable methods for text analysis"", by Maria Antoniak and David MimnoIf we are to make reproducible computational claims about literary texts, we need methods that lend themselves to robustness and reliability. Here we focus on the case study of word embeddings, which analyze collections of documents and produce numeric representations of words. Although these methods are powerful, they are also at high risk for problems with reproducibility: they are complicated enough to be essentially ""black boxes"", yet they are also known to be highly sensitive to text curation choices, parameter settings, and even random initializations (Antoniak and Mimno 2018). How can we assure researchers and their audiences that seemingly small changes would not alter or even reverse their findings?Embedding vectors are useful for their ability to operationalize thick cultural concepts. For example, the resulting vectors have been used to measure shifts in word meaning over time and geographic areas (e.g. Hamilton, Lescovec, and Jurafsky (2016); Kulkarni, Perozzi, and Skiena (2016)). Several studies have shown that embeddings can encode gender biases by probing embedding spaces using carefully chosen seed words (Gordon and Van Durme (2013); (Bolukbasi et al. 2016a); Caliskan Islam, Bryson, and Narayanan (2016)). Subsequent work in natural language processing has focused on removing biases from an embedding model (Bolukbasi et al. (2016b); Sutton, Lansdall, and Cristianini (2018)). In this context, the concern is the downstream impact of bias on systems that use embeddings, but similar work can also be motivated from an upstream perspective, as a means of studying bias in collections.Researchers from the humanities and social sciences use embeddings to provide quantitative answers to otherwise elusive political and social questions about the training corpus and its authors (e.g. Kozlowski, Taddy, and Evans (2019)). These bias detection techniques were originally intended to measure the bias encoded in a trained embedding; they were not originally tested to measure the bias of a corpus and make comparisons between corpora.We probe the stability of these measurements by testing two popular bias detection methods ( Bolukbasi et al. (2016a); Caliskan Islam, Bryson, and Narayanan (2016)) on sets of automatically constructed seed sets. These sets were constructed by randomly selecting a target term and then including its N nearest neighbors in the set; this process more closely approximates a real seed set, constructed by a scholar interested in a particular concept, than a random set of seeds. We find that bias detection techniques via word embeddings are susceptible to variability in the seed terms, in both their order (alternative pairings of seeds from two sets can significantly change the ability of the method to capture a single bias subspace) and semantic similarity (the more similar seeds set are to each other, the more difficult it is to measure their biases). If done carefully, bias detection using embeddings is feasible even for small, subdivided collections and can provide a promising tool for differential content analysis, but we encourage error analysis of the seed terms.We further highlight a central inconsistency in these bias detection methods. While these methods seek to measure biases in datasets, the researcher-selected seeds themselves can contain a variety of biases. For example, the seeds used for racial categories often include lists of names that are ""African American"" or ""European."" Such lists can be both reductive and essentializing. In addition, some seed sets contain confounding terms, e.g., contain a gendered term in a seed set for ""domestic work"" that is then used to measure gender bias. If the seed set for ""domestic work"" appears closer to the gender that it contains, it will be impossible to say whether that bias exists because of the training corpus or because of the inclusion of the gendered seed.This case study highlights the reversal in perspectives when techniques from natural language processing and machine learning are re-purposed for studies of specialized datasets. Some working assumptions from the machine learning community (e.g. large size of training set) are broken in the humanities context, where datasets are non-expandable and are the primary focus of the study, rather than a generalized training set for downstream applications. The stability and robustness of these repurposings should not be assumed but rather should be reanalyzed for the particular new contexts.","schoech@uni-trier.de, karina.van.dalen@huygens.knaw.nl, maa343@cornell.edu, fotis.jannidis@uni-wuerzburg.de, mimno@cornell.edu",Panel
"Tayler, Felicity (1); Simpkin, Sarah (1); Mitchell, Marjorie (2); Crompton, Constance (1); Shearer, Karis (2); Lincoln, Matthew (3); Proulx, Mikhel (4); Goodchild, Meghan (5)","1: University of Ottawa, Canada; 2: University of British Columbia Okanagan; 3: Carnegie Mellon University Libraries; 4: Concordia University; 5: Queen’s University and Scholars Portal, Ontario Council of University Libraries",<em>Making Research Data Public: Workshopping Data Curation for Digital Humanities Projects</em><em></em>,Making Research Data Public: Workshopping Data Curation for Digital Humanities Projects,"data communities, research data management, data curation, digital scholarship, best practices","English, North America, Contemporary, data publishing projects, systems, and methods, digital ecologies and digital communities creation management and analysis, Library & information science",English,North America,Contemporary,"data publishing projects, systems, and methods, digital ecologies and digital communities creation management and analysis",Library & information science,"A lack of formal training opportunities for data curation in multi-site DH teams means that the data produced in these teams is in danger of being lost! This four-hour workshop will cover all areas of data management including: IP permissions and informed consent, data collection, metadata standards, file sharing, preservation (data deposit), and data sharing through the open data spectrum of access. Participants will work on their own data curation challenges in break-out sessions and with reference to case study examples presented by a panel of DH scholars and digital asset management specialists: Constance Crompton (uOttawa), Karis Shearer (UBCO), Matthew Lincoln (Carnegie-Mellon U), Mikhel Proulx (Concordia U and Indigenous Digital Art Archive), Meghan Goodchild (Queen’s U and Scholars Portal). The lesson plan is designed and delivered by Felicity Tayler (uOttawa), Sarah Simpkin (uOttawa), and Marjorie Mitchell (UBCO).This workshop is a good preparation for researchers who must create a data management plan to comply with funding agency requirements. The workshop arises at a moment when DH researchers have greater access to funding to support large-scale multi-partner projects with diverse digital assets. The manifold nature of DH, and its reflexive challenges to culturally imposed power imbalances in digital systems presents unique challenges for data curation. Responding to the conference thematic of cultural and disciplinary intersections, this workshop proposes that DH is one of the social and conceptual spaces where the informal networks of international “data communities” arise through acts of data curation and sharing (Cooper and Springer). Our approach to data curation recognizes that data communities are multilingual and multi-cultural just as they cross epistemological and disciplinary lines.","ftayler@uottawa.ca, sarah.simpkin@uottawa.ca, Marjorie.Mitchell@ubc.ca, constance.crompton@uottawa.ca, karis.shearer@ubc.ca, mlincoln@andrew.cmu.edu, mikhel.proulx@concordia.ca, meghan.goodchild@queensu.ca",Workshop/Tutorial 4
"Bentley, Patricia",York University,Intersections of the Cultural Kind: Public Digital Humanities and the Museum,Intersections of the Cultural Kind: Public Digital Humanities and the Museum,"museum, data analysis, methodology","Asia, English, North America, 19th Century, 20th Century, Contemporary, cultural analytics, data modeling, Galleries and museum studies",English,"Asia, North America","19th Century, 20th Century, Contemporary","cultural analytics, data modeling",Galleries and museum studies,This presentation examines a recent research intervention that used data mining and data mapping and visualization methods to assess how visitors in a museum of Islamic art were making sense of their encounters with patterned works of art in the galleries.,bentley.pbentley@gmail.com,Short Presentation
"Mapes, Kristen; Moll, Ellen; Petersen, Andy Boyles","Michigan State University, United States of America",Bringing Newcomers into the Fold: Faculty Development through Values-Driven DH Pedagogy,Bringing Newcomers into the Fold: Faculty Development through Values-Driven DH Pedagogy,"faculty development, curriculum development, training","English, North America, Contemporary, curricular and pedagogical development and analysis, open access methods, Education/ pedagogy",English,North America,Contemporary,"curricular and pedagogical development and analysis, open access methods",Education/ pedagogy,"A pervasive goal of DH is to bring new and curious faculty into the fold in a sustained way. At Michigan State University, we have developed a highly successful DH Pedagogy Learning Community to help new-to-DH instructors effectively integrate DH into their courses. Key to faculty engagement was our focus on how DH values such as collaboration, community, inclusion, public humanities, and experimentation aligned with their own priorities in teaching. We are now launching an OER that includes the curriculum for this values-driven learning community that can be used by either individual instructors or anyone wishing to lead their own discipline-agnostic learning community for new-to-DH faculty.","kmapes@msu.edu, mollelle@msu.edu, andyjp@msu.edu",Lightning
"Kim, Hoyeol (1); Ives, Maura (2)","1: Texas A&M University, United States of America; 2: Texas A&M University, United States of America",Colorization of Illustrations in Charles Dickens’ Novels Using Deep Learning,Colorization of Illustrations in Charles Dickens’ Novels Using Deep Learning,"Colorization, Deep Learning, Illustration, Victorian Literature, Charles Dickens","Global, Europe, English, North America, 19th Century, artificial intelligence and machine learning, digital art production and analysis, Education/ pedagogy, Literary studies",English,"Global, Europe, North America",19th Century,"artificial intelligence and machine learning, digital art production and analysis","Education/ pedagogy, Literary studies","Charles Dickens dedicated himself to the development of Victorian visual culture by actively employing illustrations in his fiction. For Dickens, illustrations were a way to attract readers and boost sales, as demonstrated by the commercial success of The Pickwick Papers (1837), as well as a crucial element of artistic expression. All but two of Dickens’s novels were illustrated, and his involvement in every stage of the illustration process was well documented. Dickens collaborated closely with his illustrators by providing detailed instructions: he provided specific colors for his illustrations, although the illustrations would be printed in black and white, and he intentionally positioned each illustration in a specific location in his serials in order to communicate details and emotions effectively with his readers. Scholars of Dickens’s works generally understand the illustrations to be integral to the text.Given the expense of printing illustrations in color, most of the illustrations in Dickens’s work were printed in black and white. However, the hand-colored illustrations in A Christmas Carol (1843) demonstrate both Dickens’s interest in providing color when he could, and the significance of color in interpreting design elements as well as shaping interpretation of Dickens’s text. Because colorizing Dickens’s illustrations has the potential for enhancing the reader’s understanding of the text, and for opening up new interpretive possibilities, it has pedagogical implications that we wish to explore.Our project is the first deep learning colorization venture in Victorian era media. We will colorize all of the illustrations in several of Dickens’s major novels, using the pix2pix model based on cGANs with Kim’s Victorian400 dataset as a research method. The Victorian400 dataset, published as an open data source, is a collection of colorful illustrations painted with nineteenth-century palettes. By using Victorian paintings and hand colored illustrations as a training set, our research methods make it possible for Dickens’s illustrations to be viewed with a Victorian color palette, approximating the color choices that a contemporary audience would have expected. We will present two groups of students with both the colorized and black and white illustrations and ask them to evaluate each in terms of design and in terms of their effect on their engagement with the text. Colorization with deep learning carries possibilities of misrepresentation or misinterpretation, which can be used to clarify students’ preconceptions about the nature or role of illustration and to spur creative responses to the text; it can also provide interest, anticipation, and imagination for readers. As a pedagogical tool, we hope that colorization using deep learning will improve students’ ability to think critically about the intersection of text and image while promoting deeper understanding and interest in Dickens’s works, and lay the groundwork for experimentation with deep learning colorization of other illustrated works by Victorian authors.","elibooklover@gmail.com, m-ives@tamu.edu",Poster
"Mallen, Enrique (1); Meneses, Luis (2)","1: Sam Houston State University, United States of America; 2: University of Victoria, Canada",Using computer vision to identify graphic elements in Picasso’s poetry,Using computer vision to identify graphic elements in Picasso’s poetry,"computer vision, graphic elements, poetry","Europe, English, North America, 20th Century, image processing and analysis, manuscripts description, representation, and analysis, Art history, Humanities computing",English,"Europe, North America",20th Century,"image processing and analysis, manuscripts description, representation, and analysis","Art history, Humanities computing","Pablo Picasso started writing in 1935. The onset of this new endeavor is said to have coincided with a devastating marital crisis—a financially risky divorce to be exact—decreasing substantially his pictorial output. Writing now became his al­ternative outlet. Not surprisingly, Picasso’s poetry is quite visual. While other poets in the past have added graphic elements to enhance their poems, their use in Picasso is of special importance as they are an essential component of his writing. Furthermore, the method he used in his poetic compositions is essentially combinatorial in nature, with words co-occurring often in unexpected ways, this relates to his collage technique during Cubism. Examples of these features are shown in figures 1 and 2.Figure 1: P. Picasso, OPP35-138, 1935Figure 2: P. Picasso, OPP36-138, 1936.Among the graphic elements Picasso includes in his poems are dashes, brackets, arrows, lines, blots, etc.  The combinatorial nature of his writing highlights the interconnections between words and graphic elements (Mallen 2003). This combination of unique graphic and verbal components in Picasso’s poetry leads us to view the poems as a set distinct layers of text and images that form the layout of a document—making it quite hard to define different boundaries while providing an important contribution to the interpretation of the layers of text. Additionally, Picasso's poems are often revisited multiple times, and he kept a record of all the changes from one state to another at various moments of their creation.  Each layer with new additions and deletions is carefully dated by the author, indicating the importance that each of them had for him. The “final” published copy is artificial in the sense that the printed version has been subjected to a frozen linearized transcription (often carried out by Picasso’s secretary, Jaime Sabartés) and fails to portray the information in the additional layers that can be found in the original manuscripts which fortunately have been preserved.We previously concluded that there is a close correlation between graphic elements and the verbal context in which they occur (Meneses and Mallen 2017) and offered a solution to encode graphic features and stratified text in machine-readable form (Mallen and Meneses 2019). However, identifying and encoding the graphical elements in the corpus of poems can be a laborious and intensive process. In this paper we will present a new approach that uses OpenCV, a library of programming functions aimed at real-time computer vision (OpenCV 2019), to identify the graphic elements in Picasso’s poetry. More specifically, we will present our findings on training the models, and discerning between false positives and negatives. This analysis will get us closer to understanding how the presence of graphic elements in a specific line of a poem affects the interpretation of the word string in that line and of the poem as a whole. Our final goal is to provide a systematic encoding of both the text and the graphic elements so that their mutually dependent interpretation may be properly evaluated.","enriquemallenphd@gmail.com, ldmm@uvic.ca",Short Presentation
"Toscano, Maurizio (2); Rabadán, Aroa (3); Ros, Salvador (1); González-Blanco, Elena (1)","1: Universidad Nacional de Educación a Distancia (UNED), Spain; 2: Universidad de Granada; 3: Universidad Complutense de Madrid",Evolución y escenario actual de las Humanidades Digitales en España,Evolución y escenario actual de las Humanidades Digitales en España,"Spain, Digital Humanities, digital infrastructures","Europe, Spanish, Contemporary, database creation, management, and analysis, History of science, Humanities computing",Spanish,Europe,Contemporary,"database creation, management, and analysis","History of science, Humanities computing","IntroducciónLas Humanidades Digitales (HD) se han convertido en un campo de interés en España, especialmente en la última década, a pesar de haber llegado más tarde que en la mayoría de los demás países europeos. De hecho, representan una tendencia destacada en la investigación, ya sea como campo de estudio que como tema de financiación preferente. Al mismo tiempo, por su novedad, están siendo objeto de escrutinio por parte de la comunidad investigadora y de las instituciones gubernamentales que financian la investigación. El objetivo del estudio ha sido identificar a los investigadores que trabajan en el campo de las HD en España y explorar su financiación, sus afiliaciones institucionales, las temáticas de investigación y los recursos digitales desarrollados.En el pasado se han promovido iniciativas similares, que han producido mapeos centrados en el ámbito internacional o nacional, algunos de los cuales siguen disponibles en línea, con diferencias que van desde la cobertura geográfica hasta el tipo de datos mapeados (Ortega; Eunice-Gutiérrez, 2014; Romero-Frías; Del-Barrio-García, 2014). Otros estudios sobre este tema han optado por un enfoque diferente, utilizando la bibliografía u otras fuentes para identificar las etapas más relevantes en la evolución y consolidación de esta disciplina en España (Rojas-Castro, 2013; González-Blanco, 2013; Spence; González-Blanco, 2014; Baraibar-Echeverria, 2014). El presente trabajo no pretende ser una revisión histórica exhaustiva, sino ofrecer una visión complementaria y actualizada del panorama de las HD en España, tomando en consideración datos recientes y fuentes de información no explotadas anteriormente.Materiales y métodosA continuación se describen los resultados de la investigación sobre el estado actual de las infraestructuras de investigación digital en España, entendidas como la combinación e integración entre los recursos de información digital, las herramientas analíticas y de visualización y la comunidad activa de investigadores, colaborando a través de proyectos de investigación, financiados por el sector público o privado. Por esta razón, hemos subdividido el objeto de la investigación en cinco entidades principales, concretamente: investigadores, proyectos, recursos, bibliografía y cursos de posgrado.Nuestra metodología de recopilación de datos ha tenido una doble vertiente. Por un lado, hemos seleccionado manualmente información disponible en línea a partir de congresos, seminarios, convocatorias temáticas, mapas participativos, etc. Por el otro, hemos extraído información a partir de bases de datos existentes: publicaciones científicas en el campo de las HD presentes en Dialnet y en ÍnDICEs-CSIC y proyectos de investigación financiados a través de la Agencia Estatal de Investigación, seleccionados a través de una serie de palabras claves (Figura 1).El volumen total de registros recopilados ha sido 1.359, distribuidos de la siguiente manera: 577 investigadores; 368 proyectos; 88 recursos; 9 cursos de posgrado y 8 revistas científicas. El conjunto de datos analizados se encuentra disponible en Acceso Abierto (https://doi.org/10.5281/zenodo.3893546), junto con los Jupiter Notebooks y el código Python para reproducir los análisis.Figura 1. Fuentes de datos utilizadas en la investigación.ResultadosEntre los investigadores identificados, 305 son varones (52,9%) y 272 mujeres (47,1%): una proporción que, comparada con la proporción de género entre los investigadores de España en todas las disciplinas (61,2% de varones y 38,8% de mujeres) o limitada a las Humanidades (59,8% de varones y 40,2% de mujeres), refleja una presencia femenina significativamente mayor de la esperada.La clasificación de los investigadores por disciplinas, 19 en total, muestra una amplia variedad, con una clara prevalencia de filólogos (36%), seguidos por historiadores (16,5%) e informáticos (10,8%).El análisis de las conexiones entre disciplinas y temas de investigación (Figura 2) revela cinco grandes grupos disciplinarios: Historia (32% de los nodos), Filología (24%), Comunicación (20%), Ciencias de la Computación (17,3%) y Documentación (6,7%). Las Ciencias de la Computación, a pesar de ser la cuarta comunidad en términos de nodos, resulta ser la disciplina relacionada con el mayor número de temas de investigación (13,3%).Figura 2. Análisis de redes de disciplinas y temas de investigación. El tamaño de los nodos corresponde al número total de conexiones, el color a la comunidad y el grosor de las aristas al grado de conectividad entre parejas de nodos.Según la afiliación institucional, la mitad de los investigadores (49,2%) pertenece a un total de nueve instituciones, mientras que la otra mitad se encuentra dispersa entre 84 diferentes centros (http://sl.ugr.es/DHmap).La afiliación a departamentos (Figura 3) muestra un patrón mucho más variado en cada institución: por ejemplo, el peso relativo de la Filología es mayor en la Complutense de Madrid o en Santiago de Compostela con respecto a Granada, donde por otra parte se aprecia mucha variedad, con investigadores procedentes de 13 departamentos.Figura 3. Peso relativo de cada departamento en los nueve centros de investigación principales. Para favorecer la comparación, se ha utilizado un histograma apilado.El análisis de la inversión en 337 proyectos de investigación en los últimos 25 años (Figura 4) permite avanzar una periodización en tres fases en la consolidación de las HD en España: 1993-2003; 2004-2014; 2015-2019. La financiación media por proyecto ha sido de 64.313€, pero la mediana se encuentra en 42.350€; el 5% de los proyectos ha recibido hasta 5.000€ (micro-proyectos) y el 90% menos de 100.000€.Figura 4. Total de proyectos de investigación en HD y financiación recibida durante el período 1993-2019.Entre las fuentes de financiación, el rol del ministerio es predominante, con 77% de las propuestas financiadas y 72% de los recursos asignados, a pesar de la gran variedad de organismos implicados, hasta 26, y de un 10% de recursos procedentes del sector privado.Disciplinas como la Filología, la Lingüística y la Biblioteconomía evidencian una tradición más larga que otras, como la Historia, la Arqueología y la Historia del Arte, en el desarrollo de recursos digitales para la investigación (Figura 5). Los artefactos desarrollados con mayor frecuencia (72,4%) son diferentes tipos de bases de datos (bibliotecas digitales, catálogos, repositorios, etc.), mientras que a partir de 2014 se observa un cambio hacia instrumentos más analíticos o participativos. La sostenibilidad de las plataformas en le tiempo se ha garantizado principalmente mediante la financiación en serie de proyectos del Programa Estatal de I+D+i.Figura 5. Recursos digitales clasificados según disciplina y tipología.ConclusionesA modo de conclusión, podemos destacar los siguientes aspectos: (1) la mayoría de las evidencias detectadas por otros estudios se han confirmado numéricamente; (2) los análisis cuantitativos de la financiación, una dimensión prácticamente inexplorada en las Humanidades, han demostrado ser extremadamente valiosos en la valoración de la evolución histórica de una disciplina científica; (3) se han establecido nuevas métricas y valores que constituyen una base de referencia para monitorear la evolución de las HD en España y favorecer las comparaciones, tanto a lo largo del tiempo como con otros contextos a nivel europeo e internacional. ","maurizio.toscano@gmail.com, aroaraba@ucm.es, sros@scc.uned.es, egonzalezblanco@flog.uned.es",Short Presentation
"Tucker, Aaron (1); Ramnarine, Kieran (2)","1: York University and Ryerson University, Canada; 2: Ryerson University, Canada",Datasets of Criminal Faces Within and Under Facial Recognition Software (FRS) From a Digital Humanities Perspective ,Datasets of Criminal Faces Within and Under Facial Recognition Software (FRS) From a Digital Humanities Perspective ,"machine learning, facial recognition, computer vision, digital literacy, data ethics","English, North America, 20th Century, Contemporary, artificial intelligence and machine learning, digital activism and advocacy, Communication studies, Media studies",English,North America,"20th Century, Contemporary","artificial intelligence and machine learning, digital activism and advocacy","Communication studies, Media studies","This paper responds to the intersectional problematics of facial databases within contemporary facial recognition software and computer vision machine learning by highlight our ongoing project This Criminal Does Not Exist. Beginning with the MEDS database, our project applies a Convolutional Generative Adversarial Network to produce synthetic faces. Aesthetically, the portraits generated resemble eugencist Francis Galton’s “composite portraits” of different races that he deployed in the 19th century. The project is a data visualization project: using machine learning techniques, we have been able to surface what is the “most common” type of face within the dataset; that the portraits generated are primarily of African American males speaks to the types of faces over-represented in these virtual spaces. Further, from this data visualization, “This Criminal Does not Exist” is indicative of contemporary State applications of FRS, bringing to light the clear biases inherent in the dataset, biases further perpetuated through algorithms trained on these types of dataset.This response is made from a digital humanities perspective that combines principles of ethical data annotation and classification with critical making. In particular, this paper addresses how digital humanities can contribute potential solutions to the ethics of studying and surfacing problematic databases.More specifically, drawing from the the impacts of 19th century pseudo-science like eugenics, phrenology, physiognomy, and signaletics, our project “This Criminal Does Not Exist” signals another potential set of tactics and research creation paths that simultaneously educates the public about the nature of problematic facial datasets, alongside producing arguments about the ethical implications about such databases and their in-built classification practices. Further, this paper explores how digital humanities scholars can provide a public critical engagement with such databases that is grounded in humanized narrative, that does not further replicate and/or ingrain the intersectional and carceral biases of the databases.Our research begins by recounting how the contemporary study and fears surrounding FRS has been largely focused on large scale corporate- and state-led surveillance apparatuses and their impacts on users’ data privacy. This work, exemplified by scholars like Ann Cavoukian and her framework of Privacy by Design, is undeniably useful; similarly, research by surveillance studies theorists like David Lyon and Gary Marx has contributed greatly to advocating for responsible building and application of technologies like FRS. The initial scholarship into the problematic construction of FRS has been driven, in large part, by a wealth of research and reporting about the known inherent biases of the technology, which, as the Georgetown Law Center on Privacy & Technology’s report “The Perpetual Line-up” insists, “face recognition may be least accurate for those it is most likely to affect: African Americans.” The technology’s consistent optimization, in construction and application, for white male faces is especially troubling as the technology moves from being surveilling, national security, and law enforcement tactics, into the ubiquitous, and far more normalized, activities of intervening in job interviews, the monitoring of low incoming housing, and the granting of bank loans. These last three FRS tasks are examples of what Safiya Noble, in her text Algorithms of Oppression, would give as examples of “technological redlining,” which she explains is the use of algorithms and big data to “reinforce oppressive social relationships and enact new modes of racial profiling.”  Given this, how might digital humanities scholars make the contents of these databases public and available for wider scrutiny and potential regulation while not replicating the dangerous practices that initially led to the construction and implementation of such data? One effective example is artist Trevor Paglen’s collaboration with scholar Kate Crawford titled ImageNet Roulette. The project trains an app on the massive ImageNet database’s of images labeled in the “person” category. The result is a surfacing of how “ImageNet contains a number of problematic, offensive, and bizarre categories. Hence, the results ImageNet Roulette returns often draw upon those categories. That is by design: we want to shed light on what happens when technical systems are trained using problematic training data.” Their accompanying essay, “Excavating AI: The Politics of Images in Machine Learning Training Sets,” expands further in labelling their own work as an “archeology of datasets”: “we have been digging through the material layers, cataloguing the principles and values by which something was constructed, and analyzing what normative patterns of life were assumed, supported, and reproduced. By excavating the construction of these training sets and their underlying structures, many unquestioned assumptions are revealed.” Digital humanities scholars are extremely well suited to take up similar archeological projects, in FRS or other AI- and machine learning-aided environments, as the discipline’s focus on ethics, digital tools and humanities-based close-reading techniques grant scholars the abilities to take up the urgent problems of FRS’s everyday applications.","artucker@yorku.ca, kieran.ramnarine@ryerson.ca",Short Presentation
"Polyck-O'Neill, Julia G.","Brock University, Canada",Potential Archives: How Digital Humanities and Feminist Ethical Praxis Will Transform the Interdisciplinary Artist Archive,Potential Archives: How Digital Humanities and Feminist Ethical Praxis Will Transform the Interdisciplinary Artist Archive,"artists' archives, digital archives, multimedia, interdisciplinary art, feminist ethics","Comparative (2 or more geographical areas), English, North America, 20th Century, Contemporary, digital archiving, public humanities collaborations and methods, Art history, Feminist studies",English,"Comparative (2 or more geographical areas), North America","20th Century, Contemporary","digital archiving, public humanities collaborations and methods","Art history, Feminist studies","Potential Archives: How Digital Humanities and Feminist Ethical Praxis Will Transform the Interdisciplinary Artist ArchiveAs digital media conservators Deena Engel and Glenn Wharton identify in the premise for the Artist Archive Initiative at New York University, conventional approaches to the artist archive neglect to study how the complexity of an artist’s interdisciplinary creative practice can confound conventional archival systems and practices. My project demonstrates how artists’ archives benefit from non-traditional archival methods that combine emerging digital archival strategies that accommodate and represent community networks and collaborations with the intervention of the artists themselves in the co-creation of accessible multimedia archives. Digital methods will enable artists to augment and customize their archival holdings with attributes such as narratives/narration and networked information. The need to reconsider material and organizational aspects of artist fonds also has immediate and practical consequences for institutions. Often collections are broken up, and/or the acquisition process can be delayed by technical and policy-driven challenges. National funding opportunities, such as the Canada Council for the Art’s Digital Strategy Fund (DSF), add further incentive to the demand for fundamental procedural change. Archival scholars have identified two interrelated contentions underlying current approaches to artists’ archives within the present academic and archival milieu: systemic issues fundamental to archival conventions and practices, and shortcomings of formal organizational strategies within such practices. Feminist archival studies scholars such as Michelle Caswell, Marika Cifor, and Stacy Wood have identified that traditional archival practice is often rooted in colonial and patriarchal cultural and structural conditions. Engel and Wharton, scholars of artists’ archives, have addressed how the limitations of conventional archival systems often fail to accommodate the kinds of information, accuracy, and logistical affordances scholars and art professionals require for their research. Specialists in feminist archival studies respond to such organizational shortcomings, observing how the practice of the co-creation of archives with the artist(s) represented within the collections can contribute meaningfully to the value of the collection for scholars and communities. Caswell and Cifor’s proposal for a “feminist ethical framework” for archival studies situates the archive socially and culturally, with consideration of relational and affective contexts (24), and Cifor and Wood argue that “critical feminist theory can contribute to existing archival discourse and practice, critiquing concepts that have remained unquestioned, such as community and organization” (3). The addition of autobiographical, narrative, and networked data and digital media forms enable increased access, and have the potential to transform the relationships between artist, archival institution, and user. This paper explores two main, preliminary ideas: why a transformation of the organization of artist archives is timely and important; and how digital methods and platforms have the potential to benefit artists, arts scholars, and arts archivists. Potential Archives is both a study and a framework, providing both a map of how these non-traditional methods have worked in the past, and a model for how to develop future artist’s archives. My study and resulting framework will reconceptualize the interdisciplinary artist archive according to emerging feminist and digital epistemologies and methods to help artists plan for and prepare their future institutional archives and address emerging needs and concerns, while also assisting arts institutions in addressing such innovations.",jp03uw@brocku.ca,Short Presentation
"Santa Maria, Teresa; Dabrowska, Monika","Universidad Internacional de La Rioja, Spain",Análisis del coro como personaje en la dramaturgia grecolatina y española incluidas en DraCor,Análisis del coro como personaje en la dramaturgia grecolatina y española incluidas en DraCor,"DraCor, Coro, Teatro, Grafo, Literatura comparada","Europe, Spanish, BCE-4th Century, 19th Century, 20th Century, network analysis and graphs theory and application, semantic analysis, Literary studies, Performance Studies: Dance, Theatre",Spanish,Europe,"BCE-4th Century, 19th Century, 20th Century","network analysis and graphs theory and application, semantic analysis","Literary studies, Performance Studies: Dance, Theatre","Drama Corpora Project (DraCor) ha ido conformando un repositorio donde se encuentran, en estos momentos, once corpus teatrales diferentes, que pertenecen a periodos, géneros dramáticos y lugares geográficos diversos. Entre estos corpus encontramos veinticinco obras de la Edad de Plata española (finales del XIX-mediados del siglo XX) que conforman el “Spanish Drama Corpus” y que han sido importadas de la Biblioteca Electrónica Textual del Teatro Español de 1868-1936 (BETTE).Además de los textos dramáticos en XML-TEI que se consultan en DraCor, podemos crear las redes sociales con grafos que conforman los personajes de cada una de dichas piezas dramáticas. Uno de los personajes comunes en muchas de estas piezas y que resulta fundamental en el desarrollo de la trama y para dar corporeidad a los pensamientos de otros personajes lo constituye el Coro.","teresa.santamaria@unir.net, monika.dabrowska@unir.net",Lightning
"Jofre, Ana (1); Cole, Josh (2); Reale, Michael (1); Berardi, Vincent (3)","1: SUNY Polytechnic, United States of America; 2: Queen's University, Canada; 3: Chapman University, United States of America","What’s in a Face? Gender representation of faces in <em>Time</em>, 1940s-1990s","What’s in a Face? Gender representation of faces in Time, 1940s-1990s","Time Magazine, Gender representation, Image analysis","English, North America, 20th Century, cultural analytics, image processing and analysis, Computer science, History",English,North America,20th Century,"cultural analytics, image processing and analysis","Computer science, History","IntroductionBeginning with its inception in 1923, Time magazine, perhaps more than any comparable publication, has both reflected and influenced American popular attitudes to domestic and global politics. These include the changing ideas about women since the mid-twentieth century, which is the subject of this paper.We used supervised machine learning to extract 327,322 visual images of faces from an archive of Time magazine, which contains 3389 issues ranging from 1923 to 2014, and computationally classified the faces as male or female. We then closely read selected Time articles to make sense of this quantitative data against the background of postwar feminism writ-large, and the history of the magazine itself. Our focus is on the period between the 1940s and the 1990s, which witnessed significant changes in attitudes toward women, and where our data of the proportion of female faces exhibits significant fluctuation.We found four clear phases in the visual representation of women in Time from the 1940s to the 1990s: a peak in the mid-to-late 1940s, a dip from the mid-1950s to early 1960s, another peak in the 1970s, and another dip in the 1980s. The number of female faces depicted in Time then rises steadily since the early-1990s. We interpret these variations through an interdisciplinary framework. Through our combined quantitative and qualitative approach, we found that the percentage of female faces found in Time between 1940 and 1990 correlates with attitudes towards women in both the larger historical context as well as within the textual content of the magazine.Methods and ResultsWe first collected data through human labor using Amazon Mechanical Turk (AMT) to identify and tag faces from the archive. This data was used to train a RetinaNet detector[1] to automatically identify and extract faces from the remainder of the archive. Using an accuracy threshold of 90 percent yielded 327,322 faces. A pre-trained face descriptor convolutional neural network VGGFace[2] was then fine-tuned and used to classify each face as either male or female.Similar patterns emerge from both the AMT data and the automated data, featuring an increase in the proportion of female faces from the 1920s to 1945, a post-Second World War dip, a rebound beginning in the mid-1960s, followed by a decrease in the 1980’s, and a final rebound beginning in the early-1990s. Since similar trend lines were found using the AMT and the automatic extraction data, our analysis focuses on the latter, more comprehensive data set.The proportion of women in each issue is shown in Figure 1. While there is a significant amount of variance per issue (compared to when the data is aggregated per year), a clear trendline emerges when the data is Lowess smoothed. Charting the proportion of women in each issue was useful for identifying outliers for our close reading analysis.Figure 1: The percentage of women’s faces in each issue. The solid line is a Lowess smoothed version of the data.To interpret the image data, we analyzed Time within the broader historical context of the 20th century. Our analysis consisted of a close reading of selected issues and articles, chosen based on the following criteria: 1) outlier issues from Fig. 2 defined as those between 1940 and 1990 with > 40% women, 2) issues and articles that were referenced in our secondary sources, and 3) results from EBSCO’s Academic Search Complete database, which we used to retrieve all articles in which the word ‘woman’ or ‘women’ or ‘housewife’ was mentioned within our dates of interest.Our quantitative data coincides closely with the findings of our qualitative analysis. The number of images of women in Time magazine increases during the second world war as women’s role in the workforce expanded beyond traditional ‘feminine’ occupations to fill the gap and to satisfy increased production in the defence industry[3]. In the post-war years, the number images of women decreases as women across North America were instructed by “social engineers, such as psychologists, that they needed to be good wives and mothers in order to fit normally into post-war life”[4]. Our close reading analysis revealed that during these years, notable women who made the news bore the label of ‘housewife’, regardless of whether or not they were actually a housewife. The number of images of women increases once again as the women’s liberation movement began to take shape in the mid-sixties, peaking at the height of the movement in the 1970s. During this time, the women’s movement pushed the institutional structure of Time magazine to change from a gender-based caste system to a more equitable work place[5]. In the 1980s, there is a decrease in images, and we believe that this drop in the representation of women is consistent with the analysis in Susan Faludi’s well-known book Backlash[6], in which she describes the 1980s as a decade that rejected feminism. The representation of women then starts increasing once again in the 1990s and onward.ConclusionWe found that a distant reading of the images of faces in Time magazine is consistent with a historical analysis of American socio-political trends and with a close reading of the magazine’s content. Specifically, we found that the percentage of female faces peaks during eras when women have been more active in public life, and wanes in eras of backlash against women’s rights. This finding is particularly relevant in our contemporary post-literate world in which people absorb culture through images, and spend more time scanning images than reading print content.","jofrea@sunypoly.edu, acole3@gmail.com, realemj@sunypoly.edu, berardi@chapman.edu",Long Presentation
"Lu, Yaya Chenyue (1); Swift, Ben (1); Hawes, Greta (2)","1: Research School of Computer Science, The Australian National University; 2: Research School of Humanities & the Arts, The Australian National University",Analysis and Visualisation of Complex Familial Relationships in Greek Mythology,Analysis and Visualisation of Complex Familial Relationships in Greek Mythology,"Greek mythology, genealogy, graphic design, relationship analysis, graphing algorithms","Global, Europe, English, BCE-4th Century, Contemporary, digital biography, personography, and prosopography, Interface design, development, and analysis, Humanities computing, Literary studies",English,"Global, Europe","BCE-4th Century, Contemporary","digital biography, personography, and prosopography, Interface design, development, and analysis","Humanities computing, Literary studies","Greek Mythology contains numerous idiosyncratic familial relationships that pose unusual challenges for systematic analysis and visualisation. This project examines these idiosyncracies by using a subset of the Greek Mythology data collected according to the MANTO ontology developed by Dr Greta Hawes. This includes the design and development of a user-friendly web-based tool and graph layout that visualises and explores these complex relationships in the context of the digital humanities. In the process, it hopes to generate new avenues of research for data visualisation of complex linked open data structures, and raise societal awareness for unconventional ""family"" and ""relationship"" types.","yaya.lu@anu.edu.au, ben.swift@anu.edu.au, greta.hawes@anu.edu.au",Poster
"Lengo, Maxim","Bar-Ilan University, Israel",From Millions of Words to a Single Phrase: Examination of the TXM Software in Hebrew,From Millions of Words to a Single Phrase: Examination of the TXM Software in Hebrew,"Textometrical Research, TXM software, Online Social Networks","Global, Europe, English, North America, Contemporary, database creation, management, and analysis, social media analysis and methods, Communication studies, Linguistics",English,"Global, Europe, North America",Contemporary,"database creation, management, and analysis, social media analysis and methods","Communication studies, Linguistics","Following the technological development during the second half of the 20th Century, linguistic researchers have begun for the first time to analyze corpora of big data. One research methodology, which was developed for lexicometry and text statistical analysis, is Textometry. I.e., the attempt to combine various statistical analysis techniques, such as factorial correspondence analysis (Benzécri, 1977) and hierarchical ascendant classification (Ward Jr, 1963), with full-text search techniques such as kwic concordances (Luhn, 1960), in order to trace the precise original editorial context of any textual event participating to the analysis.The TXM software (Heiden, 2010), which was developed in France as a modular platform of a new generation of textometrical research and which I adapted to Hebrew, gives the ability to analyze a large corpus of texts as in my research, by using tools and methods based on linguistics and discourse analysis, that is, decomposing the text into factors and elements, carrying out statistical analysis, identifying the hidden social patterns, and then restoring the corpus to its original mode.As a Ph.D. student in the discipline of Social Sciences, my research focuses on the image repair theory (Benoit, 2015) as an ensemble of strategies such as evading responsibility and reducing offensiveness, used by individuals, organizations and groups in order to repair their image during times of crisis. It examines the ways in which rhetorical measures are used in online verbal exchanges among users of the online social networks who attempt to repair their personal image. To achieve my goal, I use the TXM software to analyze a corpus of more than eight million words in 365 Facebook posts, which were published by the Israeli Prime Minister Benjamin Netanyahu during his current affairs, and more than 285,000 comments, made by the users. Netanyahu's Affairs are four police investigations in which he is involved as a suspect or has given a testimony.During the poster session, I will present a review of some tools I used with the TXM software during the digitized analysis process in my Ph.D. research and the way they allowed me to recognize the following key phrase used by Netanyahu: ""They have the media, we have you"". Among these tools are Progression, which is the frequency of occurrence of one term throughout the text corpus; Co-occurrence, which is the frequency of occurrence of two terms in a text corpus alongside each other in a certain order; and Specificity, which is the score a term is given based on its occurrence in the corresponding part of the corpus relative to the one in the entire corpus, and indicates whether it is overused, underused or useless.",maximlengo@gmail.com,Poster
"Papantonakis, Panagiotis (1); Fitsilis, Fotios (2); Leventis, Sotiris (3); Mikros, George (4)","1: Hellenic OCR Team; 2: Hellenic Parliament; 3: Hypernetica; 4: Hamad Bin Khalifa University, Qatar",Xtralingua: An open-source tool for extracting quantitative text profiles,Xtralingua: An open-source tool for extracting quantitative text profiles,"quantitative text analysis, quantitative text profiles, Hellenic OCR Team, Google Summer of Code, Open Science","Global, English, Contemporary, natural language processing, text mining and analysis, Humanities computing, Linguistics",English,Global,Contemporary,"natural language processing, text mining and analysis","Humanities computing, Linguistics","The aim of this poster is to present a novel tool for extracting quantitative text profiles from corpora using a friendly Graphical User Interface. Xtralingua is a software that incorporates over 60 specialized quantitative text analysis measurements including text readability and lexical diversity indices as well as specialized measurements in the text inspired by theoretical work done in the area of Quantitative Linguistics. The tool is open-source and can be further enriched with custom quantitative text indices that the users can add using a scripting language. Xtralingua offers researchers with no specialized technical skills the ability to quickly extract rich quantitative text profiles for further processing. Moreover, it is easy to operate and can support both research and teaching needs in a variety of DH topics.","panpapantonakis@gmail.com, fotis@fitsilis.gr, sotiris.leventis@hypernetica.com, gmikros@gmail.com",Poster
"Riguet, Marine; Alrahabi, Motasem","Labex OBVIL, France",Analyse automatique pour une étude du genre : quels jugements des écrivaines au XIXe siècle ?,Analyse automatique pour une étude du genre : quels jugements des écrivaines au XIXe siècle ?,"Literary criticism, sentiment analysis, semantic annotation, gender studies, named entity recognition","Europe, French, 19th Century, semantic analysis, text mining and analysis, Gender and sexuality studies, Literary studies",French,Europe,19th Century,"semantic analysis, text mining and analysis","Gender and sexuality studies, Literary studies","Cet article présente une méthode d'annotation sémantique développée afin d’étudier le traitement particulier des écrivaines dans la critique littéraire française de la seconde moitié du XIXe siècle. Nous espérons ainsi circonscrire un discours sur la littérature féminine et questionner une pensée littéraire façonnée au prisme du genre. Mais nous entendons également proposer une méthode d’analyse sémantique exportable à d’autres discours, et adaptable aux besoins spécifiques d’autres recherches littéraires.","marineriguet@gmail.com, motasem.alrahabi@gmail.com",Short Presentation
"Alassi, Sepideh (1); Rosenthaler, Lukas (1); Iliffe, Rob (2)","1: Digital Humanities Lab, University of Basel; 2: Faculty of History, University of Oxford",An Interactive 3D Visualization of RDF-based Digital Editions,An Interactive 3D Visualization of RDF-based Digital Editions,"RDF-graph, 3D, visualization, VR, force-directed graph","Global, English, 15th-17th Century, 18th Century, Contemporary, network analysis and graphs theory and application, semantic analysis, History of science",English,Global,"15th-17th Century, 18th Century, Contemporary","network analysis and graphs theory and application, semantic analysis",History of science,"Humanities research produces a vast amount of data that needs to be visualized to understand and interpret the underlying facts. Modeling data with RDF based OWL ontologies defines a directed graph where nodes are the resources, and the properties the edges. Many digital humanities projects visualize the RDF graphs by flattening them into two dimensions. Although this representation helps researchers with recognizing the direct and indirect connections between the resources, it suffers from loss of information due to the overlap of nodes and edges. One can overcome this problem by visualizing the data as a 3D force graph. An interactive 3D visualization also introduces tangibility to the displayed data so that researchers can rotate the model to study the distribution of the data from every angle. The visualization tool is web-based and connected to a platform that serves the digital editions. This connection enables the users to access the underlying resources by directly clicking on the nodes.Figure 1: 3D force-directed graph of Newton's scientific correspondenceRepresentation of the data as a 3D force-directed graph is commonly used for scientific data (Paananen, Wong 2009), but it can be easily adapted to the humanities data as well. To illustrate this, we have chosen to visualize the early modern scientific correspondences. Our database consists of the correspondences of natural philosophers such as Leibniz, Newton, Leonhard Euler, and members of the Bernoulli dynasty. There is already a network that connects the digital editions of these correspondences and makes them openly accessible to the public through one platform (Alassi et al., 2019). This platform is a virtual research environment based on the Knora API (https://www.knora.org/), which manages and stores the data as RDF.Users of this platform can access and query the data online using Angular based front-end components of the API, Knora-ui (https://github.com/dasch-swiss/knora-ui). The 3D visualization tool will be integrated into this user interface and will be openly accessible online. Through Knora, one can derive the graph of the data in the JSON format directly from the triplestore, which is then employed to create a real-time 3D simulation (Figure 1). A configuration step defines graphical features, such as shape, colors, and labels of the nodes and edges (Figure 2). The repulsive forces prescribed on the nodes and edges prevent the overlays of the graph components, and spring-like characteristics of the edges restrain the movement of the nodes. This leads to the formation of clusters of objects that are well connected.Figure 2: Sample 3D visualization of RDF triples.Since time plays a crucial role in the study of the historical facts, an additional dimension has to be introduced into the model to represent the time (Schweizer et al., 2015, p.321). In the 3D model, this can be achieved by the dynamic appearance of the nodes relative to the creation date of the letters. All features developed for this tool will be generic and can be used to visualize any RDF based humanities data. We also intend to generate a virtual reality version of the 3D simulations to enhance the interaction of the users with the data.","sepideh.alassi@unibas.ch, lukas.rosenthaler@unibas.ch, robert.iliffe@history.ox.ac.uk",Short Presentation
"Heßbrüggen-Walter, Stefan",National Research University Higher School of Economics,What Is Formalisation? And Why Do We Need to Talk About It In DH?,What Is Formalisation? And Why Do We Need to Talk About It In DH?,"formalisation, markup, dh software, philosophy","Global, English, Contemporary, meta-criticism (reflections on digital humanities and humanities computing), Philosophy",English,Global,Contemporary,meta-criticism (reflections on digital humanities and humanities computing),Philosophy,"The digital humanities as a discipline are centered on ""formalisation"". We acknowledge the fact that the machines we use employ formal languages, but we still lack a clear idea of what follows from this insight for the self-conceptualisation of the discipline. My presentation clarifies the concept of formalisation and spells out some of its practical consequences.",shessbru@hse.ru,Lightning
"Lui, Pengfei (1); Loudcher, Sabine (1); Darmont, Jérôme (1); Perrin, Emmanuelle (2); Girard, Jean-Pierre (2); Rousset, Marie-Odile (2)","1: Université de Lyon, Lyon 2, ERIC EA 3083, France; 2: Maison de l’Orient et de la Méditerranée, France",Metadata model for an archeological data lake,Metadata model for an archeological data lake,"archéologie, thésaurus, lac de données, archives ouvertes, métadonnées","Europe, French, BCE-4th Century, 5th-14th Century, 20th Century, database creation, management, and analysis, systems and information architecture and usability, Archaeology, Computer science",French,Europe,"BCE-4th Century, 5th-14th Century, 20th Century","database creation, management, and analysis, systems and information architecture and usability","Archaeology, Computer science","The HyperThesau project was initiated by a multidisciplinary team consisting of two research laboratories of archaeology and computer science, a digital library, two archeological museums and a private company. This project has two main objectives: 1) the design and implementation of an integrated platform to host, search, share and analyze archaeological data; 2) the design of a domain-specific thesaurus taking the whole archaeological data lifecycle into account.Archeological data may bear many different types (documents, photos, drawings, sensor data, ...). The description of an archaeological object also differs with respect to users, usages and time. Such variety of archeological data induces many scientific challenges related to storing heterogeneous data in a centralized repository, guaranteeing data quality, cleaning and transforming the data to make them interoperable, finding and accessing data efficiently and cross-analyzing the data. To overcome all these challenges, we exploit the concept of data lake","liu.pengfei@hotmail.fr, sabine.loudcher@univ-lyon2.fr, jerome.darmont@univ-lyon2.fr, emmanuelle.perrin@mom.fr, truelles-pixels@laposte.net, marie-odile.rousset@mom.fr",Short Presentation
"Ivanov, Lubomir","Iona College, United States of America",Haiku Author Recognition,Haiku Author Recognition,"author attribution and identification, haiku, machine learning","English, North America, 15th-17th Century, 18th Century, 19th Century, artificial intelligence and machine learning, attribution studies and stylometric analysis, Computer science, Literary studies",English,North America,"15th-17th Century, 18th Century, 19th Century","artificial intelligence and machine learning, attribution studies and stylometric analysis","Computer science, Literary studies","Haiku Author Recognition1. IntroductionHaiku is a Japanese poetic form renowned for its brevity and expressiveness. Haiku derives from renga/renku – collaborative collections of verses with a 3-line opening hokku verse in the form 5-7-5 on (equiv. syllable). Matsuo Basho made famous the stand-alone hokku form, preserving the 5-7-5 on structure. The name haiku was associated with this form of hokku during 19th century.Four haiku authors rise in prominence above all: Matsuo Basho (17th century) is considered the “father” of haiku. Yosa Buson (18th century) focused on haiku as an art rather than a reflection of reality. Buson combined hokku with painting, inventing haiga (verse-painting). Kobayashi Issa (18-19th century) reinvented haiku through his depth of feeling and humanism. In the second half of the 19th century, Masaoka Shiki critically re-evaluated the art of haiku (coining the term), braking away from the traditional 5-7-5 form, and popularizing the poetic style beyond Japan.We present a study, which employs authorship attribution techniques to determine the distinctiveness of poetic styles in haiku, focusing on the poetry of Basho, Buson, Issa, and Shiki. There has been little work in the field of haiku attribution. A theoretical study of phonological complexity in haiku was presented in [1]. An approach to automatic evaluation of the quality of haiku was presented in [2]. An interesting work [3] deals with identifying unintended haiku in text. We approach haiku attribution as a classification problem: Given a set of attributed haikus, we train classifiers to recognize the writing style of each poet, and apply an ensemble of trained models to unattributed texts.2. Our Haiku CorpusThe first step in creating our model was obtaining a haiku corpus. There are three approaches:Use actual haikus written in hiragana (a form of Japanese alphabet)Use Roman alphabet transcriptions (rōmaji) of haikus.Use English translations of haikus.While using hiragana haikus is arguably the best option, our software lacks the capability to process hiragana text. English translations of haikus are readily available, but while research suggests that the authorial signal is stronger than the translators’ [4], we do not know if that applies to haiku. We opted to construct a corpus of rōmaji transcribed haikus. This was difficult since most resources are either hiragana originals or translations. We obtained 723 haikus by Basho from [5], 842 haikus by Buson from [6], and 603 haikus by Issa from [7]. Finding transcribed Shiki haikus proved extremely challenging. Even though Shiki wrote over 24000 haikus, only a handful have been transcribed into rōmaji. Failing to secure transcriptions, we downloaded the full set of 24000 hiragana haikus from [8]. We then used an online hiragana-to-rōmaji transcription tool [9] to transcribe 967 randomly selected haikus by Shiki. Since many of the extracted haikus were organized alphabetically or by topic, we wrote Python code to randomly shuffle the order of the haikus for each author. A different program broke up the haikus into files of size 50 haikus each.3. Attribution MethodologyOur attribution software is a based on JGAAP [10] and implements an ensemble of classifier/stylistic-feature pairs [11,12]. For this study, we limited the set of stylistic features to character-2/3/4/5-grams (CnG), word-2/3-grams (WnG), vowel-initiated words (VIW), and first-word-in-sentence (FWIS). The classifiers used were support vector machines with sequential minimal optimization (SMO) and multilayer perceptrons (MLP).4. ResultsWe conducted several experiments, where we randomly chose one 50-haiku file for each author and removed it from the training set. We trained the classifiers on the remaining set of haikus using leave-one-out (L1O) validation. The results of the training for three sets of experiments are presented in Table 1:Table 1: Training Accuracy for Basho, Buson, Issa, and ShikiNext, we tested the authorship of the 50-haiku files that were left out of the training. The results of those experiments are presented in Table 2:Table 2: Attribution Results for Basho, Buson, Issa, and ShikiIt is quite clear that even with a reduced set of stylistic features, the attribution is very strong and the author identification definitive. We conducted an additional set of experiments, where we used each of the trained models to test the authorship of five haikus by the 18th century haiku poet Takarai Kikaku. The models were not trained on Kikaku, so, as expected, the results were split among two or more authors (Table 3):Table 3: Attribution Results for KikakuInterestingly, Kikaku was a prominent student and disciple of Basho, yet none of the models makes that association. This is most likely due to the small number of Kikaku haikus tested.5. Conclusion and Future WorkWe presented results from haiku author identification experiments, which suggest that haiku authorship can be determined even with a limited set of stylistic features from rōmaji-transcribed haikus. Our next efforts will be to experiment with a larger set of haiku authors, with English translations, and, possibly, with hiragana haikus. Among the questions we wish to answer are:What is the minimal set of haikus sufficient to identify an author?Is the authorial signal stronger than the translator’s for haiku translations?Can prosodic features be used for haiku author identification?Does the historical period affect the accuracy of attribution?",livanov@iona.edu,Short Presentation
"Gillis, Roger Christopher",Dalhousie University,"""Open GLAM"": Opening up digital cultural heritage collections for the digital humanities ","""Open GLAM"": Opening up digital cultural heritage collections for the digital humanities ","Copyright, Cultural Heritage, Digital Collections, Open Access, Open GLAM","Global, English, Contemporary, copyright, licensing, and permissions standards, systems, and processes, digital access, privacy, and ethics analysis, Galleries and museum studies, Library & information science",English,Global,Contemporary,"copyright, licensing, and permissions standards, systems, and processes, digital access, privacy, and ethics analysis","Galleries and museum studies, Library & information science","Over the past twenty plus years, cultural heritage organizations have put vast amounts of digitized available online.The movement towards Open Access for digitized cultural heritage has come to be known as Open GLAM (Galleries, Libraries, Archives, and Museums). This presentation will explore key issues and recent efforts around Open GLAM and the broader issues of Open Access to cultural heritage, and in particular will highlight many current activities around Open GLAM, as well as issues and challenges that exist in this area.",roger.gillis@dal.ca,Short Presentation
"Aboelnagah, Hadeer","Prince Sultan University, Saudi Arabia",Building Online Communities as a Platform for Collaborative Learning and Cross-Cultural Self- Expression; Saudi Female Students’ Blog<em> Hajj Behind the Scenes</em> as an Example,Building Online Communities as a Platform for Collaborative Learning and Cross-Cultural Self- Expression; Saudi Female Students’ Blog Hajj Behind the Scenes as an Example,"Blogging, Collaborative Creativity, Sharing Knowledge, Open Access","Asia, Global, English, Contemporary, curricular and pedagogical development and analysis, public humanities collaborations and methods, Cultural studies, Education/ pedagogy",English,"Asia, Global",Contemporary,"curricular and pedagogical development and analysis, public humanities collaborations and methods","Cultural studies, Education/ pedagogy","Digital Humanities as an emerging field provides endless opportunities to paradigm shifts in the educational experience in higher education, it opens wide doors for interdisciplinary collaborative students’ projects that can be the seed of larger-scale national and international. Blogging is a pedagogical activity that is used to enhance collaborative learning and social responsibility (Yu-Chun Kuo 2017). Hajj Behind the Scenes is a blog by Saudi Female Students. The study explores blogging as an activity and its possible utilization to build cross-disciplinary online communities and to enhance societal involvement in the field of Digital Humanities. The said blog is used as an example to study its effect on the students and their abilities to self-expression, autonomous learning, and collaborative creativity.",habouelnagah@psu.edu.sa,Lightning
"Herrmann, J. Berenike; Messerli, Thomas C.","University of Basel, Switzerland",Metaphors we read by: Finding metaphorical conceptualizations of reading in web 2.0 book reviews,Metaphors we read by: Finding metaphorical conceptualizations of reading in web 2.0 book reviews,"metaphor, social reading, cultural analytics, metaphor identification","Comparative (2 or more geographical areas), Europe, English, Contemporary, cultural analytics, semantic analysis, Cultural studies, Literary studies",English,"Comparative (2 or more geographical areas), Europe",Contemporary,"cultural analytics, semantic analysis","Cultural studies, Literary studies","Metaphors we read by: Finding metaphorical conceptualizations of reading in web 2.0 book reviewsIntroductionWhile interdisciplinary research on metaphor is abundant (Eggs, 2000; Semino & Demjén, 2017; Veale et al., 2016), it is still scarce in Digital Humanities. At the intersection of literary studies, corpus stylistics, and digital humanities, we present an exploratory quantitative metaphor analysis of a corpus of German language lay book reviews. Using a deliberately simple methodological approach that operates on seed words for conceptual sources and targets we investigate how reading experiences of literary texts are metaphorically presented by reviewers.We explore a corpus of approx. 1.3 mill. book reviews for metaphors used to conceptualize the target domain READING EXPERIENCE.In line with conceptual metaphor theory, metaphors in language are understood as closely linked to human thought processes and experiences (Lakoff & Johnson, 1980, pp. 4–6; Shutova, 2017). They are mappings from typically more basic experiential source domains (LIFE) to more abstract target domains (READING EXPERIENCE), indicated by indirectly used lexis (the words come, end, and road in “we've come to the end of our road”, VUAMC, Steen et al., 2010).Starting from findings on literature reviews in English (Stockwell, 2009; Nuttall & Harrison, 2018) and on reviews in German (Köhler, 1999), we analyze metaphor patterns in social reading networks, with a particular focus on the mapping READING EXPERIENCE IS MOTION. The main aim at this stage is to draw up a first typology of mappings.Method and DataMetaphor IdentificationIn view of the challenges of reliable automatic metaphor detection (Veale et al., 2016), we apply a deliberately simple rule-based corpus stylistic approach (Deignan & Semino, 2010). A commonly used resource for identification of metaphorical lexical items per source domain is semi-automatic semantic tagging (Demmen et al., 2015). However, in the absence of an out-of-the-box semantic tagger for German, we rely on a ‘traditional’ onomasiological resource (Dornseiff, 2004). Metaphors are identified by (1) detecting seed words for target domains, (2a-c) detecting source domain seed words in the textual neighborhood of target domain seed words: the metaphor vehicles. Potential metaphors are examined and assigned to a typology of mappings by inspection of KWICs (3).Step 1. To identify target domain seed words, we compile a list of ‘objects of reading experience’ (OREs), i.e. noun lemmas that refer to aspects of reading (literary works, such as Buch ‘book’, Geschichte ‘story’, Roman ‘novel’ and parts thereof, such as Ende ‘ending’ or Spannung ‘suspense’, see Table 1).Step 2. (2a) Potential source domains are pre-identified by manual MIPVU annotation of small samples of the data (cf. Herrmann et al., in press), and the literature on ‘reading’ metaphors (e.g. Nuttall & Harrison, 2018). For the present paper, we focus on conceptualizations of reading experiences as MOTION (see Herrmann & Messerli, submitted, for metaphor vehicles from the domain FOOD INTAKE). (2b) The lexical access points to the MOTION domain are provided by a word list extracted from Dornseiff (2004) for the semantic field Fortbewegung (8.3, see Table 2). (2c) To find potential metaphor vehicles that refer to ORE (and not to some other referent), cooccurrences are computed between ‘motion’ lemmas and ORE, with a window of 10 lemmas around ORE (using raw frequencies, see Table 3).[1][Table 1: Objects of Reading Experience (ORE) in LoBo][Table 2: Ten most frequent potential MOTION seed words in LoBo][Table 3: Cooccurrences. Most frequent MOTION seed words within a window of ten words of an ORE]Step 3. From the resulting frequency list of potential ‘motion’-metaphor vehicles (n= 389,689) a sub-section of the most common lemmas is examined by means of KWICs to determine whether potential vehicles were indeed used metaphorically. In a qualitative step, we infer usage patterns from the resulting true metaphor positives (Table 4).DataThe LoBo corpus (extracted from the social reading platform “Lovelybooks”) contains approx. 1.3 mill. German language reviews by 54,000 users, amounting to 439,923,000 words (Table 4), spread over 15 genres. Each review features a rating (1–5 stars) that refers to a specific book. The corpus is lemmatized and PoS-tagged with TreeTagger (Schmid, 1994), and encoded in CWB (http://cwb.sourceforge.net/).[Table 4: Overview of word frequencies of ORE and source domain seed words in LoBo]AnalysisA first result is a list of those lemmas from the semantic field Fortbewegung ‘motion’ that occur frequently within a window of ten words of ORE. While it does not yet allow for conclusive results regarding metaphor use, this list serves as an intermediary step towards identifying a multitude of MOTION metaphors for subsequent analysis establishing a typology of mapping patterns.The analysis of KWICs shows that certain manners of motion are particularly frequent. Notable are the motions of walking, flying, and driving/riding, realized with the lemmas gehen ‘to go’, fliegen ‘to fly’, and Fahrt ‘ride/drive’. Notable is variance of ‘speed’, with fast motion (Fahrt, fliegen), and slower motion (gehen).Another important observation is about agency within the metaphorical scenario. Readers position themselves mainly as (a) observers who see how the plot moves along; (b) agents who actively ‘walk’ and ‘fly’ through the story (or a book’s pages); (c) patients being put in motion by the book; and (d) companions who travel along with an ORE (see Table 5). Findings demonstrate the complexity of reading that cannot be restricted to passive reception or hedonistic consumption (cf. Rebora et al., 2019).[Table 5: Categories of mappings READING IS MOTION in LoBo]In all, our study offers a first typology of metaphorical MOTION-mappings in digital shared reading, as well as evidence of the productivity of MOTION as a source domain for READING in German lay reviews (cf. Nuttall & Harrison, 2019, for English reviews). Extending this exploratory phase into statistical analysis, we plan variance analysis with factors as reader’s evaluation (star ratings) and book genre (e.g., middle brow vs. popular). Methodologically, we plan to improve precision of metaphor detection, e.g. by including semantic information from resources such as GermaNet, but also through active learning. Generally, further examination of metaphors will allow valuable insight into underlying conceptual and value systems in reader reviews.","berenike.herrmann@unibas.ch, thomas.messerli@unibas.ch",Long Presentation
"Herrmann, J. Berenike (1); Odebrecht, Carolin (2); Santos, Diana (3); Francois, Pieter (4)","1: University of Basel, Switzerland; 2: Humboldt-University Berlin, Germany; 3: University of Oslo, Norway; 4: University of Oxford, UK",Towards Modeling the European Novel. Introducing ELTeC for Multilingual and Pluricultural Distant Reading,Towards Modeling the European Novel. Introducing ELTeC for Multilingual and Pluricultural Distant Reading,"corpus, TEI, literary modeling, comparative literary studies, 19th Century","Comparative (2 or more geographical areas), Europe, English, 19th Century, data modeling, database creation, management, and analysis, Humanities computing, Literary studies",English,"Comparative (2 or more geographical areas), Europe",19th Century,"data modeling, database creation, management, and analysis","Humanities computing, Literary studies","Towards Modeling the European Novel. Introducing ELTeC for Multilingual and Pluricultural Distant ReadingThis contribution reports on the collaborative effort of building an open access multilingual corpus of European novels published 1840-1920 (the European Literary Text Collection - ELTeC) within the COST Action “Distant Reading”.[1]Working at the intersection of many languages and cultures, we address practical and technical aspects of corpus design based on a theoretical discussion of pluri-cultural computational modeling of literature. In the corpus design, we adopt a metadata-based approach that allows for representing the diversity of novels published 1840-1920 across Europe. Our sampling and balancing criteria use metadata including publication date, text length, reprint counts and authors’ gender,[2] and we deliberately focus on inclusion of non-canonical novels.We have built a workflow for systematically sampling and encoding novels as well as a consistent annotation model of data and metadata (cf. Burnard, Schöch, Odebrecht, 2019). Currently, ELTeC constitutes a dynamic intersection of fictional discourse in fourteen languages, including Czech, English, French, German, Greek, Hungarian, Italian, Norwegian, Polish, Portuguese, Romanian, Serbian, Slovenian, and Spanish (ca. 600 text candidates amounting to ca. 52 mio. words).[3] Figure 1 depicts the current state of the Portuguese sample, across categories “text length” (long, medium, short), “gender” (female, male), and “date slot” (T1-4; https://distantreading.github.io/ELTeC/por/index.html).Figure 1. Screenshot of Portuguese sampleOur TEI-XML (TEI Consortium, 2019) encoding scheme aims is minimal, but aims at facilitating a rich and well-informed distant reading. ELTeC is rooted in the open data movement, with collaborative data creation and an open access extensive documentation for (meta-)data schema, decisions and workflows.[4] Each version of ELTeC is archived via Zenodo.[5] Thus, our (meta)data are re-usable, interoperable, accessible and findable (cf. FAIR Guiding Principles; Wilkinson et al., 2016).In view of the problematic notion of “representativeness” (see Biber, 1993), ELTeC deliberately refrains from modeling the statistical distribution of populations of publication or reception (cf. Herrmann & Lauer, 2019). Rather, we address the inevitable bias included in the sampling (see Bode, 2018), as well as the explicit link to research questions (Underwood, 2019; Lüdeling, 2011) and the act of construction (Piper, 2019).ELTeC caters to the computational modeling of literature at the intersection of cultures, nations, languages, genders, but also poetologies, trends, and traditions, in a historical period of extreme aesthetic change and diversity. Giving one example, in collaboration with other working groups, using demonym and named entity recognition, it is possible to comparatively explore images of ‘the other’ (ethnic, national, regional; Leerssen, 2016). Generally, in the creation of ELTeC we aim at inductively defining what a ‘novel’ is, allowing for diverse approaches in literary theory and history to be explored and tested.The research described in this paper was conducted in the context of the COST Action ""Distant Reading for European Literary History"" (CA16204 - ""Distant-Reading""). Find out more at: http://www.distant-reading.net. COST is funded by the Horizon 2020 Framework Programme of the EU.","berenike.herrmann@unibas.ch, carolin.odebrecht@hu-berlin.de, d.s.m.santos@ilos.uio.no, pwfrancois79@gmail.com",Short Presentation
"Arbuckle, Alyssa; Siemens, Ray","University of Victoria, Canada","<em>Open, Digital Scholarship: Issues, Initiatives, and Research Commons in the Humanities and Social Sciences</em><em> </em>","Open, Digital Scholarship: Issues, Initiatives, and Research Commons in the Humanities and Social Sciences ","open scholarship, open access, research commons, digital scholarship","English, North America, Contemporary, open access methods, public humanities collaborations and methods, Communication studies",English,North America,Contemporary,"open access methods, public humanities collaborations and methods",Communication studies,"In the spirit of the public humanities, initiatives are emerging that foster the open sharing, re-purposing, and development of scholarly projects, publications, educational resources, data, and tools. One example is the Canadian Humanities and Social Sciences (HSS) Commons, an in-development prototype for a national-scale, online research commons that features:subject repository for open access publications with digital object identifiers (DOIs) upon upload and FAIR (Findable, Accessible, Interactive, Reusable) guidelines for data managementproject development environment integrated with Google Drive, Github, or Dropboxindividual user profiles with federated login/identity authorization, including ORCIDblogging capabilitiessubject interest groups and member interactions (e.g., profile building, messaging)The workshop includes:talks by key players in the Canadian digital research infrastructure world;a theoretical overview of the commons in research development and community building;instruction in how to set up a profile, upload publications, form a special interest group, and begin a digital scholarship project in the prototype Canadian HSS Commons.This initiative builds on consultations among Implementing New Knowledge Environments (INKE) Partnership members—in particular the Canadian Social Knowledge Institute, Compute Canada, and the Federation for the HSS—and has roots in similar initiatives such as Humanities Commons, a partner in this work.This platform is intended to serve the unique needs of the Canadian HSS community. Based on early-stage consultations we anticipate that it will draw a significant number of DH practitioners. This prototype is built on HUBZero—an open source content management system available for deployment within other international contexts as well.","alyssaa@uvic.ca, siemens@uvic.ca",Workshop/Tutorial 4
"Hyman, Christy Lynn (1); Crandell, Alli (2); Bergeron, Sue (3); Rouse, Jesse (4); Lynch, Shane (5); Moore-Pewu, Jamila (6); Carter, Bryan (7); Green, Hilary (8)","1: University of Nebraska Lincoln, United States of America; 2: Coastal Carolina University, United States of America; 3: Coastal Carolina University, United States of America; 4: University of North Carolina-Pembroke, United States of America; 5: University of Kansas, United States of America; 6: California State University-Fullerton, United States of America; 7: University of Arizona, United States of America; 8: University of Alabama, United States of America",Constructing Spatial Narratives: Considerations and Practices Across Communities,Constructing Spatial Narratives: Considerations and Practices Across Communities,"digital mapping, public history, critical GIS, social justice, recovery","Comparative (2 or more geographical areas), English, 5th-14th Century, 19th Century, Contemporary, public humanities collaborations and methods, spatial & spatio-temporal analysis, modeling and visualization, Geography and geo-humanities, History",English,Comparative (2 or more geographical areas),"5th-14th Century, 19th Century, Contemporary","public humanities collaborations and methods, spatial & spatio-temporal analysis, modeling and visualization","Geography and geo-humanities, History","This panel considers how critical, innovative approaches with GIS draw our attention to new pathways of digital mapping. How can the practice of digital mapping within a critical lens produce new cartographies for spaces of possibility? How can spatial narratives restore the tarnished lineages of cultural geographies obscured from history?We want to engage with these questions in our panel. Our hope is to highlight the ethical responsibilities of critically engaged mapping projects. The assorted projects that we propose to present on in this panel are connected through a theoretical grounding that harnesses the power of imagining cultural recovery of landscape as a means of redress from historical obscurity. Ultimately our panel seeks to challenge the matrix of colonial epistemic power underlying traditional foundations of how we construct spatial constructions of communities.","christy@huskers.unl.edu, allicrandell@gmail.com, sbergero@coastal.edu, jesse.rouse@uncp.edu, shane_lynch@ku.edu, jmoorepewu@fullerton.edu, bryancarter@email.arizona.edu, hngreen1@ua.edu",Panel
"Barnett, Tully (1); Cytron, Megan (2); Gairola, Rahul (3); Sumner, Tyne Daile (4)","1: Flinders University, Australia; 2: Universidad Complutense de Madrid; 3: Murdoch University/ Asia Research Centre; 4: University of Melbourne",Global Renderings in the Queer Digital Humanities,Global Renderings in the Queer Digital Humanities,"queer, intersectional, textual, margins, surveillance","Europe, English, North America, Australia/Oceania, 20th Century, Contemporary, digitization (2D & 3D), text mining and analysis, Gender and sexuality studies, Literary studies",English,"Europe, North America, Australia/Oceania","20th Century, Contemporary","digitization (2D & 3D), text mining and analysis","Gender and sexuality studies, Literary studies","How might DH textual scholars respond to call to queer DH practices, methodologies and projects? This panel brings together four papers investigating the queer margins of digital textual or methodological culture. Barnett considers the queer politics of metadata in digitisation projects identifying the way item-level markings become work-level interpretive interventions. Cytron deploys close and distant reading to position Eduardo Mendicutti’s 1982 radical act of queer world-building in a global context through analytical stylometry and TEI. Sumner considers the queer implications of distant reading techniques on the FBI files on Langston Hughes and James Baldwin, which conflate homosexuality, race, and poetics by identifying all three as “perversity.” Gairola deploys queer of color critique as a lens to read the intersectional nexus of queer and postcolonial theory through analyzing machine learning’s supposed ability to detect “gayface” with facial recognition software. Together, these papers identify and press upon diverse margins in global queer digital inscriptions.","tully.barnett@flinders.edu.au, mcytron@ucm.es, Rahul.Gairola@murdoch.edu.au, tdsumner@unimelb.edu.au",Panel
"Galleron, Ioana (1); Patras, Roxana (2); Gradinaru, Camelia (2)","1: Université Sorbonne-Nouvelle, Paris, France; 2: Universitatea ""Al. I. Cuza"", Iasi, Romania",Annotating spatial entities in Romanian Novels,Annotating spatial entities in Romanian Novels,"spatial entities, Hajdouk novels, manual annotation","Europe, English, 19th Century, spatial & spatio-temporal analysis, modeling and visualization, text encoding and markup language creation, deployment, and analysis, Literary studies, Central/Eastern European Studies",English,Europe,19th Century,"spatial & spatio-temporal analysis, modeling and visualization, text encoding and markup language creation, deployment, and analysis","Literary studies, Central/Eastern European Studies","This paper is based on HAIRO, a Franco-Romanian project for creating a library of Romanian Hajdouk novels in an XML/TEI format (see https://proiectulbrancusihairo.wordpress.com/home-1/). Hajdouks were outlaws living in the woods, that fascinated the public in the second half of the 19th century and at the beginning of the 20th century, both for their cruelty and their sense of justice. Between 1840 and 1920, they appear in almost 12% of the Romanian novels, with at least 40 titles specifically dedicated to this picturesque character.Our main concern is “the place-making mediated by the text”, and more precisely the creation of a Hajdouk space; in a rural Romania, structured by clear and stable relationships between spaces, their nomadic way of life constitutes a disrupting force, and we are looking at if and how this reflects in the novels. Much along the lines of (Hay and Butterworth 2019), our work focuses less on the “indexical relationship to the physical world”, and more on the ways in which the texts create their own spatiality.In the first part, we discuss the adaptation of Pustejovsky’s ISO metamodel (2014, 2019) to operate what we call a “basic annotation” of our set of novels. Faced with the specificities of our texts, we have defined not two, but seven types of spatial entities: toponyms, places, paths, zones, vehicles, topical spaces and potential spaces. The two last categories are the most salient difference between our annotation schema and the previous existing ones, and we advocate their interest in literary contexts, where “the other world” or “in his bosom” are frequently mentioned, to quote but two examples from a very rich list.We further characterize the spaces as “absolute” or “relative”. For this “basic annotation”, we have renounced to define other types of relations, such as orientation, movement or metrics.The annotation exercise took place in two phases. In a first, exploratory round, we have worked on XML files, and implemented our schema as a feature structure in TEI. In a second round, we have configured a BRAT server and started by measuring the inter-annotator agreement on a set of 10 samples of about 1000 words (see results in Galleron et al., forthcoming). In a third phase, currently under development, we proceed to the actual annotation of texts, using a place names dictionary to pre-annotate. Another path currently explored is that of the syntactic tagging of phrase constituents: since a large part of our space entities appear to assume a function of circumstantial complement of place, they could be spotted with a specialized dependencies tagger. However, the first experiences in this respect are quite disappointing, and all the more so they have been conducted on samples in French – results will probably be worse on Romanian samples, since Romanian is a language less equipped with NLP tools. Please note that usual NER systems (Stanford, Spacy library, etc.) do not work, or give very poor results, on Romanian texts. For all these reasons, manual annotation still appears as the best way to go, in spite of being extremely time consuming.To date, the repartition of the annotations per type, as indicated in figure 1, confirms that looking at toponyms only, with a NER/ NEL approach, fails to capture a large part of the placemaking process in a novel. Also, two major categories of novels seem to appear with regards to the writing of the space, one constituted by the texts in which places and zones are in even proportions, the other gathering novels in which places are dominant, to the detriment of zones.Figure 1. Annotations per type in a selection of novelsIn addition, categories “paths” and “vehicles” seem to be discriminant between two other types of fiction. Indeed, while the number of annotations remains quite low in both cases, they allow to identify certain novels as outliers, with lots of spatial changes, as opposed to the major part of novels that appear finally more “static”, and privileging scenes and summaries of the action. This is somewhat surprising, since we expected all our Hajdouk novels to pertain to the second category. We are currently trying to understand if the difference is motivated by the specific style of certain authors, the taste of an era, or it genuinely points towards a generic specificity within our corpus.","ioana.galleron@sorbonne-nouvelle.fr, roxana.patras@uaic.ro, cameliagradinaru2013@gmail.com",Lightning
"Wermer-Colan, Henry Alexander (1); Mulligan, Rikk (2)","1: Temple University, United States of America; 2: Carnegie Mellon University, United States of America",Prototyping the SF Nexus: Collaborative Models for Digitizing and Curating Speculative Fiction Collections as Data,Prototyping the SF Nexus: Collaborative Models for Digitizing and Curating Speculative Fiction Collections as Data,"Science Fiction, Digital Collections, Cultural Analytics","Europe, English, North America, 19th Century, 20th Century, Contemporary, cultural analytics, digital libraries creation, management, and analysis, Book and print history, Literary studies",English,"Europe, North America","19th Century, 20th Century, Contemporary","cultural analytics, digital libraries creation, management, and analysis","Book and print history, Literary studies","This paper overviews the SF Nexus prototype, the developmental stage of a research project to digitize and curate available works of Anglo-American speculative fiction. This resource enables access to texts in the public domain and under copyright, from magazines to mass-market novels. We overview challenges confronting scholars of SF book history, including restricted access to and the uncatalogued materials in special collections, copyright barriers to sharing digital texts, and the lack of comprehensive indices to SF texts and their publication records. The presentation will present models for multi-institutional, collaborative mass digitization efforts to ingest into HathiTrust, share across research centers and libraries, and curate on the web with user-friendly, embeddable tools for the cultural analytics of books as image and text data. We conclude by proposing our coalition as a collaborative model for similar digitization and curation projects requiring standardized policies, legal agreements, and data curation workflows.","alex.wermer-colan@temple.edu, rikk@cmu.edu",Long Presentation
"Blackwell, Christopher William; Blackwell, William; Norman, Max","Furman University, United States of America",<em>Les Misérables</em> & the CITE Architecture: A Publication and Toolkit,Les Misérables & the CITE Architecture: A Publication and Toolkit,"Hugo, Citation, N-Gram, Scala, Paris","Europe, English, French, 19th Century, scholarly editing and editions development, analysis, and methods, text mining and analysis, Humanities computing, Literary studies","English, French",Europe,19th Century,"scholarly editing and editions development, analysis, and methods, text mining and analysis","Humanities computing, Literary studies","Our poster will present and link to a publication and toolkit for working with Victor Hugo’s Les Misérables, in French and English, using the CITE Architecture. The published data will include CTS-Compliant texts in French and English, and programmatically derived versions of those texts: TEI-XML, HTML, stop-words removed (useful for Topic Modelling), lemmatized (stemmed) editions, vocabulary lists, contextualized concordance, and a web-based translation-alignment tool.The deliverable is not only a very rich deluxe, bilingual edition of the novel, but the documented scripts used to take a CITE/CTS text and transform it for different presentations and analyses.","cwblackwell@gmail.com, wblackwell98@gmail.com, max.norman@furman.edu",Poster
"Esteva, Maria (1); Clement, Tanya (2); Xu, Weijia (1); Aaron, Choate (3); Robbins Hopkins, Hannah (2)",1: Texas Advanced Computing Center; 2: Department of English; 3: UT Libraries,AI4AV (Artificial Intelligence for Audiovisual): Design and Evaluation of a Shared System for LAMs,AI4AV (Artificial Intelligence for Audiovisual): Design and Evaluation of a Shared System for LAMs,"audiovisual, artificial intelligence, machine learning, professional values, shared infrastructure","South America, English, North America, Contemporary, artificial intelligence and machine learning, metadata standards, systems, and methods, Computer science, Library & information science",English,"South America, North America",Contemporary,"artificial intelligence and machine learning, metadata standards, systems, and methods","Computer science, Library & information science","Audiovisual (AV) materials are predominant historical and scientific records of our times, and their numbers are increasing exponentially in collecting institutions. Tasked with preserving and making AV materials available, libraries, archives, and museums (LAMs), need to find efficient and scalable curation solutions. Using machine learning (ML) to generate metadata is promising, but to adopt such methods information professionals must overcome a host of technological and cultural challenges. We introduce the AI4AV project in which we are conducting research around the design and evaluation of a system (currently a prototype) that uses ML to translate audio to text as well as natural language processing to classify and describe AV materials within open computing infrastructure that can be shared by multiple LAMs. This presentation describes the testbed collection, the ML and NLP methods and computing resources, and the protocol to incorporate LAMs values in the design and evaluation of the system.","maria@tacc.utexas.edu, tclement@utexas.edu, xwj@tacc.utexas.edu, achoate@austin.utexas.edu, hnrobb@gmail.com",Short Presentation
"Presner, Todd (1); Bonazzi, Anna (1); Fan, Lizhou (1); Toth, Gabor (2); Deblinger, Rachel (1); Shepard, David (1)","1: UCLA, United States of America; 2: University of Southern California, United States of America",Digital Humanities Methods for Analyzing Holocaust and Genocide Testimonies,Digital Humanities Methods for Analyzing Holocaust and Genocide Testimonies,"Holocaust and genocide studies, testimony, text analysis, visualization","Comparative (2 or more geographical areas), Global, Europe, English, 20th Century, data modeling, text mining and analysis, History, Literary studies",English,"Comparative (2 or more geographical areas), Global, Europe",20th Century,"data modeling, text mining and analysis","History, Literary studies","The purpose of this panel is to present a set of digital methods for analyzing Holocaust and genocide testimonies at scale. The corpus of testimonies (approx. 55,000) and metadata come primarily from the USC Shoah Foundation and include survivors of the Armenian Genocide (1914-23), the Nanjing Massacre (1937/38), the Holocaust (1939-45), and the Rwandan Genocide (1994). Panel members also work with early Holocaust testimonies recorded in Displaced Persons Camps by David Boder (1946) as well as testimonies of from Yale’s Fortunoff Archive. One of the central research questions concerns the genre of “testimony” itself and how computational analysis can help us track changes in narrative structure, form, and content, particularly in dialogical interviews. We are also interested in how testimonies can be textually mined to fill in “gaps” and “missing voices” through linguistic analysis, including code switching, speech patterns, changes in voice and emotional expressivity, and so forth.","presner@ucla.edu, annabonazzi@g.ucla.edu, lizhou@ucla.edu, gabor.toth@maximilianeum.de, rdeblinger@library.ucla.edu, dave@humnet.ucla.edu",Panel
"Klimashevskaia, Anastasia (1); Geiger, Bernhard C. (2); Hagmüller, Martin (1); Helic, Denis (3); Fischer, Frank (4)","1: Signal Processing and Speech Communication Laboratory, Graz University of Technology, Graz, Austria; 2: Know-Center GmbH, Graz, Austria; 3: Institute of Interactive Systems and Data Science, Graz University of Technology, Graz; 4: Higher School of Economics, Moscow","""To be or not to be central"" - On the Stability of Network Centrality Measures in Shakespeare's ""Hamlet""","""To be or not to be central"" - On the Stability of Network Centrality Measures in Shakespeare's ""Hamlet""",literary network analysis,"Europe, English, 15th-17th Century, Contemporary, artificial intelligence and machine learning, network analysis and graphs theory and application, Informatics, Literary studies",English,Europe,"15th-17th Century, Contemporary","artificial intelligence and machine learning, network analysis and graphs theory and application","Informatics, Literary studies","IntroductionCentrality measures derived from character networks can be used to detect the main characters in a play. For example, previous research has shown that characters with high network centrality typically perform the majority of speech acts and appear in most of the scenes (Fischer, Trilcke, Kittel, Milling, & Skorinkin, 2018). However, one can extract character networks from plays in various ways: Close reading may omit minor characters like attendants or servants, e.g., (Moretti, 2011), while distant reading (e.g., parsing an XML file) may include aggregate characters like “All”, “Both Lords”, or similar. Furthermore, the networks may display either implicit or explicit connections, depending on whether we connect characters because they appear in the same scene or because they are directly addressing each other, respectively. Thus, as adding more characters or connections to the network affects centralities and other network measures, the interpretation of both qualitative and quantitative aspects of characternetworks depends on the extraction method. In this work we are concerned with the specific question whether details of the textual source and the extraction method, such as adding minor or aggregate characters, make the main characters less “central”. A negative answer to this question would provide us with a further evidence for the validity of automated literary network analysis.ApproachWe analyse six versions of the character network of Shakespeare’s “Hamlet”. All networks were extracted via close or distant reading from different XML or text sources (see Figure 1) and analysed with NetworkX (Python). For each network, we compute four different centrality measures (closeness, betweenness, degree, and eigenvector centrality). Subsequently, for each centrality measure, we rank the 26 characters common in all networks and compare character ranks in different networks by computing their Spearman rank correlation.Figure 1. Basic statistics of the character networksObservations, Conclusion, and OutlookThe networks including implicit connections are denser than those with only explicit connections (cf. Figure 1). This yields different centrality ranks including the most important characters (cf. Figure 2). For example, Horatio has many more implicit connections and connections to minor characters, which makes him the character with the highest degree centrality in the Haworth network. In the Moretti network, which contains only explicit connections, Hamlet has the highest degree centrality.Figure 2. Degree centralities for the Haworth (implicit connections, distant reading) and Moretti (explicit connections, close reading) networksDespite such individual differences, the groups of main characters derived from different networks exhibit relatively stable rankings, cf. (Fischer, Trilcke, Kittel, Milling, & Skorinkin, 2018). In contrast, rankings for minor characters tend to differ significantly (see Figure 3). Therefore, for detecting the group of main characters, the details of the network extraction method do not have a significant effect, at least in the datasets we consider. In future work we aim to validate the generality of this claim by considering larger corpora of dramatic plays.Figure 3. Heatmaps depicting the rank correlation between closeness centralities derived fromdifferent networks for all, the 10 most important, and the 10 least important charactersFinally, we outline some further observations about different centrality measures: In our datasets, degree centrality is the most robust, exhibiting high rank correlation for all considered sets of characters. In contrast, eigenvector centrality has the widest range of rank correlations suggesting its high sensitivity with respect to the network structure.AcknowledgementsThe authors gratefully acknowledge permissions to use material from Martin Grandjean and Roger Haworth. The work was funded by the HRSM project “KONDE – Kompetenznetzwerk Digitale Edition”. The work of Bernhard C. Geiger was partially funded by the Austrian Academy of Sciences within the go!digital Next Generation project “DiSpecs” (GDNG_2018-046_DiSpecs). The Know-Center is funded within the Austrian COMET Program – Competence Centers for Excellent Technologies – under die auspices of the Austrian Federal Ministry of Transport, Innovation and Technology, the Austrian Federal Ministry of Digital and Economic Affairs, and by the State of Styria. COMET is managed by the Austrian Research Promotion Agency FFG.","klimashevskaya.anastasia@gmail.com, geiger@ieee.org, hagmueller@tugraz.at, dhelic@tugraz.at, ffischer@hse.ru",Poster
"Ames, Sarah",National Library of Scotland,Developing the Data Foundry: the National Library of Scotland’s data-delivery platform,Developing the Data Foundry: the National Library of Scotland’s data-delivery platform,"libraries, data, open data, rights","Global, Europe, English, Contemporary, data publishing projects, systems, and methods, digital libraries creation, management, and analysis, Book and print history, Library & information science",English,"Global, Europe",Contemporary,"data publishing projects, systems, and methods, digital libraries creation, management, and analysis","Book and print history, Library & information science","The Collections as Data movement has gained significant traction in recent years, with large-scale projects leading the way in shaping and advocating for best practice (Padilla et al 2019). These studies, along with the OpenGLAM movement, have encouraged cultural heritage organisations to make collections available in machine readable formats and to support computational research with the collections, enabling libraries to cast new light on collections and present them in new ways for digital humanities audiences.However, while there have been a number of recent, essential studies around Collections as Data, as well as research into making collections available openly and the reasoning behind this (Pekel 2014, Terras 2015), there has been little to date from an institutional point of view about what is involved in opening up the collections in this way.How can libraries open up collections to wider audiences? How do we turn collections into data? What challenges does this present, relating to rights, access, and data management? What ethical considerations are needed and how can libraries be transparent about decision-making processes as they generate increasing amounts of data, becoming 'producers' of their own collections?This paper lifts the lid on the process of making data available in a national library context and considers the changes to existing activities, processes and outlook in releasing collections as data.The National Library of Scotland launched the Data Foundry (https://data.nls.uk/) in September 2019. As part of the Library’s Digital Scholarship Service, the Data Foundry provides access to data collections including digitised collections; metadata; map and spatial data; and organisational data; with further collections such as web archive and audiovisual data planned for future release.The Data Foundry is based on three core principles: open, transparent and practical (National Library of Scotland 2019 [1]). The platform was designed to be a clear, easy-to-use website, with tiered data downloads; clear rights information; and at-a-glance details contextualising the datasets.Collections on the Data Foundry are published openly, in reusable formats, and the Library does not assert further copyright over the datasets it produces (National Library of Scotland 2019 [2]). Furthermore, with transparency a key principle, the Data Foundry provides information about data provenance and the reasons behind why and how certain items have been digitised and ‘turned into’ data above others (Ames 2019).Producing the Data Foundry has been a Library-wide effort. Working at the intersection of collections, technology and research, the National Library of Scotland’s Digital Scholarship Service draws upon existing expertise across the Library – including Rights, Developers, Curators, Metadata – as well as working closely with researchers to understand their needs.This paper will highlight the practical side of opening up library collections for digital humanities use, exploring the everyday challenges and obstacles such as rights and technical issues and changes to workflows required to produce collections as data, as well as the broader implications of making collections available at scale for libraries and their users.",sarah.ames@nls.uk,Short Presentation
"Hubert, Hadassah St. (1); Rojas Castro, Antonio (2); Kraft, Tobias (2); Kraller, Kathrin (2); Afanador-Llach, María José (3); Levi, Amalia S. (4)","1: CLIR Postdoctoral Fellow-Digital Library of the Caribbean at Florida International University, USA; 2: Berlin-Brandenburgische Akademie der Wissenschaften (BBWA), Germany; 3: Universidad de los Andes, Colombia; 4: The HeritEdge Connection, Barbados",<em>Compartir lo que nos une</em>. Digitizing and Curating Colonial Records from the Caribbean and Central and South America for Public Outreach,Compartir lo que nos une. Digitizing and Curating Colonial Records from the Caribbean and Central and South America for Public Outreach,"colonialism, Caribbean, slavery, digitization, curation","South America, Comparative (2 or more geographical areas), Europe, English, Spanish, 18th Century, 19th Century, digital libraries creation, management, and analysis, History, Library & information science","English, Spanish","South America, Comparative (2 or more geographical areas), Europe","18th Century, 19th Century","digital libraries creation, management, and analysis","History, Library & information science","Compartir lo que nos une. Digitizing and Curating Colonial Records from the Caribbean and Central and South America for Public OutreachOverviewWhen digitizing and curating the colonial past, digital objects are not simply surrogates, but also ways of seeing and interpreting the world: slavery, colonization, and transculturation are embedded in any document, and if interrogated, their digital representations can become tools for understanding the impact of imperialism and colonialism in the present.Consequently, digitization and curation of nineteenth-century Caribbean and Central American collections give rise to new questions and challenges in the field of DH: How can we preserve endangered documents while keeping in mind their present-day needs & concerns? How can we digitize and give access to documents dispersed across international institutions to create meaningful collections around slavery? How can we curate data to imagine and visualize space beyond--often artificially imposed--geographical and temporal constraints? Or how can create digital records about oppressed people that transcend their oppressors?This panel brings together a diverse group of researchers from the Caribbean, South America, Central America, and Europe in order to place digitization and curatorial practices within the analytic framework of cultural encounter described as “post-colonial computing” (Dourish, 2010: 91) and of the post-custodial institutional partnership model (Kelleher, 2017; Alpert-Abrams, Bliss, & Carbajal, 2019). While these encounters and partnerships enable new and exciting collaborations, their processes, epistemological conditions, and products should be critically examined in order not to replicate neo-colonial attitudes often ingrained in digital technologies, and instead support local communities and promote public outreach.The panel addresses one of the main themes of the conference--“the historical and continued impacts of colonialism, postcolonialism, and hegemony”--and talks will be both in English (Hubert and Levi) and Spanish (Rojas Castro, Kraft and Kraller, and Afanador-Llach); presenters will provide translations to facilitate dialogue between participants and audience.Protecting Haitian Patrimony Initiatives at the Digital Library of the Caribbean (dLOC)Hadassah St. HubertCLIR Postdoctoral FellowDigital Library of the Caribbean at Florida International University hsthuber@fiu.eduHow can we protect and preserve endangered archives in Haiti? In 2010, Digital Library of the Caribbean (dLOC) launched the Protecting Haitian Patrimony Initiative, which builds on strong, existing long term partnerships, with an emphasis on accountability and transparency. This talk will focus on the numerous digitization projects, digital exhibits created since the launch of the initiative, and how dLOC engages the public.dLOC built Haiti: An Island Luminous, a tri-lingual website to help readers learn about Haiti’s history. “Haiti: An Island Luminous” combines rare books, manuscripts, and photos scanned by archives and libraries in Haiti and the United States with commentary by over one hundred (100) authors from universities around the world. “Haiti: An Island Luminous” contextualizes hundreds of historical books, documents and photos digitally preserved by dLOC’s partners, including the National Archives of Haiti, the National Library of Haiti, Haitian Library of the Fathers of the Holy Spirit (Pères du St-Esprit), Haitian Library of the Brothers of Christian Instruction (Frères de l’Instruction Chrètienne), University of Florida, Brown University, and University of Central Florida.dLOC currently hosts over 40,000 titles with more than four million pages of content, much of which is accessible content related to Haiti. Since the launch of “Haiti: An Island Luminous”, exhibit stations were placed in the Little Haiti Cultural Center, Nova Southeastern University’s Museum of Art, Sant La Haitian Neighborhood Center and more recently at North Miami Public Library in an effort to reach an even larger audience.dLOC partners in Haiti have been able to contribute more digital content due to the efforts of many scholars that have collaborated to apply for Endangered Archive Programme grants, as well as support from the World Bank, the U.S. Embassy in Haiti, and the Haitian Studies Association. All of these efforts have made dLOC the largest open access repository of Caribbean content and a significant resource for finding materials from and about the Caribbean for use in teaching, research, cultural and community life.Reconstruyendo la huella de Humboldt en Cuba. Retos y oportunidades de la digitalización del patrimonio documental cubano-alemán del siglo XIXAntonio Rojas Castro, Tobias Kraft y Kathrin KrallerBerlin-Brandenburgische Akademie der Wissenschaftenantonio.rojas-castro@bbaw.dekraft@bbaw.dekathrin.kraller@bbaw.deGrisel Terrón, Eritk Guerra y Alaina SolernouOficina del Historiador de la Ciudad de La Habanagrisel@patrimonio.ohc.cueritk@dic.ohc.cualaina@patrimonio.ohc.cuCon esta comunicación pretendemos debatir sobre los principales retos y oportunidades que supone la cooperación, en el marco del Proyecto Humboldt Digital (ProHD), entre la Academia de las Ciencias y las Humanidades de Berlín (BBAW) y la Casa Humboldt de la Oficina del Historiador de la Ciudad de La Habana (OHCH). En concreto, nos gustaría responder a la siguiente pregunta: ¿cómo podemos cooperar para el beneficio mutuo (Sennett, 2012)? A continuación, analizaremos el estado actual de la digitalización y las Humanidades Digitales en Cuba. Por último, expondremos los primeros avances orientados hacia la construcción de un repositorio digital y la edición de documentos.El futuro repositorio digital de ProHD tiene por objetivo preservar y dar acceso en línea a varias colecciones de documentos de naturaleza transnacional y multilingüe (español, francés y alemán), que permitan reconstruir no solo la “huella” del científico prusiano, sino también el pensamiento de muchos intelectuales y políticos locales, como Francisco Arango y Parreño, y las condiciones en que se producía el negocio esclavista, por ejemplo, mediante la digitalización de facturas de compra y venta. De esta manera, al poner la figura de Alexander von Humboldt en su contexto histórico, los usuarios obtendrán una mirada nueva y completa al problema de la esclavitud y al funcionamiento del sistema colonial, que condenaba a Cuba al monocultivo y a importar bienes de primera necesidad procedentes de la Metrópolis.A fin de obtener digitalizaciones y ediciones académicas digitales fáciles de encontrar, accesibles, interoperables y sostenibles, se han planeado tres medidas principales, que están implementándose desde junio de 2019: en primer lugar, se ha adquirido equipamiento para reforzar la infraestructura tecnológica (ordenadores, escáneres de alta resolución, servidor, impresora, mobiliario, etc.) de la Casa Humboldt en La Habana en donde se llevará a cabo la digitalización de los documentos; en segundo lugar, se ha formado a los miembros del equipo mediante cursos (en línea, sobre todo a partir del confinamiento provocado por la COVID-19) sobre el proceso de digitalización, la creación de metadatos y la gestión de repositorios digitales. Por último, se han definido de manera conjunta unos criterios de selección de documentos, una política de digitalización y un flujo de trabajo compartido.Inventar el virreinato de la Nueva Granada: curaduría crítica de fuentes primarias y la construcción de conjuntos de datos espaciales, 1739-1810María José Afanador-LlachUniversidad de los Andes, Colombiamj.afanador28@uniandes.edu.coLa invención del virreinato del Nuevo Reino de Granada (hoy Venezuela, Colombia, Ecuador y Panamá) en 1739 respondió a un proyecto económico de la monarquía española para proteger el norte de Suramérica de incursiones extranjeras y extraer mayores recursos para la corona. Dicho proyecto de búsqueda de unidad administrativa y geográfica se nutrió de la producción de conocimiento sobre economía política, es decir, sobre cómo crear riqueza en contextos de competencia. En fuentes primarias como mapas, textos sobre economía y descripciones geográficas creados por burócratas, naturalistas y militares en la colonia, se evidencian las dificultades de integrar un territorio montañoso y extenso y la importancia de la imaginación geográfica en este proceso.El proyecto de integración territorial del virreinato se rompió con la crisis monárquica de 1808 cuando el reino se fragmentó en más de una docena de provincias autónomas. La crisis representó una oportunidad para que pueblos y ciudades avanzaran sus proyectos económicos locales y se imaginaran espacios económicos post-coloniales. La pregunta central que guía este proyecto es: ¿cómo se puede visualizar la relación entre la búsqueda de unidad territorial en el norte de Suramérica y la diversidad de paisajes y economías políticas en tensión en una interfaz de archivo y un mapa digital?Esta ponencia explora los avances de este proyecto en sus dos primeras fases. La primera fase consiste en la curaduría crítica de fuentes primarias entre 1739 y 1810. Esta curaduría consiste en la selección de documentos, de fragmentos dentro de los documentos y de mapas del periodo, para organizarlos, enriquecer sus metadatos y disponerlos alrededor de un argumento y una narrativa histórica. Algunos de estos documentos se encuentran disponibles en libros editados, otros están digitalizados y un número pequeño se digitalizará. La segunda fase consiste la formulación de un proceso metodológico para extraer datos sobre las dimensiones espaciales y de economía política de este conjunto curado de fuentes primarias. Se trata de generar conjuntos de datos relacionales que permitan visualizar, analizar e interpretar transformaciones históricas-espaciales de la invención y crisis del virreinato de la Nueva Granada y los actores involucrados en este proceso. El proyecto busca que tanto la curaduría de fuentes primarias como los conjuntos de datos sean recursos abiertos en español para la investigación y docencia sobre el periodo. La ponencia discutirá los retos a los que se ha enfrentado el proyecto hasta el momento en términos de la curaduría de documentos y la definición metodológica para la extracción de datos.Beyond digitization: Engaging the Community to Decolonize the Archival Record for the EnslavedAmalia S. LeviArchivist, ChairThe HeritEdge Connection, Barbadosamalia@heritedge.foundationDuring 2018 and 2019, two historic newspapers, The Barbados Mercury Gazette (1783-1848) and The Barbadian (1822-1861), housed in the Barbados Department of Archives, were digitized through Endangered Archives Programme grants. As primary sources, they offer a detailed view of every aspect of the dystopian life in a British colony in the Caribbean during the eighteenth and nineteenth centuries. Digitization offers unparalleled access to these colonial newspapers that were previously largely inaccessible due to their fragility.The presentation goes beyond digitization to discuss what happens after digitization is completed. Considering that the newspapers were part of the colonial information apparatus, how do we read against the grain to locate and reveal marginalized voices hidden in the digitized pages?If our aim is to “lift” the voices of the enslaved from the pages, and retrospectively create a body of archival records, we need to provide access to the digitized pages as data. Such information however exists “locked” in digitized images that due to the deterioration of the paper and the discoloration of the pages are impossible to OCR. Finding and transcribing these ads is necessarily a manual process, at present.Among the wealth of information appearing regularly in the newspapers, of particular importance are the “runaway slaves” ads. Archives usually preserve very little descriptive information about the enslaved, because their lived experiences were rarely recorded. When they appear in the archival record, they are enumerated or appraised as commodities. Thus these ads offer a rich trove of information about individual people, including name, age, physical appearance, skin color, clothing, accent, distinguishing features (such as body modifications from their country of origin or bodily harm, the result of violence), friends, relatives, and skills.Digitization does not solve the gaps, silences, and omissions inherent in the archival record and the colonial epistemologies they contain. Colonial newspapers reflect the voices of the white class of planters, merchants, and colonial authorities. Simply digitizing and putting online risks elevating and amplifying the very colonial worldviews we mean to avoid.To decolonize the record about the enslaved, digitization is only the starting point for further projects and initiatives to engage the community with its own history. Recent scholarship and public humanities efforts have shown that it is possible to challenge the erasure of colonial archival sources and read between the lines to tease out information that is not readily visible.  Our aim is to create a collection of material by clipping “runaway slaves” ads from the digitized newspapers, transcribing the text, and enriching the human stories in each ad with additional contextual information. More specifically, the first part of the presentation will focus on work to develop the “Barbados Runaway Slaves Digital Collection,” a partnership between the Barbados Archives, the local non-profit HeritEdge Connection, and the Early Caribbean Digital Archive (ECDA) at Northeastern University (Boston).This digital collection aims to foreground the centrality of enslaved voices by ‘lifting’ ads of individuals who chose to escape slavery from the pages of the newspapers, and turning them into individual, standalone archival records previously unrecorded in the archives. When completed, the “Runaway Slaves Digital Collection” will provide a central location for collecting and presenting these ads, and other opportunities for the public, especially students, both in Barbados, as well as abroad, to interact with the material in creative ways.By ‘reading’ these ads in various ways and being able to ‘see’ them collectively, we can start seeing patterns and coping mechanisms. At the same time, the availability of this information points to what is not there, and invites us to be sensitive to gaps and silences.The second part of the presentation will discuss public outreach initiatives. Material digitized through generous grants by institutions in the Global North are eventually hosted in digital platforms in those institutions. While praiseworthy, digitization through such grants ends up benefiting scholars in the Global North. Usually local people are unaware of these platforms, and often unable to access them, either due to bandwidth issues or simply to interfaces that might not be intuitive to use. We have tried to remedy this by focusing on public history work we are doing to increase awareness of and engagement with these digitized colonial records.During the fall of 2019, we conducted a series of workshops aimed to familiarize the public with accessing the newspapers online and to transcribe ads. The workshops also provided a platform for the public to discuss the ads, and the many facets of slavery. People were able to see ancestors who chose to resist and escape bondage in adverse, inhumane conditions. Due to COVID-19, workshops planned for 2020 are being held online. The aim of these workshops is to engage people with the ads in creative ways, through genealogical research, speculative writing, or digital methods. In this way, we invite the public to contextualize the ads through their local knowledge of places mentioned in the ads, or intimate information about lived experiences. Beyond information that is there, we also hope that workshop participants can imagine what is not there, complete the stories, and give enslaved individuals their place in the archival record.Additionally, the digitization of these primary sources has great potential for digital projects by students and other researchers that can highlight various aspects of the island’s history. The “runaway slaves” ads help people challenge the customary narrative of Barbadian passivity and submissiveness to slavery; reconstruct family and community networks that supported enslaved Barbadians; and they help give voice to ancestors, whom colonial records intentionally left voiceless.","hsthuber@fiu.edu, antonio.rojas-castro@bbaw.de, kraft@bbaw.de, kathrin.kraller@bbaw.de, mj.afanador28@uniandes.edu.co, amalia@heritedge.foundation",Panel
"Beals, Melodee; Bell, Emily","Loughborough University, United Kingdom",Exploring the Atlas of Digitised Newspapers: Enhancing Access to and Collaborative Research with Digitised Historical Newspapers,Exploring the Atlas of Digitised Newspapers: Enhancing Access to and Collaborative Research with Digitised Historical Newspapers,"digitisation, newspapers, metadata, history, literature","Global, English, 19th Century, 20th Century, Contemporary, meta-criticism (reflections on digital humanities and humanities computing), metadata standards, systems, and methods, History, Library & information science",English,Global,"19th Century, 20th Century, Contemporary","meta-criticism (reflections on digital humanities and humanities computing), metadata standards, systems, and methods","History, Library & information science","Building upon the two-year Digging into Data project, Oceanic Exchanges: Tracing Global Information Networks in Historical Newspaper Repositories, 1840-1914 (http://www.oceanicexchanges.org), this workshop will introduce participants to the Atlas of Digitised Newspapers, a comprehensive guide to the histories, structures and metadata of the digitised newspapers collections studied by the project, including those held by:Chronicling AmericaThe Hemeroteca Nacional Digital de MéxicoThe British LibraryThe Times Digital ArchiveDelpherEuropeanaSuomen Kansalliskirjaston Digitoidut SanomalehdetTrovePapers PastZEFYSThe Atlas provides readers with a deep contextualisation of these collections as well as detailed technical information about how to obtain, interpret, manipulate and map metadata and content across collections. Discussions will be supported by two additional online resources–an interactive visualisation of the metadata mapping and a linked dataset to support cross-database research.Starting with humans rather than technologies or tools, the Atlas specifically uncovers the people behind the metadata and selection decisions. The workshop demonstrates a commitment to continuously co-constructing this work, and we hope to foster future collaborations. The Atlas will encourage research that “analyzes the historical and continued impacts of colonialism, postcolonialism, and hegemony”; siloed national archives privilege collections that look inward, but Oceanic Exchanges has used the combined collections to explore multilingual/multicultural groups with future possibilities for work on comparative Indigenous studies.The aims of the workshop are to:Present the findings and outputs of the project;Create a space for critical discussion of the findings and future research opportunities by participants of the workshop;Allow for initial collaborations and hands-on use of the outputs in developing historical, literary, and computer science research questions. All participants will have the opportunity to submit and review the following in advance of the workshop:An online profile of previous work with digitised newspapers and historical periodicals.Suggestions for possible in-workshop collaborations using the Atlas, visualisations and LOD datasets across our core expected disciplines.Learning Outcomes By the end of the workshop, participants will:Develop a nuanced understanding of available data beyond OCR text;Practice cross-collection data management and cleaning;Develop an understanding of metadata and selection practices.","m.h.beals@lboro.ac.uk, e.bell@lboro.ac.uk",Workshop/Tutorial 4