authors,organisations,title_plain,keywords,topics,tg1_Language,tg3_Geography,tg2_Temporal,tg4_Methods,tg5_Disciplines_Fields_of_Study,abstract,abstract_plain,all_emails,acceptance
"Hoover, David Lowell","New York University - Main Campus, United States of America","Booth Tarkington, Blindness, Dictation, and the Durability of Style","style, dictation, literary composition","English
North America
19th Century
20th Century
attribution studies and stylometric analysis
Literary studies",English,North America,"19th Century
20th Century",attribution studies and stylometric analysis,Literary studies,"<p>When Booth Tarkington suffered from severe vision problems and temporary complete blindness in 1929, he began to dictate almost all of his literary works, a practice he continued even after regaining good eyesight in 1931. This submission investigates the possible effects of this change in mode of composition on Tarkington's style. The tentative conclusion is that there is almost no evidence of any effect, which suggests that authorial style can be quite durable in the face of a complete change in the way the author produces his text.</p>
","When Booth Tarkington suffered from severe vision problems and temporary complete blindness in 1929, he began to dictate almost all of his literary works, a practice he continued even after regaining good eyesight in 1931. This submission investigates the possible effects of this change in mode of composition on Tarkington's style. The tentative conclusion is that there is almost no evidence of any effect, which suggests that authorial style can be quite durable in the face of a complete change in the way the author produces his text.",david.hoover@nyu.edu,Short Presentation
"Hoover, David Lowell","New York University - Main Campus, United States of America",Testing Rolling.Classify,"Rolling Classify, collaboration, simulation","Europe
English
North America
19th Century
20th Century
Contemporary
attribution studies and stylometric analysis
Humanities computing
Literary studies",English,"Europe
North America","19th Century
20th Century
Contemporary",attribution studies and stylometric analysis,"Humanities computing
Literary studies","<p>Rolling.Classify is a recently developed tool for studying collaboration (Eder, Rybicki, and Kestemont 2016; Eder 2016) that builds on earlier work that tested successive overlapping sections of texts (van Dalen-Oskam and van Zundert 2007, Burrows 2010, Hoover 2012).</p>
<p>The power and ease of use of Rolling.Classify (and its related Rolling.Delta) have led to several studies based on various kinds of texts.. Rigorous testing of this new method on problems with known solutions seems especially important because its results vary greatly with the choice of classification method other parameters. I will begin with simulated collaborations comprising text sections of varied lengths assembled to model different kinds of collaboration. I will then test collaborations with known contributions by the authors, and finally some in which no clear evidence of the nature of the collaboration exists.</p>
","Rolling.Classify is a recently developed tool for studying collaboration (Eder, Rybicki, and Kestemont 2016; Eder 2016) that builds on earlier work that tested successive overlapping sections of texts (van Dalen-Oskam and van Zundert 2007, Burrows 2010, Hoover 2012).
The power and ease of use of Rolling.Classify (and its related Rolling.Delta) have led to several studies based on various kinds of texts.. Rigorous testing of this new method on problems with known solutions seems especially important because its results vary greatly with the choice of classification method other parameters. I will begin with simulated collaborations comprising text sections of varied lengths assembled to model different kinds of collaboration. I will then test collaborations with known contributions by the authors, and finally some in which no clear evidence of the nature of the collaboration exists.",david.hoover@nyu.edu,Short Presentation
"Anderson, Talea","Washington State University, United States of America","Effect of Promotion, Rank, and Tenure Guidelines on Open Data Distribution",open data,"English
North America
Contemporary
data publishing projects, systems, and methods
data, object, and artefact preservation
Library & information science",English,North America,Contemporary,"data publishing projects, systems, and methods
data, object, and artefact preservation",Library & information science,"<p>Promotion, rank, and tenure (PRT) guidelines have been cited as a key motivation for why and how faculty choose to publish their research. Open-access (OA) advocates have noted in particular that lukewarm or negative portrayals of OA venues in PRT guidelines can result in decreased participation in OA publishing. In response to this concern, some universities have begun to experiment with adding language to PRT guidelines that invites broader participation in publishing venues, including OA publishing. Of interest in this study is the impact of PRT guidelines on the distribution of open data. Specifically, this poster will rely on information gathered from SHARE to consider the prevalence of participation in open data publishing at schools that include more expensive PRT guidelines.</p>
","Promotion, rank, and tenure (PRT) guidelines have been cited as a key motivation for why and how faculty choose to publish their research. Open-access (OA) advocates have noted in particular that lukewarm or negative portrayals of OA venues in PRT guidelines can result in decreased participation in OA publishing. In response to this concern, some universities have begun to experiment with adding language to PRT guidelines that invites broader participation in publishing venues, including OA publishing. Of interest in this study is the impact of PRT guidelines on the distribution of open data. Specifically, this poster will rely on information gathered from SHARE to consider the prevalence of participation in open data publishing at schools that include more expensive PRT guidelines.",talea.anderson@wsu.edu,Poster
"Palladino, Chiara (1);
Zhang, Anna (1);
Foradi, Maryam (2);
Yousef, Tariq (3)","1: Furman University, United States of America;
2: Digital Humanities, University of Leipzig, Germany;
3: NLP Group, University of Leipzig, Germany",How to Read All Languages: Translation Alignment with Ugarit,"translation alignment, digital pedagogy, language learning","Comparative (2 or more geographical areas)
English
BCE-4th Century
5th-14th Century
15th-17th Century
electronic literature production and analysis
public humanities collaborations and methods
Literary studies
Translation studies",English,Comparative (2 or more geographical areas),"BCE-4th Century
5th-14th Century
15th-17th Century","electronic literature production and analysis
public humanities collaborations and methods","Literary studies
Translation studies","<p>This workshop will illustrate the importance of translation alignment in the field of slow reading and language learning. We will provide a short theoretical overview on the principles of translation alignment, together with a hands-on tutorial on Ugarit (http://ugarit.ialigner.com/), a web-based translation alignment editor. Ugarit is designed as a Citizen Science tool, aiming at collecting training datasets of manually aligned words from diverse text corpora. The ultimate goal of Ugarit is to improve automatic translation alignment methods and to implement a set of dynamic lexica, with particular regard for languages with less supported infrastructures. However, the tool also has a strong pedagogical potential, which has been assessed in the course of various hands-on workshops and in an ongoing integration in school curricula: we have tested how translation alignment with Ugarit can help readers to engage with languages that they have never seen, grasping their essential semantic and morphological aspects. We propose text alignment as a way to empower the perception of the complexity of a language, but also as a method to leverage usual obstacles in the process of reading apparently “impenetrable” sources by directly engaging with them. At present, Ugarit includes aligned pairs from 36 languages (including less represented languages, like Bulgarian, Ethiopic, Sanskrit, Yiddish, and Armenian), 277 unique users, and about 23,400 parallel texts hosted.</p>
","This workshop will illustrate the importance of translation alignment in the field of slow reading and language learning. We will provide a short theoretical overview on the principles of translation alignment, together with a hands-on tutorial on Ugarit (http://ugarit.ialigner.com/), a web-based translation alignment editor. Ugarit is designed as a Citizen Science tool, aiming at collecting training datasets of manually aligned words from diverse text corpora. The ultimate goal of Ugarit is to improve automatic translation alignment methods and to implement a set of dynamic lexica, with particular regard for languages with less supported infrastructures. However, the tool also has a strong pedagogical potential, which has been assessed in the course of various hands-on workshops and in an ongoing integration in school curricula: we have tested how translation alignment with Ugarit can help readers to engage with languages that they have never seen, grasping their essential semantic and morphological aspects. We propose text alignment as a way to empower the perception of the complexity of a language, but also as a method to leverage usual obstacles in the process of reading apparently “impenetrable” sources by directly engaging with them. At present, Ugarit includes aligned pairs from 36 languages (including less represented languages, like Bulgarian, Ethiopic, Sanskrit, Yiddish, and Armenian), 277 unique users, and about 23,400 parallel texts hosted.","chiara.palladino@furman.edu, anna.zhang@furman.edu, maryam.foradi@uni-leipzig.de, tariq@informatik.uni-leipzig.de",Workshop/Tutorial 4
"Bordalejo, Barbara (1);
O'Donnell, Daniel Paul (2)","1: University of Saskatchewan, Canada;
2: University of Lethbridge, Canada",Diversity and Inclusion for Digital Humanists,"Diversity, Inclusion, Bias, Privilege","Global
English
Contemporary
digital activism and advocacy
Disability and differently-abled studies
Feminist studies",English,Global,Contemporary,digital activism and advocacy,"Disability and differently-abled studies
Feminist studies","<p>This workshop highlights issues of diversity, inclusivity, and collaboration in Digital Humanities.</p>
<p>Through practical exercises and dialogue, we build a safe atmosphere for the discussion of strategies to isolate obstacles preventing diversity and offer solutions for the development of inclusive environments.</p>
<p>As part of our work, we developed The Privilege Game, used to emphasize and showcase the many different kinds of privilege derived from our society's power structures and to create awareness among practitioners of the contrasts to be found in the rich and ever-growing space of the Digital Humanities.</p>
<p>We cover topics such as ""implicit bias,"" ""cultural cloning,"" privilege, intersectionality and solutions to be implemented in the creation of safe and inclusive environments.</p>
","This workshop highlights issues of diversity, inclusivity, and collaboration in Digital Humanities.
Through practical exercises and dialogue, we build a safe atmosphere for the discussion of strategies to isolate obstacles preventing diversity and offer solutions for the development of inclusive environments.
As part of our work, we developed The Privilege Game, used to emphasize and showcase the many different kinds of privilege derived from our society's power structures and to create awareness among practitioners of the contrasts to be found in the rich and ever-growing space of the Digital Humanities.
We cover topics such as ""implicit bias,"" ""cultural cloning,"" privilege, intersectionality and solutions to be implemented in the creation of safe and inclusive environments.","barbara.bordalejo@usask.ca, daniel.odonnell@uleth.ca",Workshop/Tutorial 4
"Boateng, Akwasi Bosompem","North-West University, South Africa",Social media in political engagements in Africa: A study of Twitter use in the intra-party elections of political parties in Ghana ,"Relationship Management, Elections, Political Engagement, Social Media, Twitter","Africa
English
Contemporary
social media analysis and methods
Communication studies
Political science",English,Africa,Contemporary,social media analysis and methods,"Communication studies
Political science","<p><em>The advent of social media has digitalised democracies, transforming political communication and election campaigns. This article explores how the New Patriotic Party and National Democratic Congress use social media particularly Twitter in intra-party elections. There is a considerable rise in political vigilantism in Ghana, which some have attributed to lack of direct access to parties and officials due to limited time for stakeholders during “phone-in” segments on political programmes in traditional media. With social media as popular response to these challenges, new questions emerge regarding technological appropriation for the advancement of political agenda through relationship management. This article explores “Twittering” in political communication and engagements; how political parties use Twitter especially during their 2018 intra-party elections in Ghana. It provides an overview of the social media era situating it into political communication. </em></p>
","The advent of social media has digitalised democracies, transforming political communication and election campaigns. This article explores how the New Patriotic Party and National Democratic Congress use social media particularly Twitter in intra-party elections. There is a considerable rise in political vigilantism in Ghana, which some have attributed to lack of direct access to parties and officials due to limited time for stakeholders during “phone-in” segments on political programmes in traditional media. With social media as popular response to these challenges, new questions emerge regarding technological appropriation for the advancement of political agenda through relationship management. This article explores “Twittering” in political communication and engagements; how political parties use Twitter especially during their 2018 intra-party elections in Ghana. It provides an overview of the social media era situating it into political communication.",beebeeboateng@gmail.com,Poster
"Heintzman, Kit","Harvard University, Canada",Greening the Digital Humanities,"carbon emissions, environmentalism","Global
English
Contemporary
eco-criticism and environmental analysis
sustainable procedures, systems, and methods
History of science",English,Global,Contemporary,"eco-criticism and environmental analysis
sustainable procedures, systems, and methods",History of science,"<p>This workshop would introduce audiences to the following issues: recent historiography attention to the internet's carbon footprint, a case study in a semester long system of tracking the carbon emissions of a digital humanities course, trnasferable pedagogical strategies with regard to eco-conscious digital scholarshp, and critical reflections upon issues of ""individualism"" in discourse of digital consumption patterns versus system changes.</p>
","This workshop would introduce audiences to the following issues: recent historiography attention to the internet's carbon footprint, a case study in a semester long system of tracking the carbon emissions of a digital humanities course, trnasferable pedagogical strategies with regard to eco-conscious digital scholarshp, and critical reflections upon issues of ""individualism"" in discourse of digital consumption patterns versus system changes.",kheintzman@fas.harvard.edu,Workshop/Tutorial 2
"Ohya, Kazushi","Tsurumi University, Japan","An online course system easy to make, preserve, and promote critical thinking","radio lecture system, HTML Imports, Web Components","Global
English
Contemporary
data publishing projects, systems, and methods
sustainable procedures, systems, and methods
Education/ pedagogy
Media studies",English,Global,Contemporary,"data publishing projects, systems, and methods
sustainable procedures, systems, and methods","Education/ pedagogy
Media studies","<p>In this poster presentation we will show a new online course consisting of talks and chalks that is easy to make, edit, and preserve, and is a traditional and old-fashioned lecture style, that substantially helps students learn the content spontaneously. The course materials we need to make are only scripts for talks and XML data for slides, and the process we need to undertake separately is just recording the talk. The mechanisms to realize this course are backed with HTML5 and simple codes in JavaScript based on the specification HTML Imports. This lecture system is important not only for an implementation to realize a new teaching/learning channel conveying the knowledge instantly to learners but also to show the evidence for usefullness of HTML Imports as a sign of the existence of users' requirements.</p>
","In this poster presentation we will show a new online course consisting of talks and chalks that is easy to make, edit, and preserve, and is a traditional and old-fashioned lecture style, that substantially helps students learn the content spontaneously. The course materials we need to make are only scripts for talks and XML data for slides, and the process we need to undertake separately is just recording the talk. The mechanisms to realize this course are backed with HTML5 and simple codes in JavaScript based on the specification HTML Imports. This lecture system is important not only for an implementation to realize a new teaching/learning channel conveying the knowledge instantly to learners but also to show the evidence for usefullness of HTML Imports as a sign of the existence of users' requirements.",oya-k@tsurumi-u.ac.jp,Poster
"Calvo Tello, José",Göttingen State and University Library," What is a Genre? A Graph Unified Model of Categories, Texts, and Features ","genre, model, graph, novels, Spanish","Comparative (2 or more geographical areas)
Europe
English
BCE-4th Century
19th Century
20th Century
cultural analytics
text mining and analysis
Literary studies",English,"Comparative (2 or more geographical areas)
Europe","BCE-4th Century
19th Century
20th Century","cultural analytics
text mining and analysis",Literary studies,"<p align=""justify"">Several theoretical models have been proposed for genre, such as the Aristotelian scholastic taxonomy, the family resemblance and the prototype theory. However, these models lack of empirical applications to real examples of genres. This proposal is the culmination of a series of analysis, presenting a theoretical, computational and visual graph-based model that fits several observations. This formalization unifies components of the previous theories, offering visually the intention (internal features) and extension (the best representatives and instances) of each category. Besides, it allows two intuitive interpretations based on the evaluation: the centrality as classification results, and the distance as similarity through shared features. The model is applied to three data-sets of different periods and languages: modern Spanish novels, classic French plays and the books of the Bible.</p>
","Several theoretical models have been proposed for genre, such as the Aristotelian scholastic taxonomy, the family resemblance and the prototype theory. However, these models lack of empirical applications to real examples of genres. This proposal is the culmination of a series of analysis, presenting a theoretical, computational and visual graph-based model that fits several observations. This formalization unifies components of the previous theories, offering visually the intention (internal features) and extension (the best representatives and instances) of each category. Besides, it allows two intuitive interpretations based on the evaluation: the centrality as classification results, and the distance as similarity through shared features. The model is applied to three data-sets of different periods and languages: modern Spanish novels, classic French plays and the books of the Bible.",jose.calvo@uni-wuerzburg.de,Long Presentation
"Hannesschläger, Vanessa;
Wissik, Tanja","Austrian Centre for Digital Humanities, Austrian Academy of Sciences, Austria",Opening up Open Data: Strategies & success stories,"Open Data, Open Source, Hackathon, Community involvement","Global
Europe
English
Contemporary
open access methods
public humanities collaborations and methods
Humanities computing",English,"Global
Europe",Contemporary,"open access methods
public humanities collaborations and methods",Humanities computing,"<p dir=""ltr"">Not only in the context of Digital Humanities but also in other research areas the Open Data movement is gaining momentum. For this reason, the Austrian Centre for Digital Humanities of the Austrian Academy of Sciences (ACDH) started an experiment at the beginning of 2019: For the first time ever, we published the calls for participation in three virtual hackathons funded by the Austrian manifestations of DARIAH and CLARIN, CLARIAH-AT. These hackathons focused on Open Data sets that are publicly available online, and the tasks to perform on these data involved the creation of Open Source code. Each of the hackathons had a special theme and was co-timed with an event that involved an aspect of Openness. These events also inspired the choice of the respective data sets.</p>
<p dir=""ltr"">Usually hackathons take place on site, participants are given tasks to be solved within a given timeframe in a fixed location. This requires the programmers to be flexible and available and to have access to travel funding. A virtual hackathon on the other hand offers people all over the globe the possibility to participate and contribute without having to travel. Therefore, our approach enabled a much larger community to participate in the event on the one hand, thus also promoting the benefits of Open Data on the other.</p>
<p dir=""ltr"">The first hackathon was carried out in cooperation with the European Lexicographical Infrastructure (ELEXIS) and focused on lexicographical data (the Digital Dictionary of Tunis Arabic). The task was to develop a creative mode of processing it, e.g. by enriching it, visualizing it, doing statistical analysis, or integrating it with other resources (e.g. LOD). The submissions were to be handed in by the end of the first ELEXIS observer event taking place in February 2019.</p>
<p dir=""ltr"">For the second hackathon, the ACDH cooperated with the City of Vienna and chose a set from the city’s Open Government Data platform to be processed. The task was to be completed on the Open Data Day Vienna 2019 (28 February 2019). The aim was to develop a creative mode of processing cartographical data showing damage to buildings in Vienna during World War II.</p>
<p dir=""ltr"">The third and final hackathon of the series offered a task to be completed on the International Open Data Day 2019 (2 March 2019). For this final highlight, a choice of two Open Data sets was offered to the participants. The first data set to be worked on in this task was a collection of XML/TEI transcriptions of early German travel guides. The second data set consisted of German historical newspapers and was provided in cooperation with the Europeana newspaper project.</p>
<p dir=""ltr"">The best contributions were determined by an international board of judges and received cash prizes. The criteria for judgement were creativity and innovation, accessibility, reusability and reproducibility, as well as elegance. In our presentation, we will share the lessons learned and show how Open Science was the necessary precondition for this project, as well as what inspired its ultimate success. We will make our case by giving insights into the lessons learned in 2019 and sharing how we improved the concept for the second round of the hackathon series in 2020.</p>
<p dir=""ltr""><strong>Bibliography</strong></p>
<p dir=""ltr"">ACDH-CH. ACDH Virtual Hackathon Series. 2019. https://www.oeaw.ac.at/acdh/detail/event/acdh-virtual-hackathon-series/</p>
<p dir=""ltr"">ACDH-CH. ACDH-CH Open Data Virtual Hackathon - Round Two. 2020. https://www.oeaw.ac.at/acdh/detail/event/acdh-ch-open-data-virtual-hackathon-round-two/</p>
<p dir=""ltr"">Vanessa Hannesschläger. The ACDH virtual hackathon series: Open Data for Open Source solutions. DARIAH Open Blog, 23. May 2019. https://dariahopen.hypotheses.org/571</p>
","Not only in the context of Digital Humanities but also in other research areas the Open Data movement is gaining momentum. For this reason, the Austrian Centre for Digital Humanities of the Austrian Academy of Sciences (ACDH) started an experiment at the beginning of 2019: For the first time ever, we published the calls for participation in three virtual hackathons funded by the Austrian manifestations of DARIAH and CLARIN, CLARIAH-AT. These hackathons focused on Open Data sets that are publicly available online, and the tasks to perform on these data involved the creation of Open Source code. Each of the hackathons had a special theme and was co-timed with an event that involved an aspect of Openness. These events also inspired the choice of the respective data sets.
Usually hackathons take place on site, participants are given tasks to be solved within a given timeframe in a fixed location. This requires the programmers to be flexible and available and to have access to travel funding. A virtual hackathon on the other hand offers people all over the globe the possibility to participate and contribute without having to travel. Therefore, our approach enabled a much larger community to participate in the event on the one hand, thus also promoting the benefits of Open Data on the other.
The first hackathon was carried out in cooperation with the European Lexicographical Infrastructure (ELEXIS) and focused on lexicographical data (the Digital Dictionary of Tunis Arabic). The task was to develop a creative mode of processing it, e.g. by enriching it, visualizing it, doing statistical analysis, or integrating it with other resources (e.g. LOD). The submissions were to be handed in by the end of the first ELEXIS observer event taking place in February 2019.
For the second hackathon, the ACDH cooperated with the City of Vienna and chose a set from the city’s Open Government Data platform to be processed. The task was to be completed on the Open Data Day Vienna 2019 (28 February 2019). The aim was to develop a creative mode of processing cartographical data showing damage to buildings in Vienna during World War II.
The third and final hackathon of the series offered a task to be completed on the International Open Data Day 2019 (2 March 2019). For this final highlight, a choice of two Open Data sets was offered to the participants. The first data set to be worked on in this task was a collection of XML/TEI transcriptions of early German travel guides. The second data set consisted of German historical newspapers and was provided in cooperation with the Europeana newspaper project.
The best contributions were determined by an international board of judges and received cash prizes. The criteria for judgement were creativity and innovation, accessibility, reusability and reproducibility, as well as elegance. In our presentation, we will share the lessons learned and show how Open Science was the necessary precondition for this project, as well as what inspired its ultimate success. We will make our case by giving insights into the lessons learned in 2019 and sharing how we improved the concept for the second round of the hackathon series in 2020.
Bibliography
ACDH-CH. ACDH Virtual Hackathon Series. 2019. https://www.oeaw.ac.at/acdh/detail/event/acdh-virtual-hackathon-series/
ACDH-CH. ACDH-CH Open Data Virtual Hackathon - Round Two. 2020. https://www.oeaw.ac.at/acdh/detail/event/acdh-ch-open-data-virtual-hackathon-round-two/
Vanessa Hannesschläger. The ACDH virtual hackathon series: Open Data for Open Source solutions. DARIAH Open Blog, 23. May 2019. https://dariahopen.hypotheses.org/571","vanessa.hannesschlaeger@oeaw.ac.at, tanja.wissik@oeaw.ac.at",Lightning
"SAIBU, ISRAEL ABAYOMI (1);
SAIBU, AYOMIDE JOSEPH (2)","1: ANCHOR UNIVERSITY, LAGOS NIGERIA.;
2: LAGOS STATE UNIVERSITY OJO, LAGOS NIGERIA",PRESERVATION OF OSUN-OSOGBO CULTURAL HERITAGE IN NIGERIA: A DIGITIZATION DISCOURSE,"Preservation, Digitization, Cultural Heritage, Osun-Osogbo, Nigeria","Africa
English
19th Century
Contemporary
digital archiving
digitization (2D & 3D)
Art history
History",English,Africa,"19th Century
Contemporary","digital archiving
digitization (2D & 3D)","Art history
History","<p>The Osun-Osogbo cultural heritage has become one of the most striking cultural identities to have emerged in Nigeria. The uniqueness and importance of Osun-Osogbo has led to its recognition by the United Nations Educational Scientific and Cultural Organization (UNESCO) as a global cultural heritage. Thus, it is imperative that this cultural heritage be digitized for the preservation of its essence for posterity and global visibility.</p>
","The Osun-Osogbo cultural heritage has become one of the most striking cultural identities to have emerged in Nigeria. The uniqueness and importance of Osun-Osogbo has led to its recognition by the United Nations Educational Scientific and Cultural Organization (UNESCO) as a global cultural heritage. Thus, it is imperative that this cultural heritage be digitized for the preservation of its essence for posterity and global visibility.","alerosaibu@gmail.com, ayomidesaibu2001@gmail.com",Lightning
"Herold, Nastasia (1,2);
Ottawa, Thérèse (2)","1: University of Leipzig, Germany;
2: Wikipetcia Atikamekw Nehiromowin, Canada",Ethics and responsibilities of open access - lessons learned from the Wikipedia project of the Atikamekw First Nation,"open access, wikipedia, atikamekw, first nations, collaborative","English
North America
Contemporary
digital access, privacy, and ethics analysis
open access methods
First nations and indigenous studies
Linguistics",English,North America,Contemporary,"digital access, privacy, and ethics analysis
open access methods","First nations and indigenous studies
Linguistics","<p align=""center"">Abstract by Nastasia Herold & Thérèse Ottawa<u></u></p>
<p align=""center""><u> </u></p>
<p align=""center""><u>Ethics and responsibilities of open access – lessons learned from the Wikipedia project of the Atikamekw First Nation</u></p>
<p><u> </u></p>
<p>With 97,9% (cf. INAC 2019), in Canada, the Atikamekw First Nation has the highest percentage of people who speak their native language (Atikamekw) at home. The Atikamekw live in Quebec, Canada, and have a population of 8,000 people in three communities.</p>

<p>In 2013, Nastasia Herold, one of the authors of this paper did a field study in Manawan, one of the three Atikamekw communities, for a research on the local bilingualism (Atikamekw and French). Despite the vitality of the Atikamekw language, a survey and interviews showed that francization and language change are processes noticed by all living generations of the Atikamekw (cf. Herold 2020: N.N.).</p>

<p>Communication takes place more and more often digitally (cf. Reichert 2017: 26-27), and this in written language rather than orally. Atikamekw has a standardized orthography since 1994 (cf. Dinnison <sup>2</sup>1997) and is taught in Manawan’s primary school as a first language and as medium of alphabetization. However, Herold’s (2019: 103) research in 2013 showed that the Internet contained no written text in the Atikamekw language, the Atikamekw used the Internet mainly in French. This is why a school project at Manawan’s secondary school was initiated in 2013 in order to create a Wikipedia site in the Atikamekw language.</p>

<p>Many lessons have been learned during the collaboration of academics, teachers, pupils, local language experts and other local voluntary contributors. The lessons we would like to focus on in this presentation are the lessons learned when implementing cultural knowledge to an open access platform. Open access is the free provision of (scientific) texts on the web without restriction of use (cf. Kohle 2017: 203), and this free provision has advantages and disadvantages.</p>

<p>We will give four examples which show that the Atikamekw made sure that the knowledge was published respecting their own principles, beliefs and tradition. These four examples and the development of the project show how it is important that the community itself dictates rules that have to be respected when publishing their knowledge under open access.</p>

<p>Finally we will answer Rehbein’s and Thies’ (2017: 355) question they developed as a schema for questions of responsibilities and ethics of a specific project: Who (1) is responsible for what (2) to whom (3) before which instance (4) according to which standards (5)?</p>

<p><u>References</u></p>
<p>Dinnison, Bonnie (<sup>2</sup>1997): <em>Guide orthographique de la langue atikamekw</em>. La Tuque: Atikamekw Sipi.</p>
<p>Herold, Nastasia (2019): “Kulturbedingte Herausforderungen für schulische und gesellschaftliche Teilhabe in indigenen Reservaten Kanadas: Das Beispiel des Atikamekw-Dorfes Manawan in Québec”, in: Jahr, David / Kruschel, Robert (eds.): <em>Inklusion in Kanada</em>. Internationale Perspektiven auf heterogenitätssensible Bildung. Weinheim: Beltz Juventa 92-106.</p>
<p>Herold, Nastasia (2020): “La situation linguistique bilingue dans la communauté atikamekw de Manawan – La consolidation d’une langue minoritaire”, in: <em>Recherches amérindiennes au Québec</em>, N.N. [submitted].</p>
<p>INAC (26/09/2019): <em>Tribal Council Detail</em>. Atikamekw Sipi – Conseil de la Nation Atikamekw <http://fnp-ppn.aandc-aadnc.gc.ca/fnp/Main/Search/TCMain.aspx?TC_NUMBER=1064&lang=eng> (01/10/2019).</p>
<p>Kohle, Hubertus (2017): “Digitales Publizieren”, in: Jannidis, Fotis / Kohle, Hubertus / Rehbein, Malte (eds.): <em>Digital Humanities</em>. Eine Einführung. Stuttgart: J.B. Metzler 199-205.</p>
<p>Rehbein, Malte / Thies, Christian (2017): “Ethik”, in: Jannidis, Fotis / Kohle, Hubertus / Rehbein, Malte (eds.): <em>Digital Humanities</em>. Eine Einführung. Stuttgart: J.B. Metzler 353-357.</p>
<p>Reichert, Ramón (2017): “Theorien digitaler Medien”, in: Jannidis, Fotis / Kohle, Hubertus / Rehbein, Malte (eds.): <em>Digital Humanities</em>. Eine Einführung. Stuttgart: J.B. Metzler 19-34.</p>
","Abstract by Nastasia Herold & Thérèse Ottawa
 
Ethics and responsibilities of open access – lessons learned from the Wikipedia project of the Atikamekw First Nation
 
With 97,9% (cf. INAC 2019), in Canada, the Atikamekw First Nation has the highest percentage of people who speak their native language (Atikamekw) at home. The Atikamekw live in Quebec, Canada, and have a population of 8,000 people in three communities.
In 2013, Nastasia Herold, one of the authors of this paper did a field study in Manawan, one of the three Atikamekw communities, for a research on the local bilingualism (Atikamekw and French). Despite the vitality of the Atikamekw language, a survey and interviews showed that francization and language change are processes noticed by all living generations of the Atikamekw (cf. Herold 2020: N.N.).
Communication takes place more and more often digitally (cf. Reichert 2017: 26-27), and this in written language rather than orally. Atikamekw has a standardized orthography since 1994 (cf. Dinnison 21997) and is taught in Manawan’s primary school as a first language and as medium of alphabetization. However, Herold’s (2019: 103) research in 2013 showed that the Internet contained no written text in the Atikamekw language, the Atikamekw used the Internet mainly in French. This is why a school project at Manawan’s secondary school was initiated in 2013 in order to create a Wikipedia site in the Atikamekw language.
Many lessons have been learned during the collaboration of academics, teachers, pupils, local language experts and other local voluntary contributors. The lessons we would like to focus on in this presentation are the lessons learned when implementing cultural knowledge to an open access platform. Open access is the free provision of (scientific) texts on the web without restriction of use (cf. Kohle 2017: 203), and this free provision has advantages and disadvantages.
We will give four examples which show that the Atikamekw made sure that the knowledge was published respecting their own principles, beliefs and tradition. These four examples and the development of the project show how it is important that the community itself dictates rules that have to be respected when publishing their knowledge under open access.
Finally we will answer Rehbein’s and Thies’ (2017: 355) question they developed as a schema for questions of responsibilities and ethics of a specific project: Who (1) is responsible for what (2) to whom (3) before which instance (4) according to which standards (5)?
References
Dinnison, Bonnie (21997): Guide orthographique de la langue atikamekw. La Tuque: Atikamekw Sipi.
Herold, Nastasia (2019): “Kulturbedingte Herausforderungen für schulische und gesellschaftliche Teilhabe in indigenen Reservaten Kanadas: Das Beispiel des Atikamekw-Dorfes Manawan in Québec”, in: Jahr, David / Kruschel, Robert (eds.): Inklusion in Kanada. Internationale Perspektiven auf heterogenitätssensible Bildung. Weinheim: Beltz Juventa 92-106.
Herold, Nastasia (2020): “La situation linguistique bilingue dans la communauté atikamekw de Manawan – La consolidation d’une langue minoritaire”, in: Recherches amérindiennes au Québec, N.N. [submitted].
INAC (26/09/2019): Tribal Council Detail. Atikamekw Sipi – Conseil de la Nation Atikamekw <http://fnp-ppn.aandc-aadnc.gc.ca/fnp/Main/Search/TCMain.aspx?TC_NUMBER=1064&lang=eng> (01/10/2019).
Kohle, Hubertus (2017): “Digitales Publizieren”, in: Jannidis, Fotis / Kohle, Hubertus / Rehbein, Malte (eds.): Digital Humanities. Eine Einführung. Stuttgart: J.B. Metzler 199-205.
Rehbein, Malte / Thies, Christian (2017): “Ethik”, in: Jannidis, Fotis / Kohle, Hubertus / Rehbein, Malte (eds.): Digital Humanities. Eine Einführung. Stuttgart: J.B. Metzler 353-357.
Reichert, Ramón (2017): “Theorien digitaler Medien”, in: Jannidis, Fotis / Kohle, Hubertus / Rehbein, Malte (eds.): Digital Humanities. Eine Einführung. Stuttgart: J.B. Metzler 19-34.","nastasia.herold@uni-leipzig.de, thereseottawa@gmail.com",Short Presentation
"Shibutani, Ayako (1);
Goto, Makoto (2)","1: The University Museum, the University of Tokyo, Japan;
2: National Museum of Japanese History, Japan",How Do Research Data Develop? International Standardisation of Scientific Data in Historical Studies,"Cultural Heritage Science, Japanese history, linked data, open data, standardization","Asia
English
15th-17th Century
18th Century
19th Century
digital research infrastructures development and analysis
open access methods
History
History of science",English,Asia,"15th-17th Century
18th Century
19th Century","digital research infrastructures development and analysis
open access methods","History
History of science","<p>The scientific approach to historical resources creates a synthetic discipline benefitting from open access to data from the humanities and sciences. However, technological challenges exist because of dispersed and heterogeneous resource data. Through data sharing of historical resources, this paper proposes that the establishment of integrated data repositories will capture data provenance and diversity while promoting attribution and acknowledgment of its use. TTo enhance and accelerate scientific advances in historical studies, we enumerate digital humanities applications to solve the technological and sociological challenges that have limited open access to resource data in the world. We also standardise the scientific methodology of historical materials research using the following approaches: a qualitative analysis focusing on component details to compare our findings with the classifications granted to historical materials in previous studies, and the reconstruction of papermaking methods using DNA analysis.</p>
","The scientific approach to historical resources creates a synthetic discipline benefitting from open access to data from the humanities and sciences. However, technological challenges exist because of dispersed and heterogeneous resource data. Through data sharing of historical resources, this paper proposes that the establishment of integrated data repositories will capture data provenance and diversity while promoting attribution and acknowledgment of its use. TTo enhance and accelerate scientific advances in historical studies, we enumerate digital humanities applications to solve the technological and sociological challenges that have limited open access to resource data in the world. We also standardise the scientific methodology of historical materials research using the following approaches: a qualitative analysis focusing on component details to compare our findings with the classifications granted to historical materials in previous studies, and the reconstruction of papermaking methods using DNA analysis.","ashibutani@hi.u-tokyo.ac.jp, m-goto@rekihaku.ac.jp",Poster
"Licastro, Amanda Marie (1);
Skallerup Bessette, Lee Elaine (2);
Whalen, Zachary N (3);
McGrail, Anne B. (4)","1: Stevenson University, United States of America;
2: Georgetown University;
3: University of Mary Washington;
4: Lane Community College, Eugene, Oregon US",Exploring the Undiscovered Contours of DH,"pedagogy, marginalization, digital studies, infrastructure, curriculum","English
North America
Contemporary
curricular and pedagogical development and analysis
digital activism and advocacy
Education/ pedagogy",English,North America,Contemporary,"curricular and pedagogical development and analysis
digital activism and advocacy",Education/ pedagogy,"<p>How can scholars on the margins of DH articulate their work in DH publications, grants, and other professional and disciplinary outlets? In this interactive forum, we aim to explore inclusion--or exclusion--in what counts as “digital humanities” among scholars across disciplines, institutional contexts, and employment statuses. We begin by surveying audience members about the alternative ways that they represent their digital work and their different institutional contexts. We then ask participants to explore how esoteric terms such as “digital humanities” may be illegible to administrators and the public and the effects of this illegibility on their pedagogy and professional work. After collectively articulating the problem as it stands today, forum leaders will facilitate a problem-solving conversation that might begin addressing the issue.</p>
","How can scholars on the margins of DH articulate their work in DH publications, grants, and other professional and disciplinary outlets? In this interactive forum, we aim to explore inclusion--or exclusion--in what counts as “digital humanities” among scholars across disciplines, institutional contexts, and employment statuses. We begin by surveying audience members about the alternative ways that they represent their digital work and their different institutional contexts. We then ask participants to explore how esoteric terms such as “digital humanities” may be illegible to administrators and the public and the effects of this illegibility on their pedagogy and professional work. After collectively articulating the problem as it stands today, forum leaders will facilitate a problem-solving conversation that might begin addressing the issue.","amanda.licastro@gmail.com, ls1335@georgetown.edu, zwhalen@umw.edu, mcgraila@lanecc.edu",Forum
"Ma, Rongqian","University of Pittsburgh, United States of America",CR/10 Website as a Digital Public Humanities (DPH) Site,"Public digital humanities, user experience design, CR/10, cultural revolution","Asia
English
North America
20th Century
Contemporary
public humanities collaborations and methods
user experience design and analysis
Design studies
Library & information science",English,"Asia
North America","20th Century
Contemporary","public humanities collaborations and methods
user experience design and analysis","Design studies
Library & information science","<p>Initiated by Mao Zedong (1893-1976) and lasting from 1966 to 1976, the Cultural Revolution in China has been defined as a 10-year disaster by the Chinese Communist Party (CCP), which caused civil unrest to the party, the state, and the people (CCP, 1981). Forty years after the end of the Cultural Revolution, with the intention to promote the public remembrance and discussion of this significant period of Chinese history, the East Asian Library at the University of Pittsburgh launched CR/10 (University of Pittsburgh, 2019), a digital oral history project that aims to collect and preserve authentic memories of the Cultural Revolution through 10-minute semi-structured interviews. Using the technique of snowball sampling for interviewee selection, the project has thus far collected more than 300 interviews with ordinary people from different generations, geographies, social and educational backgrounds who experienced the incident or learned about it from family, school, or other resources. Among all the interviewees, some currently live in the U.S. while some in China, whose everyday experiences post the Cultural Revolution may have exerted impacts on their individual memories. Most interviews were conducted in mandarin Chinese and were then translated into English as subtitles by the CR/10 team. All the interviews were video-recorded and made open-access on the interactive CR/10 website. The website consists of four major components: an introduction to the project, a trailer for the project that showcases interviews, a timeline and a map of China demonstrating the temporal and geographical distributions of the videos in the collection.</p>
<p>In this paper, I treat the CR/10 website as the object of study and demonstrate its value as a digital public humanities site bridging the gap between the academy and the general public. Cox and Tilton (2019) defined the term “digital public humanities (DPH)” as practices that “facilitate reflection and collaboration with participants outside of the academy through digital theories and technologies” (p. 130). Focusing on interactive and mindful design (Drucker, 2013), the CR/10 website invites the public to participate in, as well as contributing to, developing a diversified and multifaceted understanding of the Cultural Revolution. In this study, I present a series of user experience research conducted regarding the design of the website with 15 users outside of the academy, to examine the current usability as well as to identify further design possibilities of the website. Through techniques of semi-structured contextual interviews and focus group study, this research study aims to reflect on the reception and use of the CR/10 website as an interactive teaching and learning platform, rather than solely a repository of collective memories or database of historical information. More specifically, I examine how the timeline and map features of the website enrich interpretations of the Cultural Revolution, especially in terms of inspiring users to reflect upon the following questions: How could the Cultural Revolution be defined? What factors (e.g., geography, generation, family, class backgrounds, and education) influence impressions and memories of the Cultural Revolution? Extending from the user experience research, this study also proposes recommendations to improve the design of the CR/10 website to create a collective “memory atlas” (Cornell University Library, 2013; Forster, 1976) out of the video collection. Findings of this study contribute to facilitating academy-public collaborations in building DPH sites from the user perspective and a design-mediated approach.</p>
<p>References</p>
<p>Chinese Communist Party (CCP). (1981). Resolution on certain questions in the history of our Party since the founding of the People's Republic of China. Retrieved from http://en.people.cn/dengxp/vol2/text/b1420.html</p>
<p>Cornell University Library. (2013). <em>Mnemosyne: Meanderings through Aby Warburg’s Atlas</em>. Retrieved from https://warburg.library.cornell.edu/</p>
<p>Cox, J., & Tilton, L. (2019). The digital public humanities: Giving new arguments and new ways to argue. <em>Review of Communication 19</em> (2), 127-146.</p>
<p>Drucker, J. (2013). Reading interface. <em>PMLA 128 </em>(1), 213-220.</p>
<p>Forster, K. (1976). Aby Warburg’s History of Art: Collective Memory and the Social Mediation of Images. <em>Daedalus 105 </em>(1), 169-176.</p>
<p>University of Pittsburgh. (2019). <em>CR/10</em>. Retrieved from http://www.culturalrevolution.pitt.edu/.</p>
","Initiated by Mao Zedong (1893-1976) and lasting from 1966 to 1976, the Cultural Revolution in China has been defined as a 10-year disaster by the Chinese Communist Party (CCP), which caused civil unrest to the party, the state, and the people (CCP, 1981). Forty years after the end of the Cultural Revolution, with the intention to promote the public remembrance and discussion of this significant period of Chinese history, the East Asian Library at the University of Pittsburgh launched CR/10 (University of Pittsburgh, 2019), a digital oral history project that aims to collect and preserve authentic memories of the Cultural Revolution through 10-minute semi-structured interviews. Using the technique of snowball sampling for interviewee selection, the project has thus far collected more than 300 interviews with ordinary people from different generations, geographies, social and educational backgrounds who experienced the incident or learned about it from family, school, or other resources. Among all the interviewees, some currently live in the U.S. while some in China, whose everyday experiences post the Cultural Revolution may have exerted impacts on their individual memories. Most interviews were conducted in mandarin Chinese and were then translated into English as subtitles by the CR/10 team. All the interviews were video-recorded and made open-access on the interactive CR/10 website. The website consists of four major components: an introduction to the project, a trailer for the project that showcases interviews, a timeline and a map of China demonstrating the temporal and geographical distributions of the videos in the collection.
In this paper, I treat the CR/10 website as the object of study and demonstrate its value as a digital public humanities site bridging the gap between the academy and the general public. Cox and Tilton (2019) defined the term “digital public humanities (DPH)” as practices that “facilitate reflection and collaboration with participants outside of the academy through digital theories and technologies” (p. 130). Focusing on interactive and mindful design (Drucker, 2013), the CR/10 website invites the public to participate in, as well as contributing to, developing a diversified and multifaceted understanding of the Cultural Revolution. In this study, I present a series of user experience research conducted regarding the design of the website with 15 users outside of the academy, to examine the current usability as well as to identify further design possibilities of the website. Through techniques of semi-structured contextual interviews and focus group study, this research study aims to reflect on the reception and use of the CR/10 website as an interactive teaching and learning platform, rather than solely a repository of collective memories or database of historical information. More specifically, I examine how the timeline and map features of the website enrich interpretations of the Cultural Revolution, especially in terms of inspiring users to reflect upon the following questions: How could the Cultural Revolution be defined? What factors (e.g., geography, generation, family, class backgrounds, and education) influence impressions and memories of the Cultural Revolution? Extending from the user experience research, this study also proposes recommendations to improve the design of the CR/10 website to create a collective “memory atlas” (Cornell University Library, 2013; Forster, 1976) out of the video collection. Findings of this study contribute to facilitating academy-public collaborations in building DPH sites from the user perspective and a design-mediated approach.
References
Chinese Communist Party (CCP). (1981). Resolution on certain questions in the history of our Party since the founding of the People's Republic of China. Retrieved from http://en.people.cn/dengxp/vol2/text/b1420.html
Cornell University Library. (2013). Mnemosyne: Meanderings through Aby Warburg’s Atlas. Retrieved from https://warburg.library.cornell.edu/
Cox, J., & Tilton, L. (2019). The digital public humanities: Giving new arguments and new ways to argue. Review of Communication 19 (2), 127-146.
Drucker, J. (2013). Reading interface. PMLA 128 (1), 213-220.
Forster, K. (1976). Aby Warburg’s History of Art: Collective Memory and the Social Mediation of Images. Daedalus 105 (1), 169-176.
University of Pittsburgh. (2019). CR/10. Retrieved from http://www.culturalrevolution.pitt.edu/.",rom77@pitt.edu,Short Presentation
"Van Zundert, Joris J. (1);
Mar, Raymond A. (2);
van Dalen–Oskam, Karina (1,3);
Temple, Emily (4);
Bowman, Isabel (5);
Heidari, Farzaneh (6);
Nguyen, Ahn T.P. (2)","1: Department of Literary Studies, Huygens Institute for the History of the Netherlands – Royal Netherlands Academy of Arts and Sciences. Amsterdam, The Netherlands;
2: Department of Psychology, York University. Toronto, Canada;
3: Faculty of Humanities, University of Amsterdam. Amsterdam, The Netherlands;
4: Literary Hub;
5: Department of Psychology, University of Toronto. Toronto, Canada;
6: Department of Electrical Engineering and Computer Sciences, York University. Toronto, Canada",Features of Timelessness: Intermediate Report on a Quest for Stylistic Features that Mark Literary Canonicity,"stylometry, literature, features, canonicity","Europe
English
North America
20th Century
Contemporary
attribution studies and stylometric analysis
Humanities computing
Literary studies",English,"Europe
North America","20th Century
Contemporary",attribution studies and stylometric analysis,"Humanities computing
Literary studies","<p>We report on our ongoing quest to establish a validated complex of stylistic features that act as markers for literary canonicity, in specific contexts. Currentely we present a stylometric analysis of literature investigating the stylistic markers that differentiate former bestsellers from fiction that remains popular across several decades using a TfIdf vectorization of texts and UMAP dimenision reduction approach. We find that especially a greater variation in sentence length is associated with the chances of a novel to remain popular.</p>
","We report on our ongoing quest to establish a validated complex of stylistic features that act as markers for literary canonicity, in specific contexts. Currentely we present a stylometric analysis of literature investigating the stylistic markers that differentiate former bestsellers from fiction that remains popular across several decades using a TfIdf vectorization of texts and UMAP dimenision reduction approach. We find that especially a greater variation in sentence length is associated with the chances of a novel to remain popular.","joris.van.zundert@huygens.knaw.nl, mar@yorku.ca, karina.van.dalen@huygens.knaw.nl, etemple@lithub.com, isabel.bowman@mail.utoronto.ca, farzanah@cse.yorku.ca, ntpanh1602@gmail.com",Short Presentation
"Schildkamp, Philip (1);
Harzenetter, Lukas (2);
Leymann, Frank (2);
Mathiak, Brigitte (1);
Neuefeind, Claes (3);
Breitenbücher, Uwe (4)","1: Data Center for the Humanities, University of Cologne, Germany;
2: Institute of Architecture of Application Systems, University of Stuttgart, Germany;
3: Cologne Center for eHumanities, University of Cologne, Germany;
4: University of Stuttgart, Germany",Workshop on Modelling and Maintaining Research Applications in TOSCA,"Living Systems, Research Applications, Software Stacks, Sustainability, TOSCA","Global
English
Contemporary
software development, systems, analysis and methods
sustainable procedures, systems, and methods
Humanities computing
Informatics",English,Global,Contemporary,"software development, systems, analysis and methods
sustainable procedures, systems, and methods","Humanities computing
Informatics","<p dir=""ltr""><strong>Abstract</strong></p>
<p dir=""ltr"">The project ""SustainLife – Sustaining Living Digital Systems in the Humanities"" that is currently running at the Institute of Architecture of Application Systems (IAAS, University of Stuttgart) and the Data Center for the Humanities (DCH, University of Cologne) deals with the conservation of research applications in the field of Digital Humanities (DH). By employing the TOSCA standard (Topology and Orchestration Specification for Cloud Applications) to fully automate the deployment of DH applications and to keep them available in the long term, we try to tackle the problem of software obsolescence in the field of DH. To interactively demonstrate our approach to the international DH community, we would like to give a workshop on the topic ""Modelling and Maintaining Research Applications in TOSCA"" in the run-up to the DH 2020 conference. Thereinwe will show how to model (DH) software systems with TOSCA and share experiences and best practices on how to work with the OpenTOSCA ecosystem, an open-source implementation of the TOSCA standard.</p>
<p dir=""ltr""><strong></strong></p>
<p dir=""ltr""><strong>The Problem</strong></p>
<p dir=""ltr"">The establishment of the DH as an independent scientific research area as well as the increasing usage of digital methods in the research process require adjustments to common result assurance practices. For example, the long-term archiving (LTA) of primary research data uses well-established practices such as employing standardized data formats and forwarding data to permanent repositories. However, the fact that digital artifacts generated in DH-oriented research do not only consist of primary data but also contain research software is mostly disregarded (Sahle and Kronenwett, 2013). Moreover, the variety of DH research outcomes includes so-called ""living systems"" in which the software to present, access or analyze the data represents an essential part of the actual research output (Bingert et al., 2016). In contrast to classical research results such as monographs or encyclopedias, living systems cannot be served long-term without maintenance as their instantiation, supervision, and permanent provisioning represent major technical, organizational, and financial challenges. Furthermore, the heterogeneity of the research software generated in the DH requires a highly flexible preservation strategy, i.e., a suitable technology that ensures standardization, reusability, and archiving of as many digital artifacts as possible (Barzen et al., 2018). In addition to the aforementioned challenges, i.e., heterogeneity, underfunding, and obsolescence of digital artifacts, scientific practice requires long-term interoperability and traceability of all research outcomes. With regard to digital systems, these requirements are (1) constant accessibility, (2) the possibility of error-free operation, and (3) the ability to reconstruct any stage of development of a research application at any time without major structural difficulties.</p>
<p dir=""ltr""><strong></strong></p>
<p dir=""ltr""><strong>Our Approach</strong></p>
<p dir=""ltr"">The TOSCA standard (OASIS, 2013 and 2019) allows software systems to be modelled, provisioned, and deployed in a standardized and provider-independent manner. Thus, it is suitable for long-term archiving and operation of research applications produced within the field of DH (Neuefeind et al., 2018 and 2019). Following the TOSCA standard, applications are modelled in “Topology Templates” by describing their components and their relations amongst each other: Components are represented as “Node Templates”, while relations are modeled as “Relationship Templates”. Moreover, the semantics of a Node Template or Relationship Template are dictated by reusable types, i.e., “Node Types” and “Relationship Types” respectively. For example, a Python web application can be modelled as a Node Template that is an instance of the ""Python Application"" Node Type. To express that the Python Application accesses a MySQL database, a second Node Template that is of type ""MySQL Database"" can be added to the Topology Template. Then, the connection between both components can be described by a Relationship Template that is an instance of the Relationship Type ""connectsTo"". Additionally, to specify that both components are running on an Ubuntu virtual machine (VM), a Node Template of type ""Ubuntu VM"" can be added, while Relationship Templates of type “hostedOn” between the Python Application Node Template and the VM Node Template, as well as between the MySQL Database and the VM describe their respective hosting relations.</p>
<p dir=""ltr"">Hereby, TOSCA's type system enables the modelling of reusable component types, e.g., the ""Python Application"" Node Type, which can be reused in multiple Topology Templates describing different applications. Therefore, synergic effects emerge as existing Node Types can be reused in other Topology Templates, easing the modelling of new applications. In addition, the open-source TOSCA implementation OpenTOSCA (Breitenbücher et al., 2016) offers the possibility to graphically model applications using the TOSCA editor “Winery” (Kopp et al., 2013) which further simplifies the creation of new applications by providing drag-and-drop modeling capabilities.</p>
<p dir=""ltr""><strong></strong></p>
<p dir=""ltr""><strong>Workshop Curriculum</strong></p>
<p dir=""ltr"">During our four hour workshop, we will (1) give an overview to different solutions for long-term preservation of living systems and (2) describe the modeling language TOSCA. Based on these theoretical units, practical tasks will introduce (3) the modelling of an existing application using TOSCA and (4) how applications can be deployed using the OpenTOSCA ecosystem. Thus, by combining the theoretical foundations and the practical application of TOSCA, the participants will be able to model (research) software systems according to the standard and provision and deploy applications using the OpenTOSCA ecosystem.</p>
<p dir=""ltr"">The practical tasks are structured as follows: (1) Identify the components of an application and (2) describe them and their relations among each other in an TOSCA-based application topology, i.e., in a Topology Template. By fragmenting an application into its components and mapping them to TOSCA Node Types, the Topology Template describing the application can then be modelled using the OpenTOSCA ecosystem. Afterwards (3), the modelled TOSCA application will be deployed by the OpenTOSCA runtime. Moreover, by sharing our experiences and best practices in using OpenTOSCA with the community, we will introduce concepts such as ""software stacks"" in a practical way.</p>
<p dir=""ltr""><strong></strong></p>
<p dir=""ltr""><strong>Target Group</strong></p>
<p dir=""ltr"">The workshop is primarily designed for data center employees, libraries and other institutions focusing on infrastructures for long-term archiving and operation of heterogeneous software systems. Previous experience in dealing with Linux and writing shell scripts as well as with software stacks and service orchestration are helpful but not necessary for a successful participation. To provide a productive context for communicating the described content and to enable individual consultation and support, we designed the workshop for about 20 participants but limit it to a maximum of 30 participants.</p>
<p dir=""ltr""><strong></strong></p>
<p dir=""ltr""><strong>Technical Prerequisites</strong></p>
<p dir=""ltr"">For a successful participation in the workshop, it is necessary that each participant brings his/her own laptop. Although a shared instance of the OpenTOSCA ecosystem will be provided, it is desirable that all participants set up an OpenTOSCA instance on their work equipment prior to the workshop in order to perform modelling and deployment tasks on their own devices. Therefore, registered participants will be provided with all necessary information about system requirements and how to setup OpenTOSCA prior to the workshop. Furthermore, relevant documentation, publications, and manuals will be provided both in advance and in the context of the workshop. In addition, a stable internet connection as well as a sufficient number of power outlets for all electronic devices are indispensable. </p>
<p dir=""ltr""><br /><strong>About the Instructors</strong></p>
<p dir=""ltr"">Uwe Breitenbücher is a research staff member and postdoc at the Institute of Architecture of Application Systems (IAAS) at the University of Stuttgart, Germany. His research vision is to improve cloud application provisioning and application management by automating the application of management patterns. Uwe was part of the CloudCycle project, in which the OpenTOSCA Ecosystem was developed. His current research interests include cyber-physical systems, blockchains, and microservices.</p>
<p dir=""ltr"">Anna Fischer is a research assistant at the Data Center for the Humanities (DCH) at the University of Cologne and joined the “SustainLife” Project in January 2020. Her recent research and working activities have focused on data management and software development for natural language processing tasks, e.g., in collaboration with one of the chairs for Romance linguistics at the University of Cologne. </p>
<p dir=""ltr"">Lukas Harzenetter is a research associate at the Institute of Architecture of Application Systems (IAAS) at the University of Stuttgart, Germany. He received his Master of Science degree from the University of Stuttgart in Software Engineering in 2018. His research interests are in the field of cloud deployment and management models focusing on the development and change of such models over time. Lukas is part of the “SustainLife” project which is working on sustainable application deployments in the domain of digital humanities.</p>
<p dir=""ltr"">Frank Leymann is a full professor of computer science and director of the Institute of Architecture of Application Systems (IAAS) at the University of Stuttgart, Germany. His research interests include service-oriented architectures and associated middleware, workflow- and business process management, cloud computing and associated systems management aspects, and patterns. Frank is co-author of more than 400 peer-reviewed papers, about 70 patents, and several industry standards. He is an elected member of the Academy of Europe.</p>
<p dir=""ltr"">Brigitte Mathiak is chairwoman of the Data Center for the Humanities (DCH) and is particularly interested in data management and text mining. The idea for the ""SustainLife"" project arose after she had experienced again and again how living systems have to be abandoned or neglected. She is Junior Professor for Digital Humanities at the University of Cologne and Senior Scientist at the Leibniz Institute for the Social Sciences (GESIS).</p>
<p dir=""ltr"">Claes Neuefeind is a postdoc at the Cologne Center for eHumanities (CCeH) at the University of Cologne. He worked with Philip Schildkamp and Lukas Harzenetter on the DFG-LIS project ""SustainLife"" until October 2019 and changed for a position that is responsible for coordinating the Digital Humanities of the North Rhine-Westphalian Academy of Sciences and the Arts office.</p>
<p dir=""ltr"">Philip Schildkamp has been researching since 2015 and teaching since 2017 at the University of Cologne. He studied sociology, psychology, and Digital Humanities information processing. The main topics of his employment are technical infrastructure measures in the field of (Digital) Humanities and the orchestration of distributed software systems. Since March 2018, Philip has been part of the DFG-LIS project ""SustainLife"" at the Data Center for the Humanities (DCH).</p>
<p dir=""ltr""><strong></strong></p>
<p dir=""ltr""><strong>Acknowledgements</strong></p>
<p dir=""ltr"">This poster is partially funded by the DFG-LIS project “SustainLife” (GEPRIS 379522012).</p>
<p dir=""ltr""></p>
<p dir=""ltr""><strong>References</strong></p>
<ul><li dir=""ltr"">
<p dir=""ltr"">Barzen, J. and Blumtritt, J. and Breitenbücher, U. and Kronenwett, S. and Leymann, F. and Mathiak, B. and Neuefeind, C. (2018). SustainLife – Erhalt lebender, digitaler Systeme für die Geisteswissenschaften. In: Book of Abstracts of the 5th annual Conference of the Digital Humanities im deutschsprachigen Raum (DHd 2018), pp. 471-474.</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">Bingert, S. and Blumtritt, J. and Buddenbohm, S. and Engelhardt, C. and Kronenwett, S. and Kurzawe, D. (2016). Anwendungskonservierung und die Nachhaltigkeit von Forschungsanwendungen. In: Forschungsdaten​ ​in​ ​den​ ​Geisteswissenschaften​ ​(FORGE​ ​2016),​ pp. 14-16.</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">Breitenbücher, U. and Endres, C. and Képes, K. and Kopp, O. and Leymann, F. and Wagner, S. and Wettinger, J. and Zimmermann, M. (2016). The OpenTOSCA Ecosystem. Concept & Tools. In: European Space Project on Smart Systems, Big Data, Future Internet. Towards Serving the Grand Societal Challenges. Volume #1, pp. 112-130.</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">Kopp, O. and Binz, T. and Breitenbücher, U. and Leymann, F. (2013). Winery – A Modeling Tool for TOSCA-based Cloud Applications. In: Proceedings of the 11th International Conference on Service-Oriented Computing (ICSOC 2013), pp. 700-704.</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">Neuefeind, C. and Harzenetter, L. and Schildkamp, P. and Breitenbücher, U. and Mathiak, B. and Barzen, J. and Leymann, F. (2018). The SustainLife Project – Living Systems in Digital Humanities. In: Proceedings of the 12th Advanced Summer School on Service-Oriented Computing (SummerSoC 2018) (IBM Research Report RC25681), pp. 101-112.</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">Neuefeind, C. and Schildkamp, P. and Mathiak, B. and Marčić, A. and Hentschel, F. and Harzenetter, L. and Breitenbücher, U. and Barzen, J. and Leymann, F. (2019). Sustaining the Musical Competitions Database. A TOSCA-based Approach to Application Preservation in the Digital Humanities. In: Book of Abstracts of the 29th Digital Humanities Conference (DH 2019), https://dev.clariah.nl/files/dh2019/boa/0574.html (retrieved: 2019-09-10).</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">OASIS (2013). Topology and Orchestration Specification for Cloud Applications Version 1.0, http://docs.oasis-open.org/tosca/TOSCA/v1.0/TOSCA-v1.0.html (retrieved: 2019-09-10).</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">OASIS (2019). TOSCA Simple Profile in YAML Version 1.2, http://docs.oasis-open.org/tosca/TOSCA-Simple-Profile-YAML/v1.2/TOSCA-Simple-Profile-YAML-v1.2.html (retrieved: 2019-09-10).</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">Sahle, P. and Kronenwett, S. (2013). Jenseits der Daten. Überlegungen zu Datenzentren für die Geisteswissenschaften am Beispiel des Kölner Data Center for the Humanities. In: LIBREAS. Library Ideas #23, pp. 76-96.</p>
</li>
</ul>","Abstract
The project ""SustainLife – Sustaining Living Digital Systems in the Humanities"" that is currently running at the Institute of Architecture of Application Systems (IAAS, University of Stuttgart) and the Data Center for the Humanities (DCH, University of Cologne) deals with the conservation of research applications in the field of Digital Humanities (DH). By employing the TOSCA standard (Topology and Orchestration Specification for Cloud Applications) to fully automate the deployment of DH applications and to keep them available in the long term, we try to tackle the problem of software obsolescence in the field of DH. To interactively demonstrate our approach to the international DH community, we would like to give a workshop on the topic ""Modelling and Maintaining Research Applications in TOSCA"" in the run-up to the DH 2020 conference. Thereinwe will show how to model (DH) software systems with TOSCA and share experiences and best practices on how to work with the OpenTOSCA ecosystem, an open-source implementation of the TOSCA standard.
The Problem
The establishment of the DH as an independent scientific research area as well as the increasing usage of digital methods in the research process require adjustments to common result assurance practices. For example, the long-term archiving (LTA) of primary research data uses well-established practices such as employing standardized data formats and forwarding data to permanent repositories. However, the fact that digital artifacts generated in DH-oriented research do not only consist of primary data but also contain research software is mostly disregarded (Sahle and Kronenwett, 2013). Moreover, the variety of DH research outcomes includes so-called ""living systems"" in which the software to present, access or analyze the data represents an essential part of the actual research output (Bingert et al., 2016). In contrast to classical research results such as monographs or encyclopedias, living systems cannot be served long-term without maintenance as their instantiation, supervision, and permanent provisioning represent major technical, organizational, and financial challenges. Furthermore, the heterogeneity of the research software generated in the DH requires a highly flexible preservation strategy, i.e., a suitable technology that ensures standardization, reusability, and archiving of as many digital artifacts as possible (Barzen et al., 2018). In addition to the aforementioned challenges, i.e., heterogeneity, underfunding, and obsolescence of digital artifacts, scientific practice requires long-term interoperability and traceability of all research outcomes. With regard to digital systems, these requirements are (1) constant accessibility, (2) the possibility of error-free operation, and (3) the ability to reconstruct any stage of development of a research application at any time without major structural difficulties.
Our Approach
The TOSCA standard (OASIS, 2013 and 2019) allows software systems to be modelled, provisioned, and deployed in a standardized and provider-independent manner. Thus, it is suitable for long-term archiving and operation of research applications produced within the field of DH (Neuefeind et al., 2018 and 2019). Following the TOSCA standard, applications are modelled in “Topology Templates” by describing their components and their relations amongst each other: Components are represented as “Node Templates”, while relations are modeled as “Relationship Templates”. Moreover, the semantics of a Node Template or Relationship Template are dictated by reusable types, i.e., “Node Types” and “Relationship Types” respectively. For example, a Python web application can be modelled as a Node Template that is an instance of the ""Python Application"" Node Type. To express that the Python Application accesses a MySQL database, a second Node Template that is of type ""MySQL Database"" can be added to the Topology Template. Then, the connection between both components can be described by a Relationship Template that is an instance of the Relationship Type ""connectsTo"". Additionally, to specify that both components are running on an Ubuntu virtual machine (VM), a Node Template of type ""Ubuntu VM"" can be added, while Relationship Templates of type “hostedOn” between the Python Application Node Template and the VM Node Template, as well as between the MySQL Database and the VM describe their respective hosting relations.
Hereby, TOSCA's type system enables the modelling of reusable component types, e.g., the ""Python Application"" Node Type, which can be reused in multiple Topology Templates describing different applications. Therefore, synergic effects emerge as existing Node Types can be reused in other Topology Templates, easing the modelling of new applications. In addition, the open-source TOSCA implementation OpenTOSCA (Breitenbücher et al., 2016) offers the possibility to graphically model applications using the TOSCA editor “Winery” (Kopp et al., 2013) which further simplifies the creation of new applications by providing drag-and-drop modeling capabilities.
Workshop Curriculum
During our four hour workshop, we will (1) give an overview to different solutions for long-term preservation of living systems and (2) describe the modeling language TOSCA. Based on these theoretical units, practical tasks will introduce (3) the modelling of an existing application using TOSCA and (4) how applications can be deployed using the OpenTOSCA ecosystem. Thus, by combining the theoretical foundations and the practical application of TOSCA, the participants will be able to model (research) software systems according to the standard and provision and deploy applications using the OpenTOSCA ecosystem.
The practical tasks are structured as follows: (1) Identify the components of an application and (2) describe them and their relations among each other in an TOSCA-based application topology, i.e., in a Topology Template. By fragmenting an application into its components and mapping them to TOSCA Node Types, the Topology Template describing the application can then be modelled using the OpenTOSCA ecosystem. Afterwards (3), the modelled TOSCA application will be deployed by the OpenTOSCA runtime. Moreover, by sharing our experiences and best practices in using OpenTOSCA with the community, we will introduce concepts such as ""software stacks"" in a practical way.
Target Group
The workshop is primarily designed for data center employees, libraries and other institutions focusing on infrastructures for long-term archiving and operation of heterogeneous software systems. Previous experience in dealing with Linux and writing shell scripts as well as with software stacks and service orchestration are helpful but not necessary for a successful participation. To provide a productive context for communicating the described content and to enable individual consultation and support, we designed the workshop for about 20 participants but limit it to a maximum of 30 participants.
Technical Prerequisites
For a successful participation in the workshop, it is necessary that each participant brings his/her own laptop. Although a shared instance of the OpenTOSCA ecosystem will be provided, it is desirable that all participants set up an OpenTOSCA instance on their work equipment prior to the workshop in order to perform modelling and deployment tasks on their own devices. Therefore, registered participants will be provided with all necessary information about system requirements and how to setup OpenTOSCA prior to the workshop. Furthermore, relevant documentation, publications, and manuals will be provided both in advance and in the context of the workshop. In addition, a stable internet connection as well as a sufficient number of power outlets for all electronic devices are indispensable. 
About the Instructors
Uwe Breitenbücher is a research staff member and postdoc at the Institute of Architecture of Application Systems (IAAS) at the University of Stuttgart, Germany. His research vision is to improve cloud application provisioning and application management by automating the application of management patterns. Uwe was part of the CloudCycle project, in which the OpenTOSCA Ecosystem was developed. His current research interests include cyber-physical systems, blockchains, and microservices.
Anna Fischer is a research assistant at the Data Center for the Humanities (DCH) at the University of Cologne and joined the “SustainLife” Project in January 2020. Her recent research and working activities have focused on data management and software development for natural language processing tasks, e.g., in collaboration with one of the chairs for Romance linguistics at the University of Cologne. 
Lukas Harzenetter is a research associate at the Institute of Architecture of Application Systems (IAAS) at the University of Stuttgart, Germany. He received his Master of Science degree from the University of Stuttgart in Software Engineering in 2018. His research interests are in the field of cloud deployment and management models focusing on the development and change of such models over time. Lukas is part of the “SustainLife” project which is working on sustainable application deployments in the domain of digital humanities.
Frank Leymann is a full professor of computer science and director of the Institute of Architecture of Application Systems (IAAS) at the University of Stuttgart, Germany. His research interests include service-oriented architectures and associated middleware, workflow- and business process management, cloud computing and associated systems management aspects, and patterns. Frank is co-author of more than 400 peer-reviewed papers, about 70 patents, and several industry standards. He is an elected member of the Academy of Europe.
Brigitte Mathiak is chairwoman of the Data Center for the Humanities (DCH) and is particularly interested in data management and text mining. The idea for the ""SustainLife"" project arose after she had experienced again and again how living systems have to be abandoned or neglected. She is Junior Professor for Digital Humanities at the University of Cologne and Senior Scientist at the Leibniz Institute for the Social Sciences (GESIS).
Claes Neuefeind is a postdoc at the Cologne Center for eHumanities (CCeH) at the University of Cologne. He worked with Philip Schildkamp and Lukas Harzenetter on the DFG-LIS project ""SustainLife"" until October 2019 and changed for a position that is responsible for coordinating the Digital Humanities of the North Rhine-Westphalian Academy of Sciences and the Arts office.
Philip Schildkamp has been researching since 2015 and teaching since 2017 at the University of Cologne. He studied sociology, psychology, and Digital Humanities information processing. The main topics of his employment are technical infrastructure measures in the field of (Digital) Humanities and the orchestration of distributed software systems. Since March 2018, Philip has been part of the DFG-LIS project ""SustainLife"" at the Data Center for the Humanities (DCH).
Acknowledgements
This poster is partially funded by the DFG-LIS project “SustainLife” (GEPRIS 379522012).
References
Barzen, J. and Blumtritt, J. and Breitenbücher, U. and Kronenwett, S. and Leymann, F. and Mathiak, B. and Neuefeind, C. (2018). SustainLife – Erhalt lebender, digitaler Systeme für die Geisteswissenschaften. In: Book of Abstracts of the 5th annual Conference of the Digital Humanities im deutschsprachigen Raum (DHd 2018), pp. 471-474.
Bingert, S. and Blumtritt, J. and Buddenbohm, S. and Engelhardt, C. and Kronenwett, S. and Kurzawe, D. (2016). Anwendungskonservierung und die Nachhaltigkeit von Forschungsanwendungen. In: Forschungsdaten​ ​in​ ​den​ ​Geisteswissenschaften​ ​(FORGE​ ​2016),​ pp. 14-16.
Breitenbücher, U. and Endres, C. and Képes, K. and Kopp, O. and Leymann, F. and Wagner, S. and Wettinger, J. and Zimmermann, M. (2016). The OpenTOSCA Ecosystem. Concept & Tools. In: European Space Project on Smart Systems, Big Data, Future Internet. Towards Serving the Grand Societal Challenges. Volume #1, pp. 112-130.
Kopp, O. and Binz, T. and Breitenbücher, U. and Leymann, F. (2013). Winery – A Modeling Tool for TOSCA-based Cloud Applications. In: Proceedings of the 11th International Conference on Service-Oriented Computing (ICSOC 2013), pp. 700-704.
Neuefeind, C. and Harzenetter, L. and Schildkamp, P. and Breitenbücher, U. and Mathiak, B. and Barzen, J. and Leymann, F. (2018). The SustainLife Project – Living Systems in Digital Humanities. In: Proceedings of the 12th Advanced Summer School on Service-Oriented Computing (SummerSoC 2018) (IBM Research Report RC25681), pp. 101-112.
Neuefeind, C. and Schildkamp, P. and Mathiak, B. and Marčić, A. and Hentschel, F. and Harzenetter, L. and Breitenbücher, U. and Barzen, J. and Leymann, F. (2019). Sustaining the Musical Competitions Database. A TOSCA-based Approach to Application Preservation in the Digital Humanities. In: Book of Abstracts of the 29th Digital Humanities Conference (DH 2019), https://dev.clariah.nl/files/dh2019/boa/0574.html (retrieved: 2019-09-10).
OASIS (2013). Topology and Orchestration Specification for Cloud Applications Version 1.0, http://docs.oasis-open.org/tosca/TOSCA/v1.0/TOSCA-v1.0.html (retrieved: 2019-09-10).
OASIS (2019). TOSCA Simple Profile in YAML Version 1.2, http://docs.oasis-open.org/tosca/TOSCA-Simple-Profile-YAML/v1.2/TOSCA-Simple-Profile-YAML-v1.2.html (retrieved: 2019-09-10).
Sahle, P. and Kronenwett, S. (2013). Jenseits der Daten. Überlegungen zu Datenzentren für die Geisteswissenschaften am Beispiel des Kölner Data Center for the Humanities. In: LIBREAS. Library Ideas #23, pp. 76-96.","philip.schildkamp@uni-koeln.de, lukas.harzenetter@iaas.uni-stuttgart.de, frank.leymann@iaas.uni-stuttgart.de, bmathiak@uni-koeln.de, c.neuefeind@uni-koeln.de, uwe.breitenbuecher@iaas.uni-stuttgart.de",Workshop/Tutorial 4
"Chokshi, Crystal Nicole","University of Calgary, Canada",Gmail’s Smart Compose: A Critical Composit(ion),"algorithmically-mediated writing, Smart Compose, critical algorithm studies, critical media studies, linguistic capitalism","English
North America
Contemporary
information retrieval and querying algorithms and methods
text mining and analysis
Communication studies
Media studies",English,North America,Contemporary,"information retrieval and querying algorithms and methods
text mining and analysis","Communication studies
Media studies","<p>While Google once merely monitored users’ words, today the company literally writes them. This is thanks to Smart Compose, a word-prediction algorithm that Google has launched in Gmail and Google Docs. The algorithm depends on Google’s meticulous recording and machine-reading of the personal data of an untold number of its 1.5 billion Gmail users, leveraging users’ words and writing for the development of the technology. In this context, language is particularly vulnerable to corporate intervention and manipulation. As such, this presentation carefully considers Fréderic Kaplan’s (2014) call to action: “through… the advent of algorithms as a new media, something is likely happen [<em>sic</em>] to language, and, although we are not yet sure what it will be, new tools must be built in order to understand this global linguistic evolution” (p. 62).</p>
<p>Responding to Kaplan's call, I report on experiments with Smart Compose in which I am manually transcribing more than 50,000 words from published texts and, subsequently, annotating and visualizing input to and output from the algorithm. These experiments are part of my larger doctoral project that seeks to locate the shifting ""semantic coordinates"" (Striphas, 2015, p. 398) of ""language,"" ""words,"" and ""writing"" in an algorithmic culture. Applying the framework of data colonialism (Couldry & Mejias, 2019), I argue that word-prediction algorithms such as Smart Compose must necessarily shift our understanding of these terms when words become data (Thornton, 2019) and writing becomes a datafied practice.</p>
<p>More broadly, I suggest that in place of the question posed by Siva Vaidhyanathan (2011) some years ago—“what do we gain and what do we lose by inviting Google to be the lens through which we see the world?” (p. 9)—we must, urgently and necessarily, ask this: what do we gain and what do we lose by allowing Google to offer the words through which we write the world?</p>
<p align=""center""><strong>References</strong></p>
<p>Couldry, N. & Mejias, U.A. (2019). Data colonialism: Rethinking big data’s relation to the contemporary subject. Television & New Media, 20(4), 336-349. https://doi.org/10.1177/1527476418796632</p>
<p>Kaplan, F. (2014). Linguistic capitalism and algorithmic mediation. <em>Representations,</em> 127(1), 57-63. https://doi.org/10.1525/rep.2014.127.1.57</p>
<p>Striphas, T. (2015). Algorithmic culture. European Journal of Cultural Studies, 18(4-5), 395-412. https://doi.org/10.1177/1367549415577392</p>
<p>Thornton, P. (2019). Language in the age of algorithmic reproduction: A critique of linguistic capitalism [Unpublished doctoral dissertation]. Royal Holloway, University of London.  </p>
<p>Vaidhyanathan, S. (2011). The googlization of everything (and why we should worry). University of California Press.</p>
","While Google once merely monitored users’ words, today the company literally writes them. This is thanks to Smart Compose, a word-prediction algorithm that Google has launched in Gmail and Google Docs. The algorithm depends on Google’s meticulous recording and machine-reading of the personal data of an untold number of its 1.5 billion Gmail users, leveraging users’ words and writing for the development of the technology. In this context, language is particularly vulnerable to corporate intervention and manipulation. As such, this presentation carefully considers Fréderic Kaplan’s (2014) call to action: “through… the advent of algorithms as a new media, something is likely happen [sic] to language, and, although we are not yet sure what it will be, new tools must be built in order to understand this global linguistic evolution” (p. 62).
Responding to Kaplan's call, I report on experiments with Smart Compose in which I am manually transcribing more than 50,000 words from published texts and, subsequently, annotating and visualizing input to and output from the algorithm. These experiments are part of my larger doctoral project that seeks to locate the shifting ""semantic coordinates"" (Striphas, 2015, p. 398) of ""language,"" ""words,"" and ""writing"" in an algorithmic culture. Applying the framework of data colonialism (Couldry & Mejias, 2019), I argue that word-prediction algorithms such as Smart Compose must necessarily shift our understanding of these terms when words become data (Thornton, 2019) and writing becomes a datafied practice.
More broadly, I suggest that in place of the question posed by Siva Vaidhyanathan (2011) some years ago—“what do we gain and what do we lose by inviting Google to be the lens through which we see the world?” (p. 9)—we must, urgently and necessarily, ask this: what do we gain and what do we lose by allowing Google to offer the words through which we write the world?
References
Couldry, N. & Mejias, U.A. (2019). Data colonialism: Rethinking big data’s relation to the contemporary subject. Television & New Media, 20(4), 336-349. https://doi.org/10.1177/1527476418796632
Kaplan, F. (2014). Linguistic capitalism and algorithmic mediation. Representations, 127(1), 57-63. https://doi.org/10.1525/rep.2014.127.1.57
Striphas, T. (2015). Algorithmic culture. European Journal of Cultural Studies, 18(4-5), 395-412. https://doi.org/10.1177/1367549415577392
Thornton, P. (2019). Language in the age of algorithmic reproduction: A critique of linguistic capitalism [Unpublished doctoral dissertation]. Royal Holloway, University of London.  
Vaidhyanathan, S. (2011). The googlization of everything (and why we should worry). University of California Press.",crystal.chokshi1@ucalgary.ca,Short Presentation
"Horak, Laura","Carleton University, Canada",Transing DH: Adopting a Trans-Centric Approach to Building the Transgender Media Portal,"transgender, cinema, database, ethics, feminist","English
North America
20th Century
Contemporary
database creation, management, and analysis
public humanities collaborations and methods
Film and cinema arts studies
Transgender and non-binary studies",English,North America,"20th Century
Contemporary","database creation, management, and analysis
public humanities collaborations and methods","Film and cinema arts studies
Transgender and non-binary studies","<p>Guided by the fields of intersectional feminist digital humanities and transgender studies, this talk explores our team’s efforts to adopt a trans-centric approach to building a collaborative online tool to investigate and publicize films made by trans, Two Spirit, nonbinary, intersex, and gender-nonconforming people.</p>
","Guided by the fields of intersectional feminist digital humanities and transgender studies, this talk explores our team’s efforts to adopt a trans-centric approach to building a collaborative online tool to investigate and publicize films made by trans, Two Spirit, nonbinary, intersex, and gender-nonconforming people.",laura.horak@carleton.ca,Short Presentation
"Satlow, Michael (1);
Sperling, Michael (2)","1: Brown University, United States of America;
2: New York University, United States of America",The Rabbinic Social Network,"historical networks, gephi, pattern recognition","Asia
English
BCE-4th Century
5th-14th Century
network analysis and graphs theory and application
History
Theology and religious studies",English,Asia,"BCE-4th Century
5th-14th Century",network analysis and graphs theory and application,"History
Theology and religious studies","<p align=""center""></p>
<p>This project attempts to apply the techniques of social network analysis (SNA) and visualization to the representations of rabbinic interactions in the Babylonian Talmud, a sprawling text written in Hebrew and Aramaic and probably redacted in Babylonia (modern day Iraq) in the sixth century CE. Our goals are (1) to develop a workflow and methodology allowing us to visualize and analyze the interactions between rabbis as represented in the Babylonian Talmud; (2) to see if we could learn something new about the relationship between rabbis as represented in the Talmud and/or the process of its redaction; and (3) to present a public-facing interface allowing scholars to interact directly with our visualization.</p>
<p>Many of the research questions that drive this project go back more than a century. Pioneering work in Jewish studies (especially Albeck (1969); Margolioth (1987)) has attempted to detail the relationships between some of these rabbis. This work remains valuable, although it sometimes uses outdated methodological assumptions. Some of the relationships, for example, are reconstructed on the basis of stories about rabbis that most scholars today would understand as late, fictional creations. So too, scholars have long tried to understand the process by which the Babylonian Talmud was redacted (for summary of the scholarship on this, see Rubenstein 2013). Historians have also tried to understand the rabbis as a “network”, although without applying the quantitative tools now available (Hezser (1997); Lapin (2012)).</p>
<p>In this part of the project, we focused our attention on citation chains. Rabbis frequently say things in the name of other rabbis (e.g., “Rabbi X said in the name of Rabbi Y who said in the name of Rabbi Z” – these chains usually consist of two or three names but can go up to nine!). By focusing on simply the names in these chains (and not the content of what they reported), our work intersects with that of Zhitomirsky-Geffet and Prebor (2018) and Josh Waxman (2019). At the same time, both our workflow and the kinds of questions that we were asking of the network as a whole make it distinct.</p>
<p>The first step in our workflow was to identify each instance of a citation chain in the Hebrew/Aramaic text. In order to do this, we compiled a list of the names of all rabbis mentioned in ancient rabbinic literature (along with any aliases that they had) and assigned each a unique numeric identifier. The list was created through both automated and manual processes. Then, we created and ran a pattern matching program on a digital version of the “standard” printed edition (Vilna) of the Babylonian Talmud text to identify instances of rabbinic names and citation chains. Once identified, the program split the citations into “source” and “target” rabbis so we could identify who was citing whom. The results of the automated process were highly accurate as we verified through manual review of a statistically significant sample.</p>
<p>The program identified 5,245 citation instances. When grouped into unique interactions (e.g., Rabbi X may cite Rabbi Y twenty times, but we counted that as one unique interaction), we were left with 630 rabbis (our nodes) and 1217 unique interactions (our edges). We loaded our node and edges files into Gephi (Gephi) and UCINET. A visualization can be seen in Figure 1, which (using a Force Atlas 2 layout) groups the more connected rabbis toward the center.</p>
<p>Figure 1: Graph of All Rabbis in Babylonian Talmud Who Appear in Citation Chains</p>
<p>We have two major research findings. First, and less surprisingly, when separated into Modularity Classes through an unsupervised algorithm, the rabbis relatively cleanly separated into groups that clustered around rabbinic figures who themselves had many connections, which looks like a “school” structure (see Figure 2). Previous research has led us to expect this. Second, and more surprisingly, the rabbis at the centers of each of those circles were themselves densely and directly connected to each other. These ten or fifteen rabbis, over four centuries and two geographical locales, served as the backbone for the rabbinic network. It is still unclear to us whether these connections represent real social interactions or can better be explained as the result of later editing and redactional decisions and conventions.</p>
<p>Figure 2: Rabbis in Citation Chains in Babylonian Talmud in Modularity Classes</p>
<p>There are problems inherent in this data. The rabbis are themselves sometimes unsure about an attribution (and explicitly argue about it). Since rabbis sometimes shared names or nicknames, it is sometimes impossible to match with certainty a name with a distinct individual; in such cases we assigned the shared name to the more prominent rabbi. Moreover, we are just using one, easily available text. Manuscripts sometimes record these attributions differently. We feel that given the macro approach we took to this network, these problems become less significant. Nevertheless, they need to be better taken into account in future, more fine-grained analyses.</p>
<p>By the time this is published, we should have our data and code freely available on Github. It may take us longer to develop a public-facing interface, perhaps along the lines of “The Six Degrees of Francis Bacon.” We will also extend our approach to other interactions in the Babylonian Talmud (e.g., when a rabbi asks a question of another rabbi); to other rabbinic texts from this period; and to other manuscript versions of these texts.</p>
<p><strong>Bibliography</strong></p>
<p>Albeck, Ch. 1969. <em>Introduction to the Talmud Babli and Yerushalmi</em>. Tel Aviv: Dvir (in Hebrew).</p>
<p>Gephi: https://gephi.org/</p>
<p>Hezser, C. 1997. <em>The Social Structure of the Rabbinic Movement in Roman Palestine</em>. Tübingen: Mohr Siebeck.</p>
<p>Lapin, H. 2012. <em>Rabbis as Romans: The Rabbinic Movement in Palestine, 100-400 CE</em>. New York: Oxford University Press.</p>
<p>Margolioth, M. 1987 (rpt). <em>Encyclopedia of Talmudic and Geonic Literature</em>. Tel-Aviv: Y. Orenstein (in Hebrew).</p>
<p>Rubenstein, J. 2013. “Translator’s Introduction,” in David Weiss Halivni, <em>The Formation of the Babylonian Talmud</em> (New York: Oxford University Press), xvii-xxx.</p>
<p>Six Degrees of Francis Bacon: http://www.sixdegreesoffrancisbacon.com/?ids=10000473&min_confidence=60&type=network</p>
<p>UCINET: https://sites.google.com/site/ucinetsoftware/home</p>
<p>Waxman, J. “Mi vaMi: A Graph Databse of the Babylonian Talmud.” http://www.mivami.org/</p>
<p>Waxman, J. 2019. “A Graph Database of Scholastic Relationships in the Babylonian Talmud”: https://www.narcis.nl/dataset/RecordID/hdl%3A10411%2FK8HHQZ</p>
<p>Zhitomirsky-Geffet, M. and Prebor, G. 2019. “SageBook: Toward a Cross-Generational Social Network for the Jewish Sages’ Prosopography.” <em>Digital Scholarship in the Humanities</em> 34.3: 676-695 (https://doi.org/10.1093/llc/fqy065)</p>
","This project attempts to apply the techniques of social network analysis (SNA) and visualization to the representations of rabbinic interactions in the Babylonian Talmud, a sprawling text written in Hebrew and Aramaic and probably redacted in Babylonia (modern day Iraq) in the sixth century CE. Our goals are (1) to develop a workflow and methodology allowing us to visualize and analyze the interactions between rabbis as represented in the Babylonian Talmud; (2) to see if we could learn something new about the relationship between rabbis as represented in the Talmud and/or the process of its redaction; and (3) to present a public-facing interface allowing scholars to interact directly with our visualization.
Many of the research questions that drive this project go back more than a century. Pioneering work in Jewish studies (especially Albeck (1969); Margolioth (1987)) has attempted to detail the relationships between some of these rabbis. This work remains valuable, although it sometimes uses outdated methodological assumptions. Some of the relationships, for example, are reconstructed on the basis of stories about rabbis that most scholars today would understand as late, fictional creations. So too, scholars have long tried to understand the process by which the Babylonian Talmud was redacted (for summary of the scholarship on this, see Rubenstein 2013). Historians have also tried to understand the rabbis as a “network”, although without applying the quantitative tools now available (Hezser (1997); Lapin (2012)).
In this part of the project, we focused our attention on citation chains. Rabbis frequently say things in the name of other rabbis (e.g., “Rabbi X said in the name of Rabbi Y who said in the name of Rabbi Z” – these chains usually consist of two or three names but can go up to nine!). By focusing on simply the names in these chains (and not the content of what they reported), our work intersects with that of Zhitomirsky-Geffet and Prebor (2018) and Josh Waxman (2019). At the same time, both our workflow and the kinds of questions that we were asking of the network as a whole make it distinct.
The first step in our workflow was to identify each instance of a citation chain in the Hebrew/Aramaic text. In order to do this, we compiled a list of the names of all rabbis mentioned in ancient rabbinic literature (along with any aliases that they had) and assigned each a unique numeric identifier. The list was created through both automated and manual processes. Then, we created and ran a pattern matching program on a digital version of the “standard” printed edition (Vilna) of the Babylonian Talmud text to identify instances of rabbinic names and citation chains. Once identified, the program split the citations into “source” and “target” rabbis so we could identify who was citing whom. The results of the automated process were highly accurate as we verified through manual review of a statistically significant sample.
The program identified 5,245 citation instances. When grouped into unique interactions (e.g., Rabbi X may cite Rabbi Y twenty times, but we counted that as one unique interaction), we were left with 630 rabbis (our nodes) and 1217 unique interactions (our edges). We loaded our node and edges files into Gephi (Gephi) and UCINET. A visualization can be seen in Figure 1, which (using a Force Atlas 2 layout) groups the more connected rabbis toward the center.
Figure 1: Graph of All Rabbis in Babylonian Talmud Who Appear in Citation Chains
We have two major research findings. First, and less surprisingly, when separated into Modularity Classes through an unsupervised algorithm, the rabbis relatively cleanly separated into groups that clustered around rabbinic figures who themselves had many connections, which looks like a “school” structure (see Figure 2). Previous research has led us to expect this. Second, and more surprisingly, the rabbis at the centers of each of those circles were themselves densely and directly connected to each other. These ten or fifteen rabbis, over four centuries and two geographical locales, served as the backbone for the rabbinic network. It is still unclear to us whether these connections represent real social interactions or can better be explained as the result of later editing and redactional decisions and conventions.
Figure 2: Rabbis in Citation Chains in Babylonian Talmud in Modularity Classes
There are problems inherent in this data. The rabbis are themselves sometimes unsure about an attribution (and explicitly argue about it). Since rabbis sometimes shared names or nicknames, it is sometimes impossible to match with certainty a name with a distinct individual; in such cases we assigned the shared name to the more prominent rabbi. Moreover, we are just using one, easily available text. Manuscripts sometimes record these attributions differently. We feel that given the macro approach we took to this network, these problems become less significant. Nevertheless, they need to be better taken into account in future, more fine-grained analyses.
By the time this is published, we should have our data and code freely available on Github. It may take us longer to develop a public-facing interface, perhaps along the lines of “The Six Degrees of Francis Bacon.” We will also extend our approach to other interactions in the Babylonian Talmud (e.g., when a rabbi asks a question of another rabbi); to other rabbinic texts from this period; and to other manuscript versions of these texts.
Bibliography
Albeck, Ch. 1969. Introduction to the Talmud Babli and Yerushalmi. Tel Aviv: Dvir (in Hebrew).
Gephi: https://gephi.org/
Hezser, C. 1997. The Social Structure of the Rabbinic Movement in Roman Palestine. Tübingen: Mohr Siebeck.
Lapin, H. 2012. Rabbis as Romans: The Rabbinic Movement in Palestine, 100-400 CE. New York: Oxford University Press.
Margolioth, M. 1987 (rpt). Encyclopedia of Talmudic and Geonic Literature. Tel-Aviv: Y. Orenstein (in Hebrew).
Rubenstein, J. 2013. “Translator’s Introduction,” in David Weiss Halivni, The Formation of the Babylonian Talmud (New York: Oxford University Press), xvii-xxx.
Six Degrees of Francis Bacon: http://www.sixdegreesoffrancisbacon.com/?ids=10000473&min_confidence=60&type=network
UCINET: https://sites.google.com/site/ucinetsoftware/home
Waxman, J. “Mi vaMi: A Graph Databse of the Babylonian Talmud.” http://www.mivami.org/
Waxman, J. 2019. “A Graph Database of Scholastic Relationships in the Babylonian Talmud”: https://www.narcis.nl/dataset/RecordID/hdl%3A10411%2FK8HHQZ
Zhitomirsky-Geffet, M. and Prebor, G. 2019. “SageBook: Toward a Cross-Generational Social Network for the Jewish Sages’ Prosopography.” Digital Scholarship in the Humanities 34.3: 676-695 (https://doi.org/10.1093/llc/fqy065)","michael_satlow@brown.edu, mike.sperling@rocketmail.com",Lightning
"Bowker, Lynne","University of Ottawa, Canada",Improving machine translation literacy to facilitate and enhance scholarly communication,digital literacy; machine translation; machine translation literacy; human-computer-interaction; scholarly communication,"English
North America
Contemporary
artificial intelligence and machine learning
curricular and pedagogical development and analysis
Language acquisition
Translation studies",English,North America,Contemporary,"artificial intelligence and machine learning
curricular and pedagogical development and analysis","Language acquisition
Translation studies","<p>English is the main language of scholarly communication, but, most researchers are not native English speakers. Contemporary machine translation approaches such as neural machine translation (NMT) are data-driven and use artificial-intelligence-based machine learning techniques; however, such tools rarely produce high quality output of specialized text without human intervention. There is an emerging need for machine translation (MT) literacy among non-Anglpohone students and faculty who must both read and write in English in order to participate fully in the scholarly communication process. We designed and pilot tested a machine translation literacy workshop to help researchers use MT more effectively for scholarly tasks such as: 1) search and discovery of scholarly texts; 2) reading and evaluating scholarly texts; 3) research communication in international teams; and 4) writing for scholarly publishing. Pre- and post-workshop surveys were used to evaluate the success of the workshop and recommend improvements for future iterations.</p>
","English is the main language of scholarly communication, but, most researchers are not native English speakers. Contemporary machine translation approaches such as neural machine translation (NMT) are data-driven and use artificial-intelligence-based machine learning techniques; however, such tools rarely produce high quality output of specialized text without human intervention. There is an emerging need for machine translation (MT) literacy among non-Anglpohone students and faculty who must both read and write in English in order to participate fully in the scholarly communication process. We designed and pilot tested a machine translation literacy workshop to help researchers use MT more effectively for scholarly tasks such as: 1) search and discovery of scholarly texts; 2) reading and evaluating scholarly texts; 3) research communication in international teams; and 4) writing for scholarly publishing. Pre- and post-workshop surveys were used to evaluate the success of the workshop and recommend improvements for future iterations.",lbowker@uottawa.ca,Poster
"Barbot, Laure (1);
Dombrowski, Quinn (2);
Fischer, Frank (3);
Rockwell, Geoffrey (4);
Spiro, Lisa (5)","1: DARIAH;
2: Stanford University, United States of America;
3: Higher School of Economics, Moscow, Russia;
4: University of Alberta, Canada;
5: Rice University, United States of America",Who needs tool directories? A forum on sustaining discovery portals large and small,"directories, discovery, infrastructure, tools, sustainability","Global
English
Contemporary
digital research infrastructures development and analysis
meta-criticism (reflections on digital humanities and humanities computing)
Humanities computing",English,Global,Contemporary,"digital research infrastructures development and analysis
meta-criticism (reflections on digital humanities and humanities computing)",Humanities computing,"<p>Digital humanists broadly agree that tool directories are a good and valuable thing, worth building and maintaining, but there is no sustainability model. This forum aims to move beyond platitudes and interrogate the value of directories and possible models for sustaining them. <em>For whom</em> are tool directories valuable, and <em>in what context</em>?</p>
","Digital humanists broadly agree that tool directories are a good and valuable thing, worth building and maintaining, but there is no sustainability model. This forum aims to move beyond platitudes and interrogate the value of directories and possible models for sustaining them. For whom are tool directories valuable, and in what context?","laure.barbot@dariah.eu, qad@stanford.edu, frank.fischer@dariah.eu, grockwel@ualberta.ca, lspiro@rice.edu",Forum
"Fischer, Frank (1);
Busch, Anna (2);
Heyden, Linda-Rabea (3,4);
Schwindt, Mark (5)","1: Higher School of Economics, Moscow;
2: University of Potsdam;
3: Wikimedia Deutschland e.V.;
4: University of Jena;
5: Ruhr University Bochum",Faust Times Eighteen: A Network Analysis of Theatre Plays Around the Myth of Faust,"Faust, drama, plays, literature","Europe
English
18th Century
19th Century
20th Century
network analysis and graphs theory and application
Literary studies",English,Europe,"18th Century
19th Century
20th Century",network analysis and graphs theory and application,Literary studies,"<p>Alongside the comparative network analysis of larger literary corpora (Algee-Hewitt 2017, Trilcke/Fischer 2018), there has recently been a trend towards focusing on author-centred subcorpora, such as the oeuvres of Jane Austen (Wade 2017) or Anton Chekhov (Faynberg et al. 2018).<br />Instead of picking out individual authors, we can instead focus on productive literary topoi instead and examine them with the means of network analysis, something that has not been undertaken yet as far as we can see. The abundance of dramas revolving around the Faust myth will serve as an example.<br />The German Drama Corpus (https://dracor.org/ger) currently holds 18 TEI-encoded plays that centre around a Faust character. They range from early plays like Weidmann's ""Johann Faust"" (1775) to the two parts of Goethe's ""Faust"" (1808, 1832) and Friedrich Theodor Vischer's ""Faust, part III"" (1862), but also feature mash-ups like Grabbe's ""Don Juan und Faust"" (1829) and a version with a female Faust character, Wilhelm Schäfer's ""Faustine"" (1898).<br />Network analysis enables us to take a comparative macroscopic look at the different structures of these plays. For example, the roles of the devil/sub-devil Mephistopheles and the famulus Wagner can be examined more closely, shedding new light on the structural development of the sujet. It will become clear when and where these characters appear – but also where they are missing. Other types of characters also come to the fore, such as Gretchen or even Faust's parents, who play no role in Goethe's version but who do appear in other plays.<br />Our poster first offers network visualisations that concentrate on the centres, i.e., the constellation directly around the Faust character, especially in the larger plays (Soden's ""Doctor Faust"" has 62 characters, Julius von Voss's ""Faust"" has 72 characters, Avenarius' ""Faust"" has 95 characters, Goethe's first part of ""Faust"" has 115 and the second part even 189 distinguishable characters/voices). The visualisations are supported by statistical network measures, such as the Betweenness Centrality, Degree and Weighted Degree. In addition to visual evidence, these measures can also address the positioning of the characters surrounding Faust in a new way.<br />The poster will demonstrate how network analytical and quantitative aspects complement existing literary research on the topic (for an overview of the wealth of literary works around Faust cf. Hucke 1992).<br />Bibliography<br />Algee-Hewitt, Mark (2017): Distributed Character: Quantitative Models of the English Stage, 1550–1900. In: New Literary History 48(4), pp. 751–782. Johns Hopkins University Press. DOI: https://doi.org/10.1353/nlh.2017.0038<br />Faynberg, Veronika et al. (2018): Netzwerkanalytischer Blick auf die Dramen Anton Tschechows. In: DHd2018 Conference Abstracts, University of Cologne, pp. 439. DOI: https://doi.org/10.6084/m9.figshare.6410909<br />Hucke, Karl-Heinz (1992): Figuren der Unruhe. Faustdichtungen. Tübingen: Niemeyer. Reprint: de Gruyter 2014.<br />Trilcke, Peer; Fischer, Frank (2018): Literaturwissenschaft als Hackathon. Zur Praxeologie der Digital Literary Studies und ihren epistemischen Dingen. In: Martin Huber, Sybille Krämer (eds.): Wie Digitalität die Geisteswissenschaften verändert: Neue Forschungsgegenstände und Methoden (= 3rd Special Edition of 'Zeitschrift für digitale Geisteswissenschaften'). DOI: http://dx.doi.org/10.17175/sb003_003<br />Wade, Karen (2017): Jane Austen's Social Networks. In: The Sea of Books. URL: https://theseaofbooks.com/2017/07/04/jane-austens-social-networks/</p>
","Alongside the comparative network analysis of larger literary corpora (Algee-Hewitt 2017, Trilcke/Fischer 2018), there has recently been a trend towards focusing on author-centred subcorpora, such as the oeuvres of Jane Austen (Wade 2017) or Anton Chekhov (Faynberg et al. 2018).
Instead of picking out individual authors, we can instead focus on productive literary topoi instead and examine them with the means of network analysis, something that has not been undertaken yet as far as we can see. The abundance of dramas revolving around the Faust myth will serve as an example.
The German Drama Corpus (https://dracor.org/ger) currently holds 18 TEI-encoded plays that centre around a Faust character. They range from early plays like Weidmann's ""Johann Faust"" (1775) to the two parts of Goethe's ""Faust"" (1808, 1832) and Friedrich Theodor Vischer's ""Faust, part III"" (1862), but also feature mash-ups like Grabbe's ""Don Juan und Faust"" (1829) and a version with a female Faust character, Wilhelm Schäfer's ""Faustine"" (1898).
Network analysis enables us to take a comparative macroscopic look at the different structures of these plays. For example, the roles of the devil/sub-devil Mephistopheles and the famulus Wagner can be examined more closely, shedding new light on the structural development of the sujet. It will become clear when and where these characters appear – but also where they are missing. Other types of characters also come to the fore, such as Gretchen or even Faust's parents, who play no role in Goethe's version but who do appear in other plays.
Our poster first offers network visualisations that concentrate on the centres, i.e., the constellation directly around the Faust character, especially in the larger plays (Soden's ""Doctor Faust"" has 62 characters, Julius von Voss's ""Faust"" has 72 characters, Avenarius' ""Faust"" has 95 characters, Goethe's first part of ""Faust"" has 115 and the second part even 189 distinguishable characters/voices). The visualisations are supported by statistical network measures, such as the Betweenness Centrality, Degree and Weighted Degree. In addition to visual evidence, these measures can also address the positioning of the characters surrounding Faust in a new way.
The poster will demonstrate how network analytical and quantitative aspects complement existing literary research on the topic (for an overview of the wealth of literary works around Faust cf. Hucke 1992).
Bibliography
Algee-Hewitt, Mark (2017): Distributed Character: Quantitative Models of the English Stage, 1550–1900. In: New Literary History 48(4), pp. 751–782. Johns Hopkins University Press. DOI: https://doi.org/10.1353/nlh.2017.0038
Faynberg, Veronika et al. (2018): Netzwerkanalytischer Blick auf die Dramen Anton Tschechows. In: DHd2018 Conference Abstracts, University of Cologne, pp. 439. DOI: https://doi.org/10.6084/m9.figshare.6410909
Hucke, Karl-Heinz (1992): Figuren der Unruhe. Faustdichtungen. Tübingen: Niemeyer. Reprint: de Gruyter 2014.
Trilcke, Peer; Fischer, Frank (2018): Literaturwissenschaft als Hackathon. Zur Praxeologie der Digital Literary Studies und ihren epistemischen Dingen. In: Martin Huber, Sybille Krämer (eds.): Wie Digitalität die Geisteswissenschaften verändert: Neue Forschungsgegenstände und Methoden (= 3rd Special Edition of 'Zeitschrift für digitale Geisteswissenschaften'). DOI: http://dx.doi.org/10.17175/sb003_003
Wade, Karen (2017): Jane Austen's Social Networks. In: The Sea of Books. URL: https://theseaofbooks.com/2017/07/04/jane-austens-social-networks/","frank.fischer@dariah.eu, annabusch@uni-potsdam.de, heyden.linda@gmail.com, info@markschwindt.com",Poster
"Weenink, Maartje","Manchester Metropolitan University, United Kingdom",Who's Afraid of the Big Bad ?: Researching Trends in the Early Gothic Novel using Word Embeddings,"word embeddings, gothic, genre formation, sentiment analysis","Europe
English
18th Century
19th Century
database creation, management, and analysis
natural language processing
Linguistics
Literary studies",English,Europe,"18th Century
19th Century","database creation, management, and analysis
natural language processing","Linguistics
Literary studies","<p>This poster will present the preliminary results of my PhD project which researches trends in the early Gothic novel using computational methods. A corpus of over 2500 British early Gothic texts has been created and explored using a combination of the analysis of word embeddings and sub-sections of the corpus defined by annotated meta-data. Variations in the embeddings for such sub-corpora demonstrate that various established theories such as the assumed British preoccupation with European national identity in Gothic fiction, the tendency to categorise all Gothic novels as filled with negative sentiment, or the ambiguously defined 'female Gothic', warrant re-evaluation and further exploration assisted by quantitative methods. Notable changes in embeddings for specific datasets such those comprised of texts written at the onset of the Gothic's popularity, or by female authors, are visualised in this poster.</p>
","This poster will present the preliminary results of my PhD project which researches trends in the early Gothic novel using computational methods. A corpus of over 2500 British early Gothic texts has been created and explored using a combination of the analysis of word embeddings and sub-sections of the corpus defined by annotated meta-data. Variations in the embeddings for such sub-corpora demonstrate that various established theories such as the assumed British preoccupation with European national identity in Gothic fiction, the tendency to categorise all Gothic novels as filled with negative sentiment, or the ambiguously defined 'female Gothic', warrant re-evaluation and further exploration assisted by quantitative methods. Notable changes in embeddings for specific datasets such those comprised of texts written at the onset of the Gothic's popularity, or by female authors, are visualised in this poster.",maartje.weenink@stu.mmu.ac.uk,Poster
"Ciotti, Fabio","Università di Roma Tor Vergata, Italy","Theoretical intersections: cognitive poetics, cultural evolution, and distant reading in literary studies","distant reading, cognitive poetics, computational literary studies, cultural evolution, cultural analytics","Europe
English
20th Century
Contemporary
text encoding and markup language creation, deployment, and analysis
text mining and analysis
Cognitive sciences and psychology
Literary studies",English,Europe,"20th Century
Contemporary","text encoding and markup language creation, deployment, and analysis
text mining and analysis","Cognitive sciences and psychology
Literary studies","<p>Since Franco Moretti coined the widely successful term “distant reading” (Moretti, 2000) quantitative/computational text analysis methods have gained a wide circulation in literary studies. We can even speak of a <em>distant reading school</em>, nowadays. The diffusion of distant reading approaches has raised a lively debate (mostly in the North American context), and has attracted various criticisms, both from “traditional literary scholars” and self-critical adopters that can be subsumed into a threefold typology:</p>
<ul><li>Theoretical/Ideological: the intentional and qualitative nature of the literary domain is in principle irreducible to quantitative and computational methods; literature is not data (Marche, 2012) and literary criticism is not data analytics (Fish, 2012);</li>
<li>Methodological: the (statistical/computational) models and methods adopted for literary analysis are wrong, inaccurate, and ultimately inadequate (Da, 2019);</li>
<li>Pragmatical: the limits in the representativeness of the textual data set used in the analysis and the problems in defining the adequacy of its selection criteria (Bode, 2018).</li>
</ul><p>Each of these kinds of criticisms would need a deep discussion and are strictly interconnected. For instance, in the well-known Da’s articles, allegedly oriented presented as a replication failure study, the following excerpts makes apparent that the author has an a priori skepticism about the epistemological possibility of what she calls “computational literary studies:</p>
<p>There is a fundamental mismatch between the statistical tools that are used and the objects to which they are applied. (Da, 2019: 601)</p>
<p>[…] It may be the case that computational textual analysis has a threshold of optimal utility, and literature—in particular, reading literature well—is that cut-off point. (2019: 639)</p>
<p>I think that one of the main reasons underlying these more or less critical positions toward distant reading is the fact that it lacks sound and coherent rationales from the point of view of the theory: in fact, we can say that distant reading is the first methodology in literary studies that does not come with a theory of literature embedded in it, as it was for all of its predecessors. Consequently, all distant reading studies derive their theoretical frameworks and terms from theories in the literary domain that generally relies on the fundamental idea that <strong>literary texts can be explained only by the way of interpretation or if we prefer of hermeneutics</strong>.</p>
<p>The problem is that any literary interpretation based on quantitative, immanent, and purely formalist approach is subject to the theoretical criticism that was expressed by Stanley Fish in his harsh and seemingly ultimate criticism to stylistics in “What Is Stylistics, and Why Are They Saying Such Terrible Things About It?” (Fish, 1980). The point for Fish was not to criticize the methods <em>per se</em>, but the possibility to extract meaningful literary interpretations directly from the simple linguistic facts, the idea of an “algorithmic interpretation” (Fish’s words!), since interpretation always starts from a contextual and situated point of view that pre-defines the very objects of its actuation.</p>
<p>Many important scholars active in the field do believe that there is the possibility to reconcile traditional theories (of literature) with computational/quantitative methods. Just to make a couple of examples of these consilience theses, we can cite Andrew Piper (Piper, 2018) and Michael Gavin (Gavin, 2018). Piper proposes a sort of computational hermeneutics, that integrates distant and close, and quantitative and qualitative readings. Michael Gavin argues that “vector semantics share a set of assumptions with literary critic William Empson, who devoted his career to explaining how poets played with words’ many meanings”. What is suspicious in these (and similar but often less intriguing and thought-provoking) calls to the reconciliation of the two poles is that their outcomes are either literary critically unsatisfactory or are self-contradictory, in that the hermeneutical and critical part of the discourse is self-standing, the critical arguments are logically independent from the results of the computational analysis.</p>
<p>On the base of these considerations, I think that distant reading simply cannot be considered a methodological innovation to be applied to our pre-existing theories of literary texts (in all their rhizomatic variants): it is necessary to find a suitable theory or framework where these methods can yield to interesting results. To make a substantial step in this direction we should first of all take seriously the notion of distant reading and abandon the idea of literature as either made of singular special individuals (the great or the small texts, amenable to interpretations by literary critics, or the big or small writers) or reduced to abstract ideal type, under the scrutiny of literary theoreticians with no clue with empirical evidence. This move would import also taking seriously the move from (or renounce to) <strong>interpretation</strong> to (embrace) <strong>explanation</strong> as the real aim of the scholarly inquiry.</p>
<p>On what theoretical basis, then, can we construct a notion of literature amenable to distant reading methods?</p>
<p>One possible direction to be explored as some scholars like Ted Underwood suggests (English and Underwood, 2016; Underwood, 2017), is that distant reading should fall inside the tradition of sociology of literature or history of ideas a la <em>Nouvelle Histoire</em>. Although there are many reasons to lean toward a sociological vision of literature as an optimal base for distant reading, I think that an even better theoretical framework is that of the cognitive and bio-evolutionistic approaches to literature and the cultural evolution studies. Cognitive poetics/narratology, and bio/evolutionary literary studies have been two of the most interesting waves of innovation in the literary field of the last 30 years and are now established field of inquiry. With different graduation depending on the authors, they have advocated the introduction of a more scientific methodology in the study of literature, looking for methodological and theoretical insights into cognitive science and evolutionary psychology.</p>
<p>What is more interesting for my thesis is that, not surprisingly, the debate around the legitimacy and acceptability of the cognitive approaches in literary studies has determined a discussion on the problem of literary interpretation that has many similarities with the discourse I have proposed in this paper. Recently, a young and brilliant scholar active in the field, Marco Caracciolo, has re-opened the debate, and in doing this he has explicitly stated that “In order to contribute to cognitive science, literary scholarship has to complement—and in some cases even supplant—interpretation with a different set of goals and methods”. (Caracciolo, 2016: 193).</p>
<p>The other scientific field where literary studies can find a theoretical framework that takes great advantage of distant reading methodology is that of cultural evolution. This field of study that as of now has no application in literary studies, aims at providing a naturalist and empirical explanation of the nature and evolution of culture, adopting widely mathematical/statistical and computational modeling.</p>
<p>One of the theoretical underpinnings of cultural evolution is the adoption of the <strong>population thinking</strong> framework, taken from evolutionary biology (after Ernest Mayr interpretation of Darwin’s theory) and population genetics, and its application to cultural phenomena as pointed out recently by Dan Sperber and his collaborators (Claidière et al., 2014):</p>
<p>Literature is part of the cultural sphere, so it can be considered a population of individual items (the texts) whose members are defined by a set of measurable features. The description of the population at a given state (synchronic, in our beloved Sausurrian terms) and its evolution (diachronic) is feasible by the way of statistical and data-driven analysis.</p>
<p>To conclude, I think that in order to take full advantage of the most advanced methods and analytical techniques encompassed by the label Distant reading, like text mining and machine learning, in literary and cultural studies, we need to find a proper theoretical framework that gives sense to the hypothesis experiments, data sets and explanations we can generate. The attempt to justify and anchor this approach in the context of the traditional literary theories and methodologies has proven a limitation that undermines the interesting analytical results, and it is easily amenable to the ‘so what’ criticism, or ideological attacks. Maybe it is time to change the framework, and to abandon the classical hermeneutical literary studies environment.</p>
<p>References</p>
<p><strong>Bode, K.</strong> (2018). <em>A World of Fiction: Digital Collections and the Future of Literary History</em>. Ann Arbor, MI: University of Michigan Press.</p>
<p><strong>Caracciolo, M.</strong> (2016). Cognitive Literary Studies and the Status of Interpretation: An Attempt at Conceptual Mapping. <em>New Literary History</em>, <strong>47</strong>(1): 187–207 doi:10.1353/nlh.2016.0003.</p>
<p><strong>Claidière, N., Scott-Phillips, T. C. and Sperber, D.</strong> (2014). How Darwinian is cultural evolution?. <em>Philosophical Transactions of the Royal Society B: Biological Sciences</em>, <strong>369</strong>(1642): 20130368 doi:10.1098/rstb.2013.0368.</p>
<p><strong>Da, N. Z.</strong> (2019). The Computational Case against Computational Literary Studies. <em>Critical Inquiry</em>, <strong>45</strong>(3): 601–39 doi:10.1086/702594.</p>
<p><strong>English, J. F. and Underwood, T.</strong> (2016). Shifting Scales: Between Literature and Social Science. <em>Modern Language Quarterly</em>, <strong>77</strong>(3): 277–95 doi:10.1215/00267929-3570612.</p>
<p><strong>Fish, S.</strong> (2012). Mind Your P’s and B’s: The Digital Humanities and Interpretation. <em>Opinionator, New York Times</em> http://opinionator.blogs.nytimes.com/2012/01/23/mind-your-ps-and-bs-the-digital-humanities-and-interpretation/ (accessed 24 January 2012).</p>
<p><strong>Fish, S. E.</strong> (1980). What is stylistics and why are they saying such terrible things about it?. <em>Is There a Text in This Class? The Authority of Interpretive Communities</em>. Cambridge, Mass.: Harvard University Press, pp. 68–96.</p>
<p><strong>Gavin, M.</strong> (2018). Vector Semantics, William Empson, and the Study of Ambiguity. <em>Critical Inquiry</em>, <strong>44</strong>(4): 641–73 doi:10.1086/698174.</p>
<p><strong>Marche, S.</strong> (2012). Literature is not Data: Against Digital Humanities. <em>Los Angeles Review of Books</em> http://lareviewofbooks.org/essay/literature-is-not-data-against-digital-humanities#.</p>
<p><strong>Moretti, F.</strong> (2000). Conjectures on World Literature. <em>The New Left Review</em> http://newleftreview.org/A2094.</p>
<p><strong>Piper, A.</strong> (2018). <em>Enumerations: Data and Literary Study</em>. Chicago ; London: The University of Chicago Press.</p>
<p><strong>Sperber, D.</strong> (1996). <em>Explaining Culture: A Naturalistic Approach</em>. Oxford: Basil Blackwell.</p>
<p><strong>Underwood, T.</strong> (2017). A Genealogy of Distant Reading. <em>DHQ</em>, <strong>11</strong>(2) http://www.digitalhumanities.org/dhq/vol/11/2/000317/000317.html (accessed 20 October 2017).</p>
","Since Franco Moretti coined the widely successful term “distant reading” (Moretti, 2000) quantitative/computational text analysis methods have gained a wide circulation in literary studies. We can even speak of a distant reading school, nowadays. The diffusion of distant reading approaches has raised a lively debate (mostly in the North American context), and has attracted various criticisms, both from “traditional literary scholars” and self-critical adopters that can be subsumed into a threefold typology:
Theoretical/Ideological: the intentional and qualitative nature of the literary domain is in principle irreducible to quantitative and computational methods; literature is not data (Marche, 2012) and literary criticism is not data analytics (Fish, 2012);
Methodological: the (statistical/computational) models and methods adopted for literary analysis are wrong, inaccurate, and ultimately inadequate (Da, 2019);
Pragmatical: the limits in the representativeness of the textual data set used in the analysis and the problems in defining the adequacy of its selection criteria (Bode, 2018).
Each of these kinds of criticisms would need a deep discussion and are strictly interconnected. For instance, in the well-known Da’s articles, allegedly oriented presented as a replication failure study, the following excerpts makes apparent that the author has an a priori skepticism about the epistemological possibility of what she calls “computational literary studies:
There is a fundamental mismatch between the statistical tools that are used and the objects to which they are applied. (Da, 2019: 601)
[…] It may be the case that computational textual analysis has a threshold of optimal utility, and literature—in particular, reading literature well—is that cut-off point. (2019: 639)
I think that one of the main reasons underlying these more or less critical positions toward distant reading is the fact that it lacks sound and coherent rationales from the point of view of the theory: in fact, we can say that distant reading is the first methodology in literary studies that does not come with a theory of literature embedded in it, as it was for all of its predecessors. Consequently, all distant reading studies derive their theoretical frameworks and terms from theories in the literary domain that generally relies on the fundamental idea that literary texts can be explained only by the way of interpretation or if we prefer of hermeneutics.
The problem is that any literary interpretation based on quantitative, immanent, and purely formalist approach is subject to the theoretical criticism that was expressed by Stanley Fish in his harsh and seemingly ultimate criticism to stylistics in “What Is Stylistics, and Why Are They Saying Such Terrible Things About It?” (Fish, 1980). The point for Fish was not to criticize the methods per se, but the possibility to extract meaningful literary interpretations directly from the simple linguistic facts, the idea of an “algorithmic interpretation” (Fish’s words!), since interpretation always starts from a contextual and situated point of view that pre-defines the very objects of its actuation.
Many important scholars active in the field do believe that there is the possibility to reconcile traditional theories (of literature) with computational/quantitative methods. Just to make a couple of examples of these consilience theses, we can cite Andrew Piper (Piper, 2018) and Michael Gavin (Gavin, 2018). Piper proposes a sort of computational hermeneutics, that integrates distant and close, and quantitative and qualitative readings. Michael Gavin argues that “vector semantics share a set of assumptions with literary critic William Empson, who devoted his career to explaining how poets played with words’ many meanings”. What is suspicious in these (and similar but often less intriguing and thought-provoking) calls to the reconciliation of the two poles is that their outcomes are either literary critically unsatisfactory or are self-contradictory, in that the hermeneutical and critical part of the discourse is self-standing, the critical arguments are logically independent from the results of the computational analysis.
On the base of these considerations, I think that distant reading simply cannot be considered a methodological innovation to be applied to our pre-existing theories of literary texts (in all their rhizomatic variants): it is necessary to find a suitable theory or framework where these methods can yield to interesting results. To make a substantial step in this direction we should first of all take seriously the notion of distant reading and abandon the idea of literature as either made of singular special individuals (the great or the small texts, amenable to interpretations by literary critics, or the big or small writers) or reduced to abstract ideal type, under the scrutiny of literary theoreticians with no clue with empirical evidence. This move would import also taking seriously the move from (or renounce to) interpretation to (embrace) explanation as the real aim of the scholarly inquiry.
On what theoretical basis, then, can we construct a notion of literature amenable to distant reading methods?
One possible direction to be explored as some scholars like Ted Underwood suggests (English and Underwood, 2016; Underwood, 2017), is that distant reading should fall inside the tradition of sociology of literature or history of ideas a la Nouvelle Histoire. Although there are many reasons to lean toward a sociological vision of literature as an optimal base for distant reading, I think that an even better theoretical framework is that of the cognitive and bio-evolutionistic approaches to literature and the cultural evolution studies. Cognitive poetics/narratology, and bio/evolutionary literary studies have been two of the most interesting waves of innovation in the literary field of the last 30 years and are now established field of inquiry. With different graduation depending on the authors, they have advocated the introduction of a more scientific methodology in the study of literature, looking for methodological and theoretical insights into cognitive science and evolutionary psychology.
What is more interesting for my thesis is that, not surprisingly, the debate around the legitimacy and acceptability of the cognitive approaches in literary studies has determined a discussion on the problem of literary interpretation that has many similarities with the discourse I have proposed in this paper. Recently, a young and brilliant scholar active in the field, Marco Caracciolo, has re-opened the debate, and in doing this he has explicitly stated that “In order to contribute to cognitive science, literary scholarship has to complement—and in some cases even supplant—interpretation with a different set of goals and methods”. (Caracciolo, 2016: 193).
The other scientific field where literary studies can find a theoretical framework that takes great advantage of distant reading methodology is that of cultural evolution. This field of study that as of now has no application in literary studies, aims at providing a naturalist and empirical explanation of the nature and evolution of culture, adopting widely mathematical/statistical and computational modeling.
One of the theoretical underpinnings of cultural evolution is the adoption of the population thinking framework, taken from evolutionary biology (after Ernest Mayr interpretation of Darwin’s theory) and population genetics, and its application to cultural phenomena as pointed out recently by Dan Sperber and his collaborators (Claidière et al., 2014):
Literature is part of the cultural sphere, so it can be considered a population of individual items (the texts) whose members are defined by a set of measurable features. The description of the population at a given state (synchronic, in our beloved Sausurrian terms) and its evolution (diachronic) is feasible by the way of statistical and data-driven analysis.
To conclude, I think that in order to take full advantage of the most advanced methods and analytical techniques encompassed by the label Distant reading, like text mining and machine learning, in literary and cultural studies, we need to find a proper theoretical framework that gives sense to the hypothesis experiments, data sets and explanations we can generate. The attempt to justify and anchor this approach in the context of the traditional literary theories and methodologies has proven a limitation that undermines the interesting analytical results, and it is easily amenable to the ‘so what’ criticism, or ideological attacks. Maybe it is time to change the framework, and to abandon the classical hermeneutical literary studies environment.
References
Bode, K. (2018). A World of Fiction: Digital Collections and the Future of Literary History. Ann Arbor, MI: University of Michigan Press.
Caracciolo, M. (2016). Cognitive Literary Studies and the Status of Interpretation: An Attempt at Conceptual Mapping. New Literary History, 47(1): 187–207 doi:10.1353/nlh.2016.0003.
Claidière, N., Scott-Phillips, T. C. and Sperber, D. (2014). How Darwinian is cultural evolution?. Philosophical Transactions of the Royal Society B: Biological Sciences, 369(1642): 20130368 doi:10.1098/rstb.2013.0368.
Da, N. Z. (2019). The Computational Case against Computational Literary Studies. Critical Inquiry, 45(3): 601–39 doi:10.1086/702594.
English, J. F. and Underwood, T. (2016). Shifting Scales: Between Literature and Social Science. Modern Language Quarterly, 77(3): 277–95 doi:10.1215/00267929-3570612.
Fish, S. (2012). Mind Your P’s and B’s: The Digital Humanities and Interpretation. Opinionator, New York Times http://opinionator.blogs.nytimes.com/2012/01/23/mind-your-ps-and-bs-the-digital-humanities-and-interpretation/ (accessed 24 January 2012).
Fish, S. E. (1980). What is stylistics and why are they saying such terrible things about it?. Is There a Text in This Class? The Authority of Interpretive Communities. Cambridge, Mass.: Harvard University Press, pp. 68–96.
Gavin, M. (2018). Vector Semantics, William Empson, and the Study of Ambiguity. Critical Inquiry, 44(4): 641–73 doi:10.1086/698174.
Marche, S. (2012). Literature is not Data: Against Digital Humanities. Los Angeles Review of Books http://lareviewofbooks.org/essay/literature-is-not-data-against-digital-humanities#.
Moretti, F. (2000). Conjectures on World Literature. The New Left Review http://newleftreview.org/A2094.
Piper, A. (2018). Enumerations: Data and Literary Study. Chicago ; London: The University of Chicago Press.
Sperber, D. (1996). Explaining Culture: A Naturalistic Approach. Oxford: Basil Blackwell.
Underwood, T. (2017). A Genealogy of Distant Reading. DHQ, 11(2) http://www.digitalhumanities.org/dhq/vol/11/2/000317/000317.html (accessed 20 October 2017).",fabio.ciotti@uniroma2.it,Short Presentation
"Satlow, Michael;
Mylonas, Elli","Brown University, United States of America",Inscriptions of Israel/Palestine,"epidoc, inscriptions, non-roman alphabets, epigraphy","Asia
English
BCE-4th Century
5th-14th Century
scholarly editing and editions development, analysis, and methods
text encoding and markup language creation, deployment, and analysis
History
Theology and religious studies",English,Asia,"BCE-4th Century
5th-14th Century","scholarly editing and editions development, analysis, and methods
text encoding and markup language creation, deployment, and analysis","History
Theology and religious studies","<p align=""center""></p>
<p>The “Inscriptions of Israel/Palestine” (IIP) project (www.brown.edu/iip) seeks to create a corpus of inscriptions (texts written on durable materials, other than coins) from the geographical location of present-day Israel/Palestine, that date from around the sixth century BCE to the seventh century CE. The inscriptions are in Greek, Latin, Hebrew, and Aramaic. The purpose of the project is not only to allow for access and robust (and ultimately, federated) searching but also for scholarly analyses. As one of the longest running active digital epigraphy projects (with over 4,000 inscriptions entered to date), IIP provides several use cases of working with a complex and challenging multi-lingual corpus. This abstract will focus on our data modeling and approach to Linked Open Data.</p>
<p>IIP was an early adopter of the Epidoc schema, a customization of TEI developed especially for those working with ancient texts preserved on durable materials, such as inscriptions and coins (Elliott, Bodard, Cayless). Many users of Epidoc are in contact with each other through epigraphy.info, and the schema is continually being modified in response to user requests. The general principle, however, is that each material object on which a text is inscribed or written is treated as a discrete XML file. IIP thus gives each inscription a unique, findable ID that also serves as its document name (e.g., ash0001.xml). Each file has an extensive teiHeader, in which we encode the metadata, where possible using controlled vocabularies as attributes of elements, linked to authority files. This kind of robust encoding allows for the database-style, faceted indexing and searching powered by SOLR that we provide through our interface. We also include images (although we have much more work to do collecting them) and geographical information. We have a geographical interface that allows for mapping (see Figure 1).</p>
<p>Figure 1: Screenshot of Search Page of ""Inscriptions of Israel/Palestine""</p>
<p>Our data has always been open. All of our XML files can be seen and downloaded individually directly from our site, or downloaded in bulk from our open Github site (https://github.com/Brown-University-Library/iip-texts) or through our API (for which we give detailed instructions on the site itself). We also encode our permission license (CC BY-NC 4.0) into our files.</p>
<p>Since participating in the conference, “The Big Ancient Mediterranean” (BAM) we have sought to create links in our data in three ways. The primary <em>geographical</em> data within each inscription is linked to its corresponding Pleiades Gazetteer id (Pleiades). The primary chronological data within each inscription is linked to its corresponding PeriodO id (PeriodO).   And each of the types of object upon which the inscription is written is linked to the Getty Art and Architecture Thesaurus (Getty). For example:</p>
<p>                        <origin></p>
<p>                     <date period=""http://n2t.net/ark:/99152/p0m63njbxb9""</p>
<p>                           notBefore=""0001""</p>
<p>                           notAfter=""0100"">First century CE</date></p>
<p>                     <placeName></p>
<p>                                <region>Judaea</region></p>
<p>                                <settlement ref = ""http://pleiades.stoa.org/places/687928""> Jerusalem</settlement></p>
<p>                                <geogName type=""site"">Akeldama Caves</geogName></p>
<p>                                <geogFeat type=""locus"">Cave 2 Chamber B</geogFeat></p>
<p>                            </placeName></p>
<p>                          </p>
<p>                            <p/></p>
<p>                        </origin></p>
<p>The use of these ids allows our data to be scraped live by different projects, such as Pleiades and the Pelagios Network (Pelagios Network). The community is just now beginning to develop an ecosystem that allows for the fruitful exchange of data between sites using Linked Open Data and we hope that through this expansion the usefulness of our data will expand.</p>
<p>We added these links to existing data, which was a costly process. We first developed an XSLT script to extract the place, time, and object values into a spreadsheet. We then manually found and added the links, in the process having to submit new geographical and chronological values for inclusion in the other authorities (and then, in return, adding the new id numbers). We then used another XSLT script to insert the new links into the XML files. Along the way there was a great deal of checking and testing. For new data files, the additions are added at the time of creation.</p>
<p>One of our active projects involves the lexicographical tagging of the texts in a way that could similarly be linked and shared. This entails performing word segmentation on the existing XML files, and then assigning part of speech tags using natural language parsing. This will enable new interface features and better forms of analysis. The Global Philology Project is an exploratory project that began to lay the infrastructure for compiling and analyzing lexicographical data in many different languages across multiple sites (Global Philology). We want to further explore how our tagging of individual words could make our data – on the level of the individual words of our texts – accessible and more useful to researchers in different fields.</p>
<p>For a broader description of IIP and our goals, see Satlow (forthcoming) and Lembi (forthcoming). We describe our approach to bibliographical management at Lembi, Mylonas and Satlow (2016) and our approach to the FAIR principles (FAIR) in Mylonas, Lembi, Creamer, and Satlow.</p>
<p><strong>References</strong></p>
<p>BAM: The Big Ancient Mediterranean: https://dsps.lib.uiowa.edu/bam/2016/05/10/linking-the-big-ancient-mediterranean-conference-june-6-8-2016/</p>
<p>Elliott, Tom, Gabriel Bodard, Hugh Cayless <em>et al.</em> (2006-2019). <em>EpiDoc: Epigraphic Documents in TEI XML</em>: http://epidoc.sf.net<strong>.</strong></p>
<p>Global Philology Project: https://www.dh.uni-leipzig.de/wo/projects/global-philology-project/</p>
<p>Lembi, Gaia, forthcoming, “Inscriptions of Israel/Palestine,” in Sarah Schulthess, ed., <em>Digital Biblical Studies</em></p>
<p>Lembi, Gaia, Elli Mylonas, Michael Satlow, 2016. “Bibliography in the Inscriptions of Israel/Palestine”: https://tei-c.org/Vault/MembersMeetings/2016/sites/default/files/Gaia%20Lembi,%20Elli%20Mylonas,%20Michael%20Satlow%20Bibliography%20in%20the%20Inscriptions%20of%20Israel%20Palestine%20Epigraphic%20Project.pdf</p>
<p>Mylonas, Elli, Gaia Lembi, Andrew Creamer, Michael Satlow, forthcoming. “Archiving a TEI Project FAIRly,” in TEI Conference Volume.</p>
<p>Pelagios Network: https://www.pelagios.org</p>
<p>Getty Art and Architecture Thesaurus Online: https://www.getty.edu/research/tools/vocabularies/aat/</p>
<p>Periodo: A gazetteer of period definitions for linking and visualizing data. https://perio.do.</p>
<p>Pleiades: https://pleiades.stoa.org.</p>
<p>Satlow, Michael L, forthcoming, “Inscriptions of Israel/Palestine,” <em>Jewish Studies Quarterly</em>.</p>
","The “Inscriptions of Israel/Palestine” (IIP) project (www.brown.edu/iip) seeks to create a corpus of inscriptions (texts written on durable materials, other than coins) from the geographical location of present-day Israel/Palestine, that date from around the sixth century BCE to the seventh century CE. The inscriptions are in Greek, Latin, Hebrew, and Aramaic. The purpose of the project is not only to allow for access and robust (and ultimately, federated) searching but also for scholarly analyses. As one of the longest running active digital epigraphy projects (with over 4,000 inscriptions entered to date), IIP provides several use cases of working with a complex and challenging multi-lingual corpus. This abstract will focus on our data modeling and approach to Linked Open Data.
IIP was an early adopter of the Epidoc schema, a customization of TEI developed especially for those working with ancient texts preserved on durable materials, such as inscriptions and coins (Elliott, Bodard, Cayless). Many users of Epidoc are in contact with each other through epigraphy.info, and the schema is continually being modified in response to user requests. The general principle, however, is that each material object on which a text is inscribed or written is treated as a discrete XML file. IIP thus gives each inscription a unique, findable ID that also serves as its document name (e.g., ash0001.xml). Each file has an extensive teiHeader, in which we encode the metadata, where possible using controlled vocabularies as attributes of elements, linked to authority files. This kind of robust encoding allows for the database-style, faceted indexing and searching powered by SOLR that we provide through our interface. We also include images (although we have much more work to do collecting them) and geographical information. We have a geographical interface that allows for mapping (see Figure 1).
Figure 1: Screenshot of Search Page of ""Inscriptions of Israel/Palestine""
Our data has always been open. All of our XML files can be seen and downloaded individually directly from our site, or downloaded in bulk from our open Github site (https://github.com/Brown-University-Library/iip-texts) or through our API (for which we give detailed instructions on the site itself). We also encode our permission license (CC BY-NC 4.0) into our files.
Since participating in the conference, “The Big Ancient Mediterranean” (BAM) we have sought to create links in our data in three ways. The primary geographical data within each inscription is linked to its corresponding Pleiades Gazetteer id (Pleiades). The primary chronological data within each inscription is linked to its corresponding PeriodO id (PeriodO).   And each of the types of object upon which the inscription is written is linked to the Getty Art and Architecture Thesaurus (Getty). For example:
                        <origin>
                     <date period=""http://n2t.net/ark:/99152/p0m63njbxb9""
                           notBefore=""0001""
                           notAfter=""0100"">First century CE</date>
                     <placeName>
                                <region>Judaea</region>
                                <settlement ref = ""http://pleiades.stoa.org/places/687928""> Jerusalem</settlement>
                                <geogName type=""site"">Akeldama Caves</geogName>
                                <geogFeat type=""locus"">Cave 2 Chamber B</geogFeat>
                            </placeName>
                          
                            <p/>
                        </origin>
The use of these ids allows our data to be scraped live by different projects, such as Pleiades and the Pelagios Network (Pelagios Network). The community is just now beginning to develop an ecosystem that allows for the fruitful exchange of data between sites using Linked Open Data and we hope that through this expansion the usefulness of our data will expand.
We added these links to existing data, which was a costly process. We first developed an XSLT script to extract the place, time, and object values into a spreadsheet. We then manually found and added the links, in the process having to submit new geographical and chronological values for inclusion in the other authorities (and then, in return, adding the new id numbers). We then used another XSLT script to insert the new links into the XML files. Along the way there was a great deal of checking and testing. For new data files, the additions are added at the time of creation.
One of our active projects involves the lexicographical tagging of the texts in a way that could similarly be linked and shared. This entails performing word segmentation on the existing XML files, and then assigning part of speech tags using natural language parsing. This will enable new interface features and better forms of analysis. The Global Philology Project is an exploratory project that began to lay the infrastructure for compiling and analyzing lexicographical data in many different languages across multiple sites (Global Philology). We want to further explore how our tagging of individual words could make our data – on the level of the individual words of our texts – accessible and more useful to researchers in different fields.
For a broader description of IIP and our goals, see Satlow (forthcoming) and Lembi (forthcoming). We describe our approach to bibliographical management at Lembi, Mylonas and Satlow (2016) and our approach to the FAIR principles (FAIR) in Mylonas, Lembi, Creamer, and Satlow.
References
BAM: The Big Ancient Mediterranean: https://dsps.lib.uiowa.edu/bam/2016/05/10/linking-the-big-ancient-mediterranean-conference-june-6-8-2016/
Elliott, Tom, Gabriel Bodard, Hugh Cayless et al. (2006-2019). EpiDoc: Epigraphic Documents in TEI XML: http://epidoc.sf.net.
Global Philology Project: https://www.dh.uni-leipzig.de/wo/projects/global-philology-project/
Lembi, Gaia, forthcoming, “Inscriptions of Israel/Palestine,” in Sarah Schulthess, ed., Digital Biblical Studies
Lembi, Gaia, Elli Mylonas, Michael Satlow, 2016. “Bibliography in the Inscriptions of Israel/Palestine”: https://tei-c.org/Vault/MembersMeetings/2016/sites/default/files/Gaia%20Lembi,%20Elli%20Mylonas,%20Michael%20Satlow%20Bibliography%20in%20the%20Inscriptions%20of%20Israel%20Palestine%20Epigraphic%20Project.pdf
Mylonas, Elli, Gaia Lembi, Andrew Creamer, Michael Satlow, forthcoming. “Archiving a TEI Project FAIRly,” in TEI Conference Volume.
Pelagios Network: https://www.pelagios.org
Getty Art and Architecture Thesaurus Online: https://www.getty.edu/research/tools/vocabularies/aat/
Periodo: A gazetteer of period definitions for linking and visualizing data. https://perio.do.
Pleiades: https://pleiades.stoa.org.
Satlow, Michael L, forthcoming, “Inscriptions of Israel/Palestine,” Jewish Studies Quarterly.","michael_satlow@brown.edu, elli_mylonas@brown.edu",Short Presentation
"Siemens, Lynne",University of Victoria,University-Industry Partnerships in the Humanities: View from the partner and academic perspective,"collaboration, academic teams, project management","English
North America
Contemporary
project design, organization, management
Humanities computing",English,North America,Contemporary,"project design, organization, management",Humanities computing,"<p>While university-industry partnerships are common in the sciences, they are more rare on the humanities side. This provides an opportunity to explore one such collaboration based in the humanities. In this case, exploring open scholarship, the partnership involves libraries, academic-adjacent organizations and academics. Through a series of interviews, these parties express that they find the experience positive and see both benefits and challenges, albeit from different perspectives. For example, libraries and academic-adjacent organizations focus on learning while the academics are interested in moving research to production. The challenges include a focus on cultural differences and the partners' ability to navigate these. In terms of measures of success and desired outcomes, they are both in agreement that these measures and outcomes are soft in nature, though focused on influencing government policy on social scholarship. The partnership members will continue to invest time, resources, and intellectual capacity to the endeavor.</p>
","While university-industry partnerships are common in the sciences, they are more rare on the humanities side. This provides an opportunity to explore one such collaboration based in the humanities. In this case, exploring open scholarship, the partnership involves libraries, academic-adjacent organizations and academics. Through a series of interviews, these parties express that they find the experience positive and see both benefits and challenges, albeit from different perspectives. For example, libraries and academic-adjacent organizations focus on learning while the academics are interested in moving research to production. The challenges include a focus on cultural differences and the partners' ability to navigate these. In terms of measures of success and desired outcomes, they are both in agreement that these measures and outcomes are soft in nature, though focused on influencing government policy on social scholarship. The partnership members will continue to invest time, resources, and intellectual capacity to the endeavor.",siemensl@uvic.ca,Lightning
"Houston, Natalie M. (1);
Plecháč, Petr (2);
Ruiz Fabo, Pablo (3);
Bermúdez Sabel, Helena (4);
Birnbaum, David (5);
Thorsen, Elise (6)","1: University of Massachusetts Lowell;
2: Institute of Czech Literature, Czech Academy of Sciences;
3: LiLPa, Université de Strasbourg;
4: Université de Lausanne;
5: University of Pittsburgh;
6: Novetta",Understanding Rhyme Through Network Analysis,"rhyme, network analysis, machine learning, stylometry, intertextuality","South America
Europe
English
North America
15th-17th Century
19th Century
20th Century
network analysis and graphs theory and application
text mining and analysis
Linguistics
Literary studies",English,"South America
Europe
North America","15th-17th Century
19th Century
20th Century","network analysis and graphs theory and application
text mining and analysis","Linguistics
Literary studies","<p>This panel brings together different approaches to using network analysis to understand the relationships that rhyme enacts in poetry in English, Czech, Russian, and Spanish. The papers on this panel use different network analysis and graph visualization methods to examine rhyme at the level of the corpus or dataset, rather than the individual poem, in order to understand how rhyme practice changes over time, across languages, and in relation to literary canon formation. This panel thus contributes both to computational poetics and distant reading methodologies within the digital humanities.</p>
","This panel brings together different approaches to using network analysis to understand the relationships that rhyme enacts in poetry in English, Czech, Russian, and Spanish. The papers on this panel use different network analysis and graph visualization methods to examine rhyme at the level of the corpus or dataset, rather than the individual poem, in order to understand how rhyme practice changes over time, across languages, and in relation to literary canon formation. This panel thus contributes both to computational poetics and distant reading methodologies within the digital humanities.","Natalie_Houston@uml.edu, plechac@ucl.cas.cz, ruizfabo@unistra.fr, helena.bermudezsabel@unil.ch, djbpitt@gmail.com, enthorsen@gmail.com",Panel
"Wolff, Mark","Hartwick College, United States of America",Computation and Rhetorical Invention: Finding Things To Say With word2vec,"rhetoric, invention, word2vec, machine learning","Europe
English
North America
Contemporary
artificial intelligence and machine learning
electronic literature production and analysis
Literacy, composition, and creative writing
Literary studies",English,"Europe
North America",Contemporary,"artificial intelligence and machine learning
electronic literature production and analysis","Literacy, composition, and creative writing
Literary studies","<p>In his recent book <em>Friending the Past</em>, Alan Liu laments the waning of a rhetorical regime that until recently had held sway in literary studies as a means of making sense of the past. Instead of using what Liu names as <em>rhetoric-representation-interpretation</em> to convey an understanding of history, we are now stuck in an ambiguous regime of <em>communication-information-media</em> where it is not clear how we reach an understanding of anything (2-3). The shift from <em>rhetoric-representation-interpretation</em> to <em>communication-information-media</em> is not unique in the history of literary studies, however. It follows another shift that occurred over 100 years ago in how rhetoric was deployed. Gérard Genette observed in 1966 that literary studies had not always emphasized representations. Before the end of the nineteenth century, literary studies revolved around the art of writing. Texts were not objects to interpret but models to imitate: students demonstrated their understanding of literature by mastering elocution and reproducing figures of style in the works they read. With the institution of literary history as a nationalist project at the end of the nineteenth century, academic reading approached texts as objects to be explained according to prescribed methods for documenting how literature represented a national identity. This new way of studying literature stressed disposition, or the arrangement of ideas in the service of ideology.</p>
<p>The methods of literary history would eventually be used by literary scholars in the twentieth century to turn narratives about literature away from nationalism toward other priorities, most notably poststructuralism and the critique of cultural hegemonies.</p>
<p>Recent developments in information technology have further challenged paradigms for reading literature. Digital tools for text analysis allow for the study of large corpora using quantitative methods. As Ted Underwood, Andrew Piper and others have shown, large-scale computational text analysis has called into question fundamental concepts in literary history such as periodization, nationality, and genre. Using computational methods can enable us to develop models for literary studies, but these models are not limited to interpretation. Computational techniques such as topic modeling and word vector spaces can facilitate investigations into the possibilities for literary creation. Technology has the potential for exploring invention, or the finding of ideas to express through language given a context that can be parameterized.</p>
<p>If, in literary studies, an emphasis on elocution or style served the perpetuation of social hierarchies, and if an emphasis on disposition or argumentation challenged these hierarchies by promoting forms of knowledge and ideologies, a new rhetorical emphasis is needed to respond to the ontological condition of the <em>communication-information-media</em> era. We are surrounded by data with no clear way to make sense of it, and we need to explore inventional methods of finding things to say within this state of being. Digital environments today constitute in part the material context for suasive activity, and as Thomas Rickert argues, contemporary rhetoric must attend to how humans and the world <em>are</em> in this context (xv). The affordances of networked access to texts and computational processing contribute to a rhetorical ambience that grants a degree of agency to the environment in what is said about the world, which includes literature.</p>
<p>I will consider two examples of how tools for computational literary studies lend themselves to inventional practices. The first is <em>ReRites</em>, a year-long project by David (Jhave) Johnston who used a neural net trained on various corpora to produce poetry (“Why A.I.?”, 172). The raw text of the poetry was generated by computation but Johnston edited the output. In terms that emphasize the materiality of computation, Johnston describes his role as “carving the text.” Neither the computer nor Johnston writes these poems in the sense we usually give to writing: they emerge from the world in which a machine and a human find themselves. After performing complex analyses on very large corpora, the machine produces something the human takes to find something to say with language.</p>
<p>The second is <em>SonGenApp</em>, a web application I developed that enables a user to select verses from a large corpus of sonnets to assemble a new poem. From all the verses in the corpus a word embedding is modeled with <em>word2vec</em>, and from the model the user selects verses semantically with an analogy based on a pair of words. The user can modify a selected verse as long as it follows the rules of scansion and rhyme for sonnets. With the application attending to formal constraints, the task of the user is to find verses that are meaningful in some way at the moment of using the application. The user can always read the source texts for selected verses and base the construction of the generated poem on a knowledge of literary themes and history. But this prior knowledge is not necessary. The user can encounter verses in the corpus by changing the analogy as if it were a knob on a black box.</p>
<p>The quantity of digital texts at our disposal opens possibilities for discovery in rhetorical invention. Stephen Ramsay has described a “hermeneutics of screwing around” where browsing resources leads serendipitously to the pleasure of finding things one had not anticipated. Computation has the potential to afford the same discovery in finding things to express through writing.</p>
<p><strong>Works Cited</strong></p>
<p>Genette, Gérard. “Rhétorique et Enseignement.” <em>Figures II</em>, Editions du Seuil, 1969, pp. 23–42.</p>
<p>Johnston, David (Jhave). <em>ReRites: Machine Learning Poetry Edited by a Human.</em> http://glia.ca/rerites/. Accessed 8 Oct. 2019.</p>
<p>——. “Why A.I.?” <em>ReRites : Human + A.I. Poetry ; Raw Output : A.I. Trained on Custom Poetry Corpus ; Responses : 8 Essays about Poetry and A.I.</em>, edited by Stephanie Strickland. Anteism Books, 2019, pp. 171–76.</p>
<p>Liu, Alan. <em>Friending the Past : The Sense of History in the Digital Age.</em> University of Chicago Press, 2018.</p>
<p>Piper, Andrew. <em>Enumerations: Data and Literary Study</em>. U of Chicago P, 2018.</p>
<p>Ramsay, Stephen. “The Hermeneutics of Screwing Around; or What You Do with a Million Books.” <em>Pastplay: Teaching and Learning History with Technology</em>, edited by Kevin Kee, University of Michigan Press, 2014, pp. 111–20.</p>
<p>Rickert, Thomas. <em>Ambient Rhetoric: The Attunements of Rhetorical Being</em>. U of Pittsburgh P, 2013.</p>
<p>Underwood, Ted. <em>Distant Horizons: Digital Evidence and Literary Change</em>. U of Chicago P, 2019.</p>
<p>Wolff, Mark. <em>mbwolff/SonGenApp</em>. 2019. GitHub, https://github.com/mbwolff/SonGenApp.</p>
","In his recent book Friending the Past, Alan Liu laments the waning of a rhetorical regime that until recently had held sway in literary studies as a means of making sense of the past. Instead of using what Liu names as rhetoric-representation-interpretation to convey an understanding of history, we are now stuck in an ambiguous regime of communication-information-media where it is not clear how we reach an understanding of anything (2-3). The shift from rhetoric-representation-interpretation to communication-information-media is not unique in the history of literary studies, however. It follows another shift that occurred over 100 years ago in how rhetoric was deployed. Gérard Genette observed in 1966 that literary studies had not always emphasized representations. Before the end of the nineteenth century, literary studies revolved around the art of writing. Texts were not objects to interpret but models to imitate: students demonstrated their understanding of literature by mastering elocution and reproducing figures of style in the works they read. With the institution of literary history as a nationalist project at the end of the nineteenth century, academic reading approached texts as objects to be explained according to prescribed methods for documenting how literature represented a national identity. This new way of studying literature stressed disposition, or the arrangement of ideas in the service of ideology.
The methods of literary history would eventually be used by literary scholars in the twentieth century to turn narratives about literature away from nationalism toward other priorities, most notably poststructuralism and the critique of cultural hegemonies.
Recent developments in information technology have further challenged paradigms for reading literature. Digital tools for text analysis allow for the study of large corpora using quantitative methods. As Ted Underwood, Andrew Piper and others have shown, large-scale computational text analysis has called into question fundamental concepts in literary history such as periodization, nationality, and genre. Using computational methods can enable us to develop models for literary studies, but these models are not limited to interpretation. Computational techniques such as topic modeling and word vector spaces can facilitate investigations into the possibilities for literary creation. Technology has the potential for exploring invention, or the finding of ideas to express through language given a context that can be parameterized.
If, in literary studies, an emphasis on elocution or style served the perpetuation of social hierarchies, and if an emphasis on disposition or argumentation challenged these hierarchies by promoting forms of knowledge and ideologies, a new rhetorical emphasis is needed to respond to the ontological condition of the communication-information-media era. We are surrounded by data with no clear way to make sense of it, and we need to explore inventional methods of finding things to say within this state of being. Digital environments today constitute in part the material context for suasive activity, and as Thomas Rickert argues, contemporary rhetoric must attend to how humans and the world are in this context (xv). The affordances of networked access to texts and computational processing contribute to a rhetorical ambience that grants a degree of agency to the environment in what is said about the world, which includes literature.
I will consider two examples of how tools for computational literary studies lend themselves to inventional practices. The first is ReRites, a year-long project by David (Jhave) Johnston who used a neural net trained on various corpora to produce poetry (“Why A.I.?”, 172). The raw text of the poetry was generated by computation but Johnston edited the output. In terms that emphasize the materiality of computation, Johnston describes his role as “carving the text.” Neither the computer nor Johnston writes these poems in the sense we usually give to writing: they emerge from the world in which a machine and a human find themselves. After performing complex analyses on very large corpora, the machine produces something the human takes to find something to say with language.
The second is SonGenApp, a web application I developed that enables a user to select verses from a large corpus of sonnets to assemble a new poem. From all the verses in the corpus a word embedding is modeled with word2vec, and from the model the user selects verses semantically with an analogy based on a pair of words. The user can modify a selected verse as long as it follows the rules of scansion and rhyme for sonnets. With the application attending to formal constraints, the task of the user is to find verses that are meaningful in some way at the moment of using the application. The user can always read the source texts for selected verses and base the construction of the generated poem on a knowledge of literary themes and history. But this prior knowledge is not necessary. The user can encounter verses in the corpus by changing the analogy as if it were a knob on a black box.
The quantity of digital texts at our disposal opens possibilities for discovery in rhetorical invention. Stephen Ramsay has described a “hermeneutics of screwing around” where browsing resources leads serendipitously to the pleasure of finding things one had not anticipated. Computation has the potential to afford the same discovery in finding things to express through writing.
Works Cited
Genette, Gérard. “Rhétorique et Enseignement.” Figures II, Editions du Seuil, 1969, pp. 23–42.
Johnston, David (Jhave). ReRites: Machine Learning Poetry Edited by a Human. http://glia.ca/rerites/. Accessed 8 Oct. 2019.
——. “Why A.I.?” ReRites : Human + A.I. Poetry ; Raw Output : A.I. Trained on Custom Poetry Corpus ; Responses : 8 Essays about Poetry and A.I., edited by Stephanie Strickland. Anteism Books, 2019, pp. 171–76.
Liu, Alan. Friending the Past : The Sense of History in the Digital Age. University of Chicago Press, 2018.
Piper, Andrew. Enumerations: Data and Literary Study. U of Chicago P, 2018.
Ramsay, Stephen. “The Hermeneutics of Screwing Around; or What You Do with a Million Books.” Pastplay: Teaching and Learning History with Technology, edited by Kevin Kee, University of Michigan Press, 2014, pp. 111–20.
Rickert, Thomas. Ambient Rhetoric: The Attunements of Rhetorical Being. U of Pittsburgh P, 2013.
Underwood, Ted. Distant Horizons: Digital Evidence and Literary Change. U of Chicago P, 2019.
Wolff, Mark. mbwolff/SonGenApp. 2019. GitHub, https://github.com/mbwolff/SonGenApp.",wolff.mark.b@gmail.com,Lightning
"Swanstrom, Elizabeth Anne",University of Utah,Coding Literary Ecologies,"environmental humanities, digital humanities, literary studies, natural language processing, media ecologies","English
North America
19th Century
20th Century
Contemporary
eco-criticism and environmental analysis
natural language processing
Literary studies
Environmental, ocean, and waterway studies",English,North America,"19th Century
20th Century
Contemporary","eco-criticism and environmental analysis
natural language processing","Literary studies
Environmental, ocean, and waterway studies","<p>Literary scholarship tends to treat the natural world as the stage upon which narrative unfolds, rather than the essential pre-condition for existence. “Coding Literary Ecologies” brings the environmental features of literary texts into detailed relief in a playful yet strategic manner. It is comprised of four web-based applications, each accompanied by an essay that situates it within literary history and contextualizes it within contemporary discourse about the digital and environmental humanities.<strong></strong></p>
","Literary scholarship tends to treat the natural world as the stage upon which narrative unfolds, rather than the essential pre-condition for existence. “Coding Literary Ecologies” brings the environmental features of literary texts into detailed relief in a playful yet strategic manner. It is comprised of four web-based applications, each accompanied by an essay that situates it within literary history and contextualizes it within contemporary discourse about the digital and environmental humanities.",swanstro@gmail.com,Lightning
"van Wissen, Leon (1);
Latronico, Chiara (1);
van Ginhoven, Sandra (2);
Zamborlini, Veruska (1)","1: University of Amsterdam;
2: Getty Research Institute",The Montias Case: an experiment with data reconciliation and provenance between research and cultural heritage institutions,"Reconciliation and Disambiguation of Linked Open Data, Provenance, Named Entity Recognition in Archival Sources, Cooperation public and research institutions, Dutch Golden Age","Europe
English
North America
15th-17th Century
18th Century
digital archiving
linked (open) data
Art history
History",English,"Europe
North America","15th-17th Century
18th Century","digital archiving
linked (open) data","Art history
History","<p dir=""ltr"">This paper discusses the complex process of reconciliation of data coming from research and public cultural heritage institutions with their own selection criteria that have shaped the provenance of their collections. We demonstrate how the Golden Agents digital humanities research infrastructure [1] can play an intersecting role as intermediary data provider between these distributed collections in the reconciliation, disambiguation and deduplication of data by taking their provenance into account. To this end we analyse an art historical case we call ‘The Montias Case’, with data from three different sources: the Getty Provenance Index, [2] the Frick Collection, [3] and the notarial acts of the Amsterdam City Archives. [4]</p>
<p dir=""ltr"">In the 1980s, John Michael Montias (1928-2005) began to compile a database containing records of ownership of works of art of the Dutch Golden Age. To this end he also selected records from the Amsterdam City Archives using inventories dated 1597-1681. In 1985 and 1987, Montias was invited to the Getty Research Institute to automate his work and incorporate it into the Getty Provenance Index. When his collaboration with the Getty concluded around 1990, Montias continued his project and donated his work to the Frick Art Reference Library. Both datasets were enriched by others. The Getty Provenance Index and the Frick Collection have now two partially overlapping datasets with a shared provenance that are in need of deduplication/disambiguation to fully exploit them for art-historical research. </p>
<p dir=""ltr"">The Getty Research Institute and the Golden Agents consortium set up an experiment in 2019 to reconcile the Montias data with their provenance and enrichments to their original source: the notarial acts. This reconciliation via the Golden Agents infrastructure is necessary because the Amsterdam City Archives due to their public mission only index person and geographical names, but not the art objects that Montias listed that we need as researchers.</p>
<p dir=""ltr"">We first link the inventories from the Getty Provenance Index and the Frick Collection to their corresponding notarial deeds. Additionally, we deduplicate the names indexed in these data sources and identify people utilizing the Lenticular Lenses II tool (Idrissou et al., 2018, 2019) in development within the Golden Agents project. Then we link the individual items indexed in the Montias database to the transcription of the historical archival source. The Web Annotation Model is used as a data model for the text and serves both modelling and presentational purposes. This allows us to link and to validate the exact piece of text indicated by coordinates on the scan where a certain object is listed and to present it for instance in a IIIF-environment. The thesauri AAT and TGN, [5] and ICONCLASS [6] help in describing these items in a standardized way. Finally, we demonstrate how, in line with the goals of the Golden Agents and the remodel of the Getty Provenance Index as Linked Open Data projects, [7] the results of the Montias Case can be relevant for data reconciliation and for modeling the provenance of distributed heterogeneous cultural heritage collections for (re-)use in digital humanities research projects in general.</p>
<p dir=""ltr"">[1] https://www.goldenagents.org</p>
<p dir=""ltr"">[2] https://www.getty.edu/research/tools/provenance/search.html</p>
<p dir=""ltr"">[3] https://research.frick.org/montias/</p>
<p dir=""ltr"">[4] http://archief.amsterdam/archief/5075/</p>
<p dir=""ltr"">[5] https://www.getty.edu/research/tools/vocabularies/</p>
<p dir=""ltr"">[6] http://www.iconclass.org/</p>
<p dir=""ltr"">[7] https://www.getty.edu/research/tools/provenance/provenance_remodel/index.html</p>
<p><strong>References</strong></p>
<p><strong>Bibliography</strong></p>
<ul><li>Al Idrissou, Veruska Zamborlini, Chiara Latronico, Frank van Harmelen, Charles van den Heuvel. Amsterdamers from the Golden Age to the Information Age via Lenticular Lenses. DHBenelux 2018, 6-8 June. Amsterdam, The Netherlands.</li>
<li>Al Idrissou, Frank van Harmelen, Veruska Carretta Zamborlini, Chiara Latronico. Contextual Entity Disambiguation in Domains with Weak Identity Criteria: Disambiguating Golden Age Amsterdamers. K-CAP 2019, ACM, New York, USA (forthcoming).</li>
</ul><p><strong>Web</strong></p>
<ul><li>Art & Architecture Thesaurus (AAT) and Getty Thesaurus of Geographic Names (TGN), https://www.getty.edu/research/tools/vocabularies/ .</li>
<li>Frick Collection, https://research.frick.org/montias/ .</li>
<li>Getty Provenance Index,  https://www.getty.edu/research/tools/provenance/search.html .</li>
<li>Getty Provenance Index Remodel Project,  https://www.getty.edu/research/tools/provenance/provenance_remodel/index.html .</li>
<li>Golden Agents, https://www.goldenagents.org .</li>
<li>Iconclass, http://www.iconclass.org .</li>
<li>Notarial Acts of the Amsterdam City Archives, http://archief.amsterdam/archief/5075/ .</li>
</ul>","This paper discusses the complex process of reconciliation of data coming from research and public cultural heritage institutions with their own selection criteria that have shaped the provenance of their collections. We demonstrate how the Golden Agents digital humanities research infrastructure [1] can play an intersecting role as intermediary data provider between these distributed collections in the reconciliation, disambiguation and deduplication of data by taking their provenance into account. To this end we analyse an art historical case we call ‘The Montias Case’, with data from three different sources: the Getty Provenance Index, [2] the Frick Collection, [3] and the notarial acts of the Amsterdam City Archives. [4]
In the 1980s, John Michael Montias (1928-2005) began to compile a database containing records of ownership of works of art of the Dutch Golden Age. To this end he also selected records from the Amsterdam City Archives using inventories dated 1597-1681. In 1985 and 1987, Montias was invited to the Getty Research Institute to automate his work and incorporate it into the Getty Provenance Index. When his collaboration with the Getty concluded around 1990, Montias continued his project and donated his work to the Frick Art Reference Library. Both datasets were enriched by others. The Getty Provenance Index and the Frick Collection have now two partially overlapping datasets with a shared provenance that are in need of deduplication/disambiguation to fully exploit them for art-historical research. 
The Getty Research Institute and the Golden Agents consortium set up an experiment in 2019 to reconcile the Montias data with their provenance and enrichments to their original source: the notarial acts. This reconciliation via the Golden Agents infrastructure is necessary because the Amsterdam City Archives due to their public mission only index person and geographical names, but not the art objects that Montias listed that we need as researchers.
We first link the inventories from the Getty Provenance Index and the Frick Collection to their corresponding notarial deeds. Additionally, we deduplicate the names indexed in these data sources and identify people utilizing the Lenticular Lenses II tool (Idrissou et al., 2018, 2019) in development within the Golden Agents project. Then we link the individual items indexed in the Montias database to the transcription of the historical archival source. The Web Annotation Model is used as a data model for the text and serves both modelling and presentational purposes. This allows us to link and to validate the exact piece of text indicated by coordinates on the scan where a certain object is listed and to present it for instance in a IIIF-environment. The thesauri AAT and TGN, [5] and ICONCLASS [6] help in describing these items in a standardized way. Finally, we demonstrate how, in line with the goals of the Golden Agents and the remodel of the Getty Provenance Index as Linked Open Data projects, [7] the results of the Montias Case can be relevant for data reconciliation and for modeling the provenance of distributed heterogeneous cultural heritage collections for (re-)use in digital humanities research projects in general.
[1] https://www.goldenagents.org
[2] https://www.getty.edu/research/tools/provenance/search.html
[3] https://research.frick.org/montias/
[4] http://archief.amsterdam/archief/5075/
[5] https://www.getty.edu/research/tools/vocabularies/
[6] http://www.iconclass.org/
[7] https://www.getty.edu/research/tools/provenance/provenance_remodel/index.html
References
Bibliography
Al Idrissou, Veruska Zamborlini, Chiara Latronico, Frank van Harmelen, Charles van den Heuvel. Amsterdamers from the Golden Age to the Information Age via Lenticular Lenses. DHBenelux 2018, 6-8 June. Amsterdam, The Netherlands.
Al Idrissou, Frank van Harmelen, Veruska Carretta Zamborlini, Chiara Latronico. Contextual Entity Disambiguation in Domains with Weak Identity Criteria: Disambiguating Golden Age Amsterdamers. K-CAP 2019, ACM, New York, USA (forthcoming).
Web
Art & Architecture Thesaurus (AAT) and Getty Thesaurus of Geographic Names (TGN), https://www.getty.edu/research/tools/vocabularies/ .
Frick Collection, https://research.frick.org/montias/ .
Getty Provenance Index, https://www.getty.edu/research/tools/provenance/search.html .
Getty Provenance Index Remodel Project, https://www.getty.edu/research/tools/provenance/provenance_remodel/index.html .
Golden Agents, https://www.goldenagents.org .
Iconclass, http://www.iconclass.org .
Notarial Acts of the Amsterdam City Archives, http://archief.amsterdam/archief/5075/ .","l.vanwissen@uva.nl, c.latronico@uva.nl, svanginhoven@getty.edu, v.carrettazamborlini@uva.nl",Lightning
"Antonini, Alessio;
Benatti, Francesca;
King, Edmund","The Open University, United Kingdom",Restoration and Repurposing of DH Legacy Projects,"Information Systems, Legacy Systems","Global
English
Contemporary
meta-criticism (reflections on digital humanities and humanities computing)
sustainable procedures, systems, and methods
Informatics",English,Global,Contemporary,"meta-criticism (reflections on digital humanities and humanities computing)
sustainable procedures, systems, and methods",Informatics,"<p dir=""ltr"">The institutional funding system of Digital Humanities (DH) is usually devoted to the creation of new projects, creating a recurring problem of unsupported legacy projects whose material cost of upkeep depends on the voluntary contributions of institutions and individuals. The lack of resources to invest in “remedial” actions [10] pushes DH projects towards outdatedness. Additional funding success delays this process by introducing extra resources but, simultaneously, it fast-forwards obsolescence by advancing the field.</p>
<p dir=""ltr"">Indeed, the impact of a DH project can be considered as the ability to establish as common knowledge what was once innovative and cutting-edge by fulfilling its research questions. In this scenario, managing successful DH projects requires addressing competing issues related to the preservation of their integrity [1] (i.e. consistency of data, questions and vision) and of their role and purpose (i.e. their use in the field).</p>
<p dir=""ltr"">The management of legacy systems has been widely studied from a technical perspective [2,3,4,5,6,7,8], e.g. cost/value [9], approach to integration [1,9], change of use [1] and archiving [3,4]. Rather than presenting technical solutions, this contribution focuses on the rationale for defining an approach through human, financial and political perspectives [2].</p>
<p dir=""ltr"">The issue of legacy is not one of data formats but principally a cultural one that we analyse from two distinct approaches:</p>
<ol><li dir=""ltr"">
<p dir=""ltr""><strong>The “restoration” approach</strong>, implementing remedial actions [10] that “update” the project to new contexts to preserve its function and role (e.g. extending its data structure to address new questions)</p>
</li>
<li dir=""ltr"">
<p dir=""ltr""><strong>The repurposing approach </strong>[1], implementing actions that rethink the value of the project by finding it new purposes, functions and roles in new contexts (e.g. defining new questions to be addressed with its existing data).</p>
</li>
</ol><p dir=""ltr"">At stake in the two approaches are the <strong>integrity</strong> and <strong>identity</strong> of the project. A project’s integrity is the logical and historical connection between its origin, output and outcomes. A project’s identity is the meaning or role it has within the community of people involved. In this scenario, we argue that addressing a project’s legacy should take into account:</p>
<ol><li dir=""ltr"">
<p dir=""ltr""><strong>The project’s vision</strong>, research questions and target “knowledge gap”; the project creators’ motivations and aims; the funding bodies’ goals and priorities</p>
</li>
<li dir=""ltr"">
<p dir=""ltr""><strong>The project’s practices</strong>, orchestration of people, organisations and tools, operational limits and constraints</p>
</li>
<li dir=""ltr"">
<p dir=""ltr""><strong>The project’s knowledge</strong>, research data and outputs, correlated research activities, answers produced and outcomes (e.g. new projects, scholarly research, education, impact on the field).</p>
</li>
</ol><p dir=""ltr"">The contribution then discusses a real case, the UK Reading Experience Database (RED), as emblematic of the challenges of managing legacy DH projects. RED has had a long history, repeated funding successes and significant visibility in Book History scholarly literature. It was devised by Simon Eliot in 1993, first implemented in 1996, published on the web in 2007 and finally closed to new submissions in 2018 [11]. RED’s vision was to advance research in the history of reading by establishing a new methodology based on empirical evidence [12]. RED’s practice established strong synergies between researchers, students and volunteers for the distributed acquisition and curation of evidence of reading. The RED contribution form’s structured approach to knowledge encouraged data inputters to pay close attention to the contexts and agents involved in the reading experience: who was reading, what was read, and when and where the act of reading took place [13].</p>
<p dir=""ltr"">RED data has been successfully converted from a legacy custom relational database to linked open data. Still, RED is a legacy project because of what its data expresses about reading experiences: a now-outdated vision established more than twenty years ago, which has now become embedded in the DH community through successful activities, follow-on projects, publications, research and training initiatives [14,15,16,17]. RED is both a vast database and the centre a wide network of collaborations; therefore addressing its legacy is not a trivial decision.</p>
<p dir=""ltr"">1. The Repurposing of RED: Repurposing RED’s vision means, for instance, rethinking its role from research infrastructure to an educational resource. Consequently, RED’s practices could be reframed as a playground for DH students, providing an environment for training and annotation evaluation. The knowledge produced and encompassed in RED could document the history of DH methods or become a training set for machine-learning algorithms.</p>
<p dir=""ltr"">2. The Restoration of RED. Restoring RED’s vision means, for instance, incorporating in RED new approaches currently required by funding bodies (e.g. collaboration with data science) and current research priorities within Book History, changed considerably since 1993. Consequently, the study of sources could be combined with machine-learning and natural-language processing tools not included in the original structure of RED. Finally, new research questions such as the effects of reading [18] and the multi-modality of reading on new media [19] could be addressed.</p>
<p dir=""ltr"">With a repurposing approach, the integrity of the dataset could be preserved by relinquishing RED’s role as a research project. With a restoration approach, RED’s role as a research project could be preserved through the entire re-curation of its data, the complete re-development of the tool ecosystem to include automatic steps and the entire reassessment of its value as a research resource in light of current DH and Book History research agendas. Unsurprisingly, to keep a project’s role we must face the cost of adapting to the new context, while to keep its form, we must search for a new purpose.</p>
<p dir=""ltr"">On a more general level, there is a question about how to preserve the “human legacy” of RED, e.g. the network of collaborations, student volunteers and contributors engaged. A DH project is a Cultural Artefact, and therefore its historical context can guide the re-tuning of its role as the context changes or the search for new purposes compatible with the values and vision of the social system of the project.</p>
<p dir=""ltr"">As a final remark, we hope these questions can elicit a broader discussion about new and future DH projects and how we could design for their legacy, e.g. the new Reading Europe Advanced Data Investigation Tool.</p>
","The institutional funding system of Digital Humanities (DH) is usually devoted to the creation of new projects, creating a recurring problem of unsupported legacy projects whose material cost of upkeep depends on the voluntary contributions of institutions and individuals. The lack of resources to invest in “remedial” actions [10] pushes DH projects towards outdatedness. Additional funding success delays this process by introducing extra resources but, simultaneously, it fast-forwards obsolescence by advancing the field.
Indeed, the impact of a DH project can be considered as the ability to establish as common knowledge what was once innovative and cutting-edge by fulfilling its research questions. In this scenario, managing successful DH projects requires addressing competing issues related to the preservation of their integrity [1] (i.e. consistency of data, questions and vision) and of their role and purpose (i.e. their use in the field).
The management of legacy systems has been widely studied from a technical perspective [2,3,4,5,6,7,8], e.g. cost/value [9], approach to integration [1,9], change of use [1] and archiving [3,4]. Rather than presenting technical solutions, this contribution focuses on the rationale for defining an approach through human, financial and political perspectives [2].
The issue of legacy is not one of data formats but principally a cultural one that we analyse from two distinct approaches:
The “restoration” approach, implementing remedial actions [10] that “update” the project to new contexts to preserve its function and role (e.g. extending its data structure to address new questions)
The repurposing approach [1], implementing actions that rethink the value of the project by finding it new purposes, functions and roles in new contexts (e.g. defining new questions to be addressed with its existing data).
At stake in the two approaches are the integrity and identity of the project. A project’s integrity is the logical and historical connection between its origin, output and outcomes. A project’s identity is the meaning or role it has within the community of people involved. In this scenario, we argue that addressing a project’s legacy should take into account:
The project’s vision, research questions and target “knowledge gap”; the project creators’ motivations and aims; the funding bodies’ goals and priorities
The project’s practices, orchestration of people, organisations and tools, operational limits and constraints
The project’s knowledge, research data and outputs, correlated research activities, answers produced and outcomes (e.g. new projects, scholarly research, education, impact on the field).
The contribution then discusses a real case, the UK Reading Experience Database (RED), as emblematic of the challenges of managing legacy DH projects. RED has had a long history, repeated funding successes and significant visibility in Book History scholarly literature. It was devised by Simon Eliot in 1993, first implemented in 1996, published on the web in 2007 and finally closed to new submissions in 2018 [11]. RED’s vision was to advance research in the history of reading by establishing a new methodology based on empirical evidence [12]. RED’s practice established strong synergies between researchers, students and volunteers for the distributed acquisition and curation of evidence of reading. The RED contribution form’s structured approach to knowledge encouraged data inputters to pay close attention to the contexts and agents involved in the reading experience: who was reading, what was read, and when and where the act of reading took place [13].
RED data has been successfully converted from a legacy custom relational database to linked open data. Still, RED is a legacy project because of what its data expresses about reading experiences: a now-outdated vision established more than twenty years ago, which has now become embedded in the DH community through successful activities, follow-on projects, publications, research and training initiatives [14,15,16,17]. RED is both a vast database and the centre a wide network of collaborations; therefore addressing its legacy is not a trivial decision.
1. The Repurposing of RED: Repurposing RED’s vision means, for instance, rethinking its role from research infrastructure to an educational resource. Consequently, RED’s practices could be reframed as a playground for DH students, providing an environment for training and annotation evaluation. The knowledge produced and encompassed in RED could document the history of DH methods or become a training set for machine-learning algorithms.
2. The Restoration of RED. Restoring RED’s vision means, for instance, incorporating in RED new approaches currently required by funding bodies (e.g. collaboration with data science) and current research priorities within Book History, changed considerably since 1993. Consequently, the study of sources could be combined with machine-learning and natural-language processing tools not included in the original structure of RED. Finally, new research questions such as the effects of reading [18] and the multi-modality of reading on new media [19] could be addressed.
With a repurposing approach, the integrity of the dataset could be preserved by relinquishing RED’s role as a research project. With a restoration approach, RED’s role as a research project could be preserved through the entire re-curation of its data, the complete re-development of the tool ecosystem to include automatic steps and the entire reassessment of its value as a research resource in light of current DH and Book History research agendas. Unsurprisingly, to keep a project’s role we must face the cost of adapting to the new context, while to keep its form, we must search for a new purpose.
On a more general level, there is a question about how to preserve the “human legacy” of RED, e.g. the network of collaborations, student volunteers and contributors engaged. A DH project is a Cultural Artefact, and therefore its historical context can guide the re-tuning of its role as the context changes or the search for new purposes compatible with the values and vision of the social system of the project.
As a final remark, we hope these questions can elicit a broader discussion about new and future DH projects and how we could design for their legacy, e.g. the new Reading Europe Advanced Data Investigation Tool.","alessio.antonini@open.ac.uk, francesca.benatti@open.ac.uk, edmund.king@open.ac.uk",Long Presentation
"Page, Kevin (1);
Delmas-Glass, Emmanuelle (2);
Beaudet, David (3);
Norling, Samantha (4);
Rother, Lynn (5,6);
Hänsli, Thomas (7,8)","1: Oxford e-Research Centre, University of Oxford, United Kingdom;
2: The Yale Center for British Art, Yale University, USA;
3: The National Gallery of Art, Washington D.C., USA;
4: Indianapolis Museum of Art at Newfields, Indianapolis, USA;
5: Museum of Modern Art, New York, USA;
6: Leuphana Universität, Germany;
7: University of Zurich, Switzerland;
8: ETH Zurich, Switzerland",Linked Art: Networking Digital Collections and Scholarship,"Linked Data, Art, Art History, Art Provenance, Cultural Heritage Institutions","Global
English
18th Century
19th Century
20th Century
data modeling
linked (open) data
Art history
Humanities computing",English,Global,"18th Century
19th Century
20th Century","data modeling
linked (open) data","Art history
Humanities computing","<p><strong><strong>Panel Overview: Linked Art - Networking Digital Collections and Scholarship<br /></strong></strong></p>
<p dir=""ltr"">Linked Art[1] is a major new initiative by which art museums will publish information about their collections as interconnected open data. Building upon long standing interdisciplinary thinking from the digital humanities and information engineering, Linked Art is an international collaboration across twenty-four institutions identifying focussed, practical models which meet their requirements on a sustainable basis.</p>
<p dir=""ltr"">The central aim of Linked Art is the development and application of Linked Data to cultural heritage collections, with an emphasis on works of art and their provenance. Linked Data will provide the foundation for multi-modal digital scholarship across these rich collections; as an open data standard, Linked Art provides consistent, structured ways for arts institutions to publish art-related data where, in many cases, there has not been a consistent shared model to date.</p>
<p dir=""ltr"">This panel presents a range of perspectives representative of the collaborative intersections found in the Linked Art community: with speakers from universities and art museums; who are practitioners and academics; on topics ranging from implementation, to curation, and research.</p>
<p dir=""ltr"">The panel will takes the form of six position papers, outlined below, followed by questions and answers between the audience and panel. In doing so, Linked Art seeks to engage with the Digital Humanities community, building capacity for future collaborative implementations and research investigations.</p>
<p dir=""ltr""></p>
<p dir=""ltr""></p>
<p><strong><strong>1. Linked Data and Open Data in Cultural Heritage</strong></strong></p>
<p dir=""ltr""><em>Emmanuelle Delmas-Glass, The Yale Center for British Art, Yale University, USA.</em></p>
<p dir=""ltr"">Cultural heritage institutions have a great deal to gain from deeply engaging in the networked environment. They have poured many resources in the digitization of their collections for the benefit of their audiences, from students to experts, who want to have access to more online material of a higher quality. The current landscape of cultural heritage knowledge on the Web, however, is very much siloed, which both harms the relevance of individual institutions as well as the overall state of scholarship which might use that knowledge.</p>
<p dir=""ltr"">This paper reflects on the main challenges that cultural heritage institutions face when it comes to publishing their collections descriptions as Linked Open Data resources. Some challenges can be due to friction between a declared digital mission and the resources allocated, which might seem to be in competition with other institutional priorities. Other memory institutions are still in the process of understanding that managing their knowledge and data<em> – </em>so it can then be leveraged for the Semantic Web<em> – </em>needs to be a core data curation activity. It is also partly due to the lack of entry-level technology and ontology resources that has prevented museums from engaging more deeply with Linked Open Data.</p>
<p dir=""ltr"">This talk will give an overview of previous initiatives and technologies intended to open up access to cultural heritage institutions, particularly art museums, including the International Council of Museums Committee for Documentation Conceptual Reference Model (CIDOC CRM, also an ISO standard); The American Art Collaborative (AAC); PHAROS, the International Consortium of Photo Archives; the Art and Architecture Thesaurus (AAT); the Union List of Artist Names (ULAN); and the International Image Interoperability Framework (IIIF).</p>
<p dir=""ltr"">In the context of the successes and limitations of these efforts, this paper will outline the strategy taken by Linked Art, which is both a standard-based data model and a community, which has emerged from earlier work at the Getty Research Institute. This presentation will challenge the traditional paradigm which has large and wealthy institutions succeed in the face of structural challenges. In this era of hyper connectedness, the solution to museums’ relevance in the Web cannot be developed by a lone institution, and indeed the model that Linked Art promotes is instead based on inclusion with the goal to create institutional and individual partnerships. The other precept that the Linked Art data model advocates for is usability over absolute data completeness, and this talk will go over some specific data modeling principles that allow balance between the requirements of the institution, domain knowledge experts, technologists who will implement the standard, and scholars and other users of Linked Art.</p>
<p dir=""ltr""><em>Emmanuelle Delmas-Glass is the Collections Data Manager at the Yale Center for British Art, a member of the IIIF Operating Committee, and co-chair of the Linked Art working group of the International Council of Museums’ Committee for Documentation (ICOM CIDOC). She oversees the creation, access to and distribution of the museum’s collections information and metadata, playing the lead role in ensuring the intellectual and technical integrity of the collections data and metadata.</em></p>
<p><strong><strong>2. Conceptual models meet practice and scholarship: conventions, standards, and technology</strong></strong></p>
<p dir=""ltr""><em>Kevin Page, Oxford e-Research Centre, University of Oxford, UK.</em></p>
<p dir=""ltr"">The utility of a data model is dependent on its usage, and the context in which this use occurs. While conceptual models play an important role in providing a framework within which different data sets can be consistently represented and combined, in practice the resulting information structures can be perceived as complex or unwieldy, which can stifle adoption in cultural heritage institutions.</p>
<p dir=""ltr"">Specialisms in scholarship and practice bring differing requirements and benefits to the structuring of collections data and its analysis, which are similarly defined by use – the operational needs of a collection or library are different from that of an exhibition, or of the intellectual needs of academic study. The challenge, then, is in respecting and encouraging these different ‘information perspectives’, whilst benefiting from their intersections where they occur, and managing the complexity of the systems and organisational implications. In addition, we recognise scholarship within cataloguing and curation activities, whilst appreciating these have different – but complementary – information needs and outputs from academic study of the collections. Digital tools and methods should reflect these activities, roles and specialisms, rather than constrain them. Doing so can ease adoption of digital approaches alongside existing established practice, increasing the sources of compatible structured information, and achieving overall progress through the combination of multiple information intersections.</p>
<p dir=""ltr"">In considering the above, this paper reflects upon more nuanced notions of authority, standards, and how these are realised technologically; moving from necessary ‘on the wire’ interoperability to a progression from local practice, through emergent community conventions, to international standards bodies. It can be beneficial for different stages of maturity to exist simultaneously across distinct but complementary information structures, reflecting the communities of specialist practice and scholarship who are using the data. The technologies of Linked Open Data, including RDF and ontologies, provide a flexible foundation through which we can realise such an iterative and incremental approach to standardisation, and in which alternative information perspectives can co-exist.</p>
<p dir=""ltr"">Linked Art recognises a further first-class perspective: that of the software developer writing (potentially for, or with scholars) applications which consume collection data provided by cultural heritage institutions. It adopts the principles of Linked Open Usable Data, as proposed by Rob Sanderson of the Getty Research Institute, to create a profile of the CIDOC CRM tailored to this information perspective, and in which established practice – for example, in the use of AAT – can be respected.</p>
<p dir=""ltr""><em>Kevin Page is Associate Faculty at the University of Oxford e-Research Centre. He was Technical Director of the Oxford Linked Open Data (OXLOD) project, a prototype for the use of CIDOC CRM across the Gardens, Libraries, and Museums of the University of Oxford. Kevin is Principal Investigator of the Linked Art Research Network and Linked Art II project, both funded by the UK’s Arts and Humanities Research Council, and a member of the Linked Art Editorial Board.</em></p>
<p><strong><strong><br />3. Practicing Linked Art: Evolving Art Data at the National Gallery of Art</strong></strong></p>
<p dir=""ltr""><em>David Beaudet, National Gallery of Art, Washington D.C., USA.</em></p>
<p dir=""ltr"">The National Gallery of Art (NGA) seeks to serve the United States in a national role by preserving, collecting, exhibiting, and fostering the understanding of works of art, at the highest possible museum and scholarly standards. As such, the institution is frequently engaged in activities that require production of the structured data describing our collection, the associated media, and writings related to the collection’s historical significance. These demands for data manifest in a variety of ways. Images are requested en masse, data sets are requested in support of research, and both data and media are increasingly requested in support of on-site and remote visitor experiences as well as for analytical purposes.</p>
<p dir=""ltr"">In recognition of the need to automate the accurate publication of data about art, artists, depictions of art, and associated media, the NGA’s department of Analytics and Enterprise Architecture has been seeking data modeling and dissemination standards that are accepted by the cultural heritage art community in order to ensure the greatest reach possible from its automated art data services. The NGA has selected Linked Art as a strategic data standard.  </p>
<p dir=""ltr"">Standardizing formats and exchanges of data through automated means has paid dividends for many industries in the past and continues to do so. For example, the IIIF standards[2] for image sharing have expanded the reach of deep zoom technologies and image collections across the cultural heritage sector and it is expanding into other communities[3]. Whilst data standards are not in themselves novel, no prevailing broadly adopted standard for modeling art data exists. Linked Art seeks to fill that gap.</p>
<p dir=""ltr"">As part of its participation in the Linked Art community, the NGA is evolving an existing art data interface, one that currently provides collection data and images to its Conservation Space system, to use the Linked Art standards as a proof of concept. This paper will give details of that implementation, alongside the basics of the Linked Art model[4] - for representing artworks, people, and depiction. The evolving, open source[5], public-facing NGA interface already provides art data to a location-aware mobile app[6] available for visitors to download on their iOS devices, which will be demonstrated.</p>
<p dir=""ltr""><em>A solution architect with the National Gallery of Art since 2005, David Beaudet designs and builds technical solutions for authoring and publishing rich art imagery, content, and metadata. David is a member of the editorial board of Linked Art and collaborates on the IIIF Discovery API.</em></p>
<p><strong><strong><br />4. The Linked Art of Georgia O’Keeffe: Collections Across Institutional Boundaries</strong></strong></p>
<p dir=""ltr""><em>Samantha Norling, Indianapolis Museum of Art at Newfields, Indianapolis, USA.</em></p>
<p dir=""ltr"">The creation and publication of linked open data allows for previously siloed data to be connected with other related data on the Web, breaking down institutional barriers and facilitating research and new scholarship – in traditional and digital formats – that may not have previously been possible. With members and community participants representing over 20 different arts-related organizations, the Editorial Board for the developing Linked Art data model for describing art collections reflects this barrier-breaking nature of linked data.</p>
<p dir=""ltr"">In order to showcase the connections that can be made when multiple institutions publish their collections data in a consistent format and utilize shared vocabularies, members of the Linked Art community collaborated to create a cross-institutional sample data set. The artist Georgia O’Keeffe was selected to serve as the common thread for the data to be contributed by participating institutions. With an emphasis on relationships, the linked data collected for the showcase naturally expanded to include not just O’Keeffe and her artworks, but also the works of her contemporaries, the exhibitions in which the artworks were exhibited, and the various organizations and individuals that had participated in provenance events in the lifecycle of the artworks.</p>
<p dir=""ltr"">The development of the Linked Art O’Keeffe data set serendipitously coincided with the launch of the Georgia O’Keeffe Museum’s (GOKM) beta version of the GOKM’s Collections Online, which was built on a linked data foundation. The GOKM expressed the importance of linked data to their digital strategy to make it possible for “meaningful connections to be expressed across different types of collections (artworks, archival items, books, etc.) to establish a more complete understanding of Georgia O’Keeffe’s life, work, and contexts.”</p>
<p dir=""ltr"">Drawing on both the GOKM’s Collections Online[7] and the co-constructed O’Keeffe showcase data set that is now available publicly within the Linked Art GitHub repository[8], this paper and panel presentation will explore the network of Linked Art data with Georgia O’Keeffe at the center. Following intersections between and connections across data sets, the exploration will follow a path through the many relationships identified that link artworks, archives, exhibitions, and people within O’Keefe’s linked data network. In discussing some of the specific relationships identified between institutional collections, the exploration will also highlight key patterns within the Linked Art data model. The Georgia O’Keeffe showcase data set, while small in scale, demonstrates the potential for the Linked Art data model to facilitate the creation of new connections between collections of all types – connections that cross institutional boundaries and facilitate scholarship at the intersections between GLAM collections.</p>
<p dir=""ltr""><em>As Digital Collections Manager at the Indianapolis Museum of Art at Newfields, Samantha Norling manages digital assets and data related to the museum's art, archival, and horticultural collections. A trained librarian and professional archivist working within an art museum, Samantha is particularly interested in digital projects that break down barriers between GLAM institutions. She is a member of the Linked Art Editorial Board.</em></p>
<p dir=""ltr""></p>
<p dir=""ltr""></p>
<p><strong><strong>5. The History of Art is Linked but the Data Is Not: Georgia O’Keeffe, Provenance and Scholarship</strong></strong></p>
<p dir=""ltr""><em>Lynn Rother, Leuphana Universität, Germany.</em></p>
<p dir=""ltr"">The history of artworks is linked. They were produced by the same artists, traded by the same dealers, collected by the same people, transferred, looted or confiscated by the same entities while eventually finding their permanent home in the same museums – or not. To date, these links across museum collections are only visible to the few scholars or experts studying the artworks’ history of ownership. But the field of provenance research has matured enough to enable and support structuring and aggregating provenance records as Linked Data.</p>
<p dir=""ltr"">Though shaped by complex and diverse contexts, an artwork’s provenance record can be broken down into empirical data consisting of objects, protagonists, dates, locations and types of transactions. To this day, however, the majority of museums record the valuable information harvested through time-consuming and resource-intensive provenance research within their collection management systems without machine-readable structure, hindering the analysis and linking of the data across institutions on a larger scale. Digital humanities tools offer the potential to standardize, aggregate, and consider the museum accumulated provenance data broadly to reveal new stories about the global circulation and displacement of artworks and nuance the existing histories of collecting and art market practices.</p>
<p dir=""ltr"">As museum objects and their movements through time and space tell stories beyond object-based art historical research and collection cataloguing, this paper will elaborate on the potential of Linked Art for provenance research and for scholarship in related fields. The intertwined histories of selected works by the American Modern artist Georgia O’Keeffe – from different museum collections including the Georgia O’Keeffe Museum in Santa Fe, New Mexico and The Museum of Modern Art (MoMA) in New York – will serve as an example.</p>
<p dir=""ltr"">In particular, MoMA’s acquisition and deaccession of O’Keeffe’s Kachinas – representations of Pueblo and Hopi spirits used in ceremonies and rituals and therefore considered culturally sensitive objects in museum collections – will show how structured provenance data of museums using Linked Art can benefit related research fields such as the histories of collecting and art market practices but also museum, Native American, and Indigenous studies.</p>
<p dir=""ltr""><em>Lynn Rother is the Lichtenberg-Professor for Provenance Studies at Leuphana Universität, Lüneburg, Germany, and a member of the Linked Art Editorial Board. Previously, Lynn was Senior Provenance Specialist at The Museum of Modern Art, New York, where she oversaw provenance research, procedures, documentation, digital strategies and funding in conjunction with all curatorial departments regarding works in the Collection, loans, acquisitions, and deaccessions.</em></p>
<p dir=""ltr""></p>
<p dir=""ltr""></p>
<p><strong><strong>6. Topographia Helvetiae: Linked Art in Switzerland</strong></strong></p>
<p dir=""ltr""><em>Thomas Hänsli, University of Zurich / ETH Zurich, Switzerland.</em></p>
<p dir=""ltr"">The historical view of Switzerland’s nature has been defined by artworks describing a ‘visual topography’ of the Alpine country long before the emergence of a broader touristic interest for Switzerland across Europe. Paintings, drawings, prints, and photographs depicting alpine landscapes, natural monuments, picturesque villages and much more shaped the perception of Swiss landscapes both nationwide and beyond.</p>
<p dir=""ltr"">The Swiss Art Research Infrastructure (SARI) provides unified and mutual access to Swiss collection data, research data, and digitised visual resources from museums, archives, collections, as well as academic and public research institutes, based on a Linked Open Data network. Being part of a national research infrastructure programme, its mission is to combine the unique scholarly expertise from specialised research institutions beyond technical, linguistic, and institutional borders and to enhance the visibility and accessibility of Switzerland’s valuable collections and research resources.</p>
<p dir=""ltr"">The aggregation and access of these visual resources is fundamental for browsing and understanding the evolution of the framing of Switzerland. The project »Bilder der Schweiz« (Views of Switzerland), developed under the aegis of SARI, provides a unique access point to topographic artworks and photographs from the eighteenth to the early twentieth century from major Swiss libraries, museums, and private collections. Starting from mass-printed, but hand-coloured vedute of the so-called ‘Schweizer Kleinmeister’ -- an affordable visual medium to propagate a canonised view of Switzerland that gained increased popularity over time -- the research portal includes related materials such as landscape paintings, drawings, printed textual sources, travel guides, travel journals, and further materials related to artist production, printing and marketing of printed ‘vedute’.</p>
<p dir=""ltr"">The project will provide unified access to these heterogeneous materials from comparable, but technically different, institutional repositories following the Linked Art data model; and in doing so enhance public visibility of these little known, but widely consumed ‘Schweizer Kleinmeister’, and make them accessible to scholars.</p>
<p dir=""ltr"">The project also provides an excellent test of the Linked Art framework’s flexibility, assessing the model’s ability to describe specific non-mainstream subject-based collections. This paper will reflect upon the overall transformation of a diverse selection of data sources into a Linked Art compliant format; with a specific focus on the advantages and drawbacks of the framework when describing tight semantic integration between the expression (the depicted visual apparatus) and the content (the perspective over the object). Finally, a reflection over the possible coexistence of multiple levels of description of an object, each level addressing specific communities, will be presented.</p>
<p dir=""ltr""><em>Head of gta Digital (ETH Zurich, 2011) and director of the Swiss Art Research Infrastructure (University of Zurich, 2017), Thomas Hänsli has authored the strategy for a national research network and co-authored the development and implementation of comprehensive reference data models based on CIDOC-CRM. He is responsible for the implementation of several Linked Open Data projects in Switzerland and a member of the Linked Art Editorial Board.</em></p>
<p dir=""ltr""></p>
<p dir=""ltr"">[1] https://linked.art/</p>
<p dir=""ltr"">[2] https://iiif.io</p>
<p dir=""ltr"">[3] https://iiif.io/community/groups/</p>
<p dir=""ltr"">[4] https://linked.art/model/</p>
<p dir=""ltr"">[5] https://github.com/NationalGalleryOfArt/dataServices</p>
<p dir=""ltr"">[6] https://apps.apple.com/us/app/national-gallery-of-art-dc/id1455655720</p>
<p dir=""ltr"">[7] https://collections.okeeffemuseum.org</p>
<p dir=""ltr"">[8] https://github.com/linked-art/showcase1/</p>
<p dir=""ltr""></p>
","Panel Overview: Linked Art - Networking Digital Collections and Scholarship
Linked Art[1] is a major new initiative by which art museums will publish information about their collections as interconnected open data. Building upon long standing interdisciplinary thinking from the digital humanities and information engineering, Linked Art is an international collaboration across twenty-four institutions identifying focussed, practical models which meet their requirements on a sustainable basis.
The central aim of Linked Art is the development and application of Linked Data to cultural heritage collections, with an emphasis on works of art and their provenance. Linked Data will provide the foundation for multi-modal digital scholarship across these rich collections; as an open data standard, Linked Art provides consistent, structured ways for arts institutions to publish art-related data where, in many cases, there has not been a consistent shared model to date.
This panel presents a range of perspectives representative of the collaborative intersections found in the Linked Art community: with speakers from universities and art museums; who are practitioners and academics; on topics ranging from implementation, to curation, and research.
The panel will takes the form of six position papers, outlined below, followed by questions and answers between the audience and panel. In doing so, Linked Art seeks to engage with the Digital Humanities community, building capacity for future collaborative implementations and research investigations.
1. Linked Data and Open Data in Cultural Heritage
Emmanuelle Delmas-Glass, The Yale Center for British Art, Yale University, USA.
Cultural heritage institutions have a great deal to gain from deeply engaging in the networked environment. They have poured many resources in the digitization of their collections for the benefit of their audiences, from students to experts, who want to have access to more online material of a higher quality. The current landscape of cultural heritage knowledge on the Web, however, is very much siloed, which both harms the relevance of individual institutions as well as the overall state of scholarship which might use that knowledge.
This paper reflects on the main challenges that cultural heritage institutions face when it comes to publishing their collections descriptions as Linked Open Data resources. Some challenges can be due to friction between a declared digital mission and the resources allocated, which might seem to be in competition with other institutional priorities. Other memory institutions are still in the process of understanding that managing their knowledge and data – so it can then be leveraged for the Semantic Web – needs to be a core data curation activity. It is also partly due to the lack of entry-level technology and ontology resources that has prevented museums from engaging more deeply with Linked Open Data.
This talk will give an overview of previous initiatives and technologies intended to open up access to cultural heritage institutions, particularly art museums, including the International Council of Museums Committee for Documentation Conceptual Reference Model (CIDOC CRM, also an ISO standard); The American Art Collaborative (AAC); PHAROS, the International Consortium of Photo Archives; the Art and Architecture Thesaurus (AAT); the Union List of Artist Names (ULAN); and the International Image Interoperability Framework (IIIF).
In the context of the successes and limitations of these efforts, this paper will outline the strategy taken by Linked Art, which is both a standard-based data model and a community, which has emerged from earlier work at the Getty Research Institute. This presentation will challenge the traditional paradigm which has large and wealthy institutions succeed in the face of structural challenges. In this era of hyper connectedness, the solution to museums’ relevance in the Web cannot be developed by a lone institution, and indeed the model that Linked Art promotes is instead based on inclusion with the goal to create institutional and individual partnerships. The other precept that the Linked Art data model advocates for is usability over absolute data completeness, and this talk will go over some specific data modeling principles that allow balance between the requirements of the institution, domain knowledge experts, technologists who will implement the standard, and scholars and other users of Linked Art.
Emmanuelle Delmas-Glass is the Collections Data Manager at the Yale Center for British Art, a member of the IIIF Operating Committee, and co-chair of the Linked Art working group of the International Council of Museums’ Committee for Documentation (ICOM CIDOC). She oversees the creation, access to and distribution of the museum’s collections information and metadata, playing the lead role in ensuring the intellectual and technical integrity of the collections data and metadata.
2. Conceptual models meet practice and scholarship: conventions, standards, and technology
Kevin Page, Oxford e-Research Centre, University of Oxford, UK.
The utility of a data model is dependent on its usage, and the context in which this use occurs. While conceptual models play an important role in providing a framework within which different data sets can be consistently represented and combined, in practice the resulting information structures can be perceived as complex or unwieldy, which can stifle adoption in cultural heritage institutions.
Specialisms in scholarship and practice bring differing requirements and benefits to the structuring of collections data and its analysis, which are similarly defined by use – the operational needs of a collection or library are different from that of an exhibition, or of the intellectual needs of academic study. The challenge, then, is in respecting and encouraging these different ‘information perspectives’, whilst benefiting from their intersections where they occur, and managing the complexity of the systems and organisational implications. In addition, we recognise scholarship within cataloguing and curation activities, whilst appreciating these have different – but complementary – information needs and outputs from academic study of the collections. Digital tools and methods should reflect these activities, roles and specialisms, rather than constrain them. Doing so can ease adoption of digital approaches alongside existing established practice, increasing the sources of compatible structured information, and achieving overall progress through the combination of multiple information intersections.
In considering the above, this paper reflects upon more nuanced notions of authority, standards, and how these are realised technologically; moving from necessary ‘on the wire’ interoperability to a progression from local practice, through emergent community conventions, to international standards bodies. It can be beneficial for different stages of maturity to exist simultaneously across distinct but complementary information structures, reflecting the communities of specialist practice and scholarship who are using the data. The technologies of Linked Open Data, including RDF and ontologies, provide a flexible foundation through which we can realise such an iterative and incremental approach to standardisation, and in which alternative information perspectives can co-exist.
Linked Art recognises a further first-class perspective: that of the software developer writing (potentially for, or with scholars) applications which consume collection data provided by cultural heritage institutions. It adopts the principles of Linked Open Usable Data, as proposed by Rob Sanderson of the Getty Research Institute, to create a profile of the CIDOC CRM tailored to this information perspective, and in which established practice – for example, in the use of AAT – can be respected.
Kevin Page is Associate Faculty at the University of Oxford e-Research Centre. He was Technical Director of the Oxford Linked Open Data (OXLOD) project, a prototype for the use of CIDOC CRM across the Gardens, Libraries, and Museums of the University of Oxford. Kevin is Principal Investigator of the Linked Art Research Network and Linked Art II project, both funded by the UK’s Arts and Humanities Research Council, and a member of the Linked Art Editorial Board.
3. Practicing Linked Art: Evolving Art Data at the National Gallery of Art
David Beaudet, National Gallery of Art, Washington D.C., USA.
The National Gallery of Art (NGA) seeks to serve the United States in a national role by preserving, collecting, exhibiting, and fostering the understanding of works of art, at the highest possible museum and scholarly standards. As such, the institution is frequently engaged in activities that require production of the structured data describing our collection, the associated media, and writings related to the collection’s historical significance. These demands for data manifest in a variety of ways. Images are requested en masse, data sets are requested in support of research, and both data and media are increasingly requested in support of on-site and remote visitor experiences as well as for analytical purposes.
In recognition of the need to automate the accurate publication of data about art, artists, depictions of art, and associated media, the NGA’s department of Analytics and Enterprise Architecture has been seeking data modeling and dissemination standards that are accepted by the cultural heritage art community in order to ensure the greatest reach possible from its automated art data services. The NGA has selected Linked Art as a strategic data standard.  
Standardizing formats and exchanges of data through automated means has paid dividends for many industries in the past and continues to do so. For example, the IIIF standards[2] for image sharing have expanded the reach of deep zoom technologies and image collections across the cultural heritage sector and it is expanding into other communities[3]. Whilst data standards are not in themselves novel, no prevailing broadly adopted standard for modeling art data exists. Linked Art seeks to fill that gap.
As part of its participation in the Linked Art community, the NGA is evolving an existing art data interface, one that currently provides collection data and images to its Conservation Space system, to use the Linked Art standards as a proof of concept. This paper will give details of that implementation, alongside the basics of the Linked Art model[4] - for representing artworks, people, and depiction. The evolving, open source[5], public-facing NGA interface already provides art data to a location-aware mobile app[6] available for visitors to download on their iOS devices, which will be demonstrated.
A solution architect with the National Gallery of Art since 2005, David Beaudet designs and builds technical solutions for authoring and publishing rich art imagery, content, and metadata. David is a member of the editorial board of Linked Art and collaborates on the IIIF Discovery API.
4. The Linked Art of Georgia O’Keeffe: Collections Across Institutional Boundaries
Samantha Norling, Indianapolis Museum of Art at Newfields, Indianapolis, USA.
The creation and publication of linked open data allows for previously siloed data to be connected with other related data on the Web, breaking down institutional barriers and facilitating research and new scholarship – in traditional and digital formats – that may not have previously been possible. With members and community participants representing over 20 different arts-related organizations, the Editorial Board for the developing Linked Art data model for describing art collections reflects this barrier-breaking nature of linked data.
In order to showcase the connections that can be made when multiple institutions publish their collections data in a consistent format and utilize shared vocabularies, members of the Linked Art community collaborated to create a cross-institutional sample data set. The artist Georgia O’Keeffe was selected to serve as the common thread for the data to be contributed by participating institutions. With an emphasis on relationships, the linked data collected for the showcase naturally expanded to include not just O’Keeffe and her artworks, but also the works of her contemporaries, the exhibitions in which the artworks were exhibited, and the various organizations and individuals that had participated in provenance events in the lifecycle of the artworks.
The development of the Linked Art O’Keeffe data set serendipitously coincided with the launch of the Georgia O’Keeffe Museum’s (GOKM) beta version of the GOKM’s Collections Online, which was built on a linked data foundation. The GOKM expressed the importance of linked data to their digital strategy to make it possible for “meaningful connections to be expressed across different types of collections (artworks, archival items, books, etc.) to establish a more complete understanding of Georgia O’Keeffe’s life, work, and contexts.”
Drawing on both the GOKM’s Collections Online[7] and the co-constructed O’Keeffe showcase data set that is now available publicly within the Linked Art GitHub repository[8], this paper and panel presentation will explore the network of Linked Art data with Georgia O’Keeffe at the center. Following intersections between and connections across data sets, the exploration will follow a path through the many relationships identified that link artworks, archives, exhibitions, and people within O’Keefe’s linked data network. In discussing some of the specific relationships identified between institutional collections, the exploration will also highlight key patterns within the Linked Art data model. The Georgia O’Keeffe showcase data set, while small in scale, demonstrates the potential for the Linked Art data model to facilitate the creation of new connections between collections of all types – connections that cross institutional boundaries and facilitate scholarship at the intersections between GLAM collections.
As Digital Collections Manager at the Indianapolis Museum of Art at Newfields, Samantha Norling manages digital assets and data related to the museum's art, archival, and horticultural collections. A trained librarian and professional archivist working within an art museum, Samantha is particularly interested in digital projects that break down barriers between GLAM institutions. She is a member of the Linked Art Editorial Board.
5. The History of Art is Linked but the Data Is Not: Georgia O’Keeffe, Provenance and Scholarship
Lynn Rother, Leuphana Universität, Germany.
The history of artworks is linked. They were produced by the same artists, traded by the same dealers, collected by the same people, transferred, looted or confiscated by the same entities while eventually finding their permanent home in the same museums – or not. To date, these links across museum collections are only visible to the few scholars or experts studying the artworks’ history of ownership. But the field of provenance research has matured enough to enable and support structuring and aggregating provenance records as Linked Data.
Though shaped by complex and diverse contexts, an artwork’s provenance record can be broken down into empirical data consisting of objects, protagonists, dates, locations and types of transactions. To this day, however, the majority of museums record the valuable information harvested through time-consuming and resource-intensive provenance research within their collection management systems without machine-readable structure, hindering the analysis and linking of the data across institutions on a larger scale. Digital humanities tools offer the potential to standardize, aggregate, and consider the museum accumulated provenance data broadly to reveal new stories about the global circulation and displacement of artworks and nuance the existing histories of collecting and art market practices.
As museum objects and their movements through time and space tell stories beyond object-based art historical research and collection cataloguing, this paper will elaborate on the potential of Linked Art for provenance research and for scholarship in related fields. The intertwined histories of selected works by the American Modern artist Georgia O’Keeffe – from different museum collections including the Georgia O’Keeffe Museum in Santa Fe, New Mexico and The Museum of Modern Art (MoMA) in New York – will serve as an example.
In particular, MoMA’s acquisition and deaccession of O’Keeffe’s Kachinas – representations of Pueblo and Hopi spirits used in ceremonies and rituals and therefore considered culturally sensitive objects in museum collections – will show how structured provenance data of museums using Linked Art can benefit related research fields such as the histories of collecting and art market practices but also museum, Native American, and Indigenous studies.
Lynn Rother is the Lichtenberg-Professor for Provenance Studies at Leuphana Universität, Lüneburg, Germany, and a member of the Linked Art Editorial Board. Previously, Lynn was Senior Provenance Specialist at The Museum of Modern Art, New York, where she oversaw provenance research, procedures, documentation, digital strategies and funding in conjunction with all curatorial departments regarding works in the Collection, loans, acquisitions, and deaccessions.
6. Topographia Helvetiae: Linked Art in Switzerland
Thomas Hänsli, University of Zurich / ETH Zurich, Switzerland.
The historical view of Switzerland’s nature has been defined by artworks describing a ‘visual topography’ of the Alpine country long before the emergence of a broader touristic interest for Switzerland across Europe. Paintings, drawings, prints, and photographs depicting alpine landscapes, natural monuments, picturesque villages and much more shaped the perception of Swiss landscapes both nationwide and beyond.
The Swiss Art Research Infrastructure (SARI) provides unified and mutual access to Swiss collection data, research data, and digitised visual resources from museums, archives, collections, as well as academic and public research institutes, based on a Linked Open Data network. Being part of a national research infrastructure programme, its mission is to combine the unique scholarly expertise from specialised research institutions beyond technical, linguistic, and institutional borders and to enhance the visibility and accessibility of Switzerland’s valuable collections and research resources.
The aggregation and access of these visual resources is fundamental for browsing and understanding the evolution of the framing of Switzerland. The project »Bilder der Schweiz« (Views of Switzerland), developed under the aegis of SARI, provides a unique access point to topographic artworks and photographs from the eighteenth to the early twentieth century from major Swiss libraries, museums, and private collections. Starting from mass-printed, but hand-coloured vedute of the so-called ‘Schweizer Kleinmeister’ -- an affordable visual medium to propagate a canonised view of Switzerland that gained increased popularity over time -- the research portal includes related materials such as landscape paintings, drawings, printed textual sources, travel guides, travel journals, and further materials related to artist production, printing and marketing of printed ‘vedute’.
The project will provide unified access to these heterogeneous materials from comparable, but technically different, institutional repositories following the Linked Art data model; and in doing so enhance public visibility of these little known, but widely consumed ‘Schweizer Kleinmeister’, and make them accessible to scholars.
The project also provides an excellent test of the Linked Art framework’s flexibility, assessing the model’s ability to describe specific non-mainstream subject-based collections. This paper will reflect upon the overall transformation of a diverse selection of data sources into a Linked Art compliant format; with a specific focus on the advantages and drawbacks of the framework when describing tight semantic integration between the expression (the depicted visual apparatus) and the content (the perspective over the object). Finally, a reflection over the possible coexistence of multiple levels of description of an object, each level addressing specific communities, will be presented.
Head of gta Digital (ETH Zurich, 2011) and director of the Swiss Art Research Infrastructure (University of Zurich, 2017), Thomas Hänsli has authored the strategy for a national research network and co-authored the development and implementation of comprehensive reference data models based on CIDOC-CRM. He is responsible for the implementation of several Linked Open Data projects in Switzerland and a member of the Linked Art Editorial Board.
[1] https://linked.art/
[2] https://iiif.io
[3] https://iiif.io/community/groups/
[4] https://linked.art/model/
[5] https://github.com/NationalGalleryOfArt/dataServices
[6] https://apps.apple.com/us/app/national-gallery-of-art-dc/id1455655720
[7] https://collections.okeeffemuseum.org
[8] https://github.com/linked-art/showcase1/","kevin.page@oerc.ox.ac.uk, emmanuelle.delmas-glass@yale.edu, d-beaudet@nga.gov, snorling@discovernewfields.org, lynn.rother@leuphana.de, thomas.haensli@uzh.ch",Panel
"Viola, Lorella;
Verheul, Jaap","Utrecht University, Netherlands, The",The GeoNewsMiner: An interactive spatial humanities tool to visualize geographical references in historical newspapers,"Spatial Humanities, diasporic newspapers, transatlantic migration, NER, machine learning","Comparative (2 or more geographical areas)
Europe
English
North America
19th Century
20th Century
artificial intelligence and machine learning
text mining and analysis
Geography and geo-humanities
History",English,"Comparative (2 or more geographical areas)
Europe
North America","19th Century
20th Century","artificial intelligence and machine learning
text mining and analysis","Geography and geo-humanities
History","<p>The <em>GeoNewsMiner<strong>[1]</strong></em>: An interactive spatial humanities tool to visualize geographical references in historical newspapers<strong>[2]</strong></p>
<p>Lorella Viola<sup>a</sup> and Jaap Verheul<sup>b</sup></p>
<p><sup>a</sup>Luxembourg Centre for Contemporary and Digital History (C<sup>2</sup>DH), University of Luxembourg</p>
<p><sup></sup><sup>b</sup>Department of History and Art History, Utrecht University, Utrecht, the Netherlands</p>
<p>The <em>GeoNewsMiner</em> (GNM) is an interactive tool that maps and visualizes geographical references in historical newspapers. As a use case, we analysed Italian immigrant newspapers published in the United States from 1898 to 1920, as collected in the corpus <em>ChroniclItaly</em> (Viola 2018). Immigrant newspapers form a rich source that adds a historical dimension to the study of both the migration of the past century and the migratory experiences of migrant communities (Viola and Verheul 2019). They for instance enable researchers to compare references to the homeland and the host land (Vellon 2010; Forlenza and Thomassen 2016), thus offering an indication of the way diasporic media negotiate processes of assimilation and ethnic identification (Park 1922; Rhodes 2010; Viola and Musolff 2019, Viola and Verheul 2019), a topic that bears great relevance in the global age of satellite dishes and internet connectivity (Dhoest et al. 2012; Hickerson and Gustafson 2016; Parks 2005; Matsaganis, Katz, and Ball-Rokeach 2011; Appadurai 2008).</p>
<p>In order to offer new perspectives on the geographies of the past, we employed a state-of-the-art deep learning method to extract and disambiguate place names from historical newspapers. Deep learning outperforms the state-of-the-art of place name extraction and disambiguation based on static lists in gazetteers or ensembles of NER-tools (Canale, Lisena, and Troncy 2018; Won, Murrieta-Flores, and Martins 2018; Mariona Coll Ardanuy and Sporleder 2017; Maria Coll Ardanuy 2017, Yadav & Bethard 2019). The two major advantages lie in its potential for text enriching: 1) they may be based on the historical context of a historical corpus; 2) they are able to recognize toponyms in a dynamic way, for instance as as a geographical concept (Viola and Verheul 2020). For the development of the GNM, we the deep learning sequence tagging tool developed by Riedl and Padó (2018). The sequence tagging retrieved 1,369 unique locations which occurred 214,110 times throughout the whole corpus. Because each individual document is timestamped, it was possible to quantify the number of references to each location was at any given time within the timeframe of <em>ChroniclItaly</em>, that is 1898-1920. Afterwards, locations were geocoded by using the Google API which identifies a place as it is stored in the Google Places database and in Google Maps. The tagged version of <em>ChroniclItaly</em> is available as an open access resource<em> </em>(<em>ChroniclItaly 2.0</em>, Viola 2019).</p>
<p>Finally, to visualise and explore the data, we developed the GNM App (Figure 1). Unique to this tool is the possibility to aggregate the data according to a wide range of parameters (time; newspaper’s title; least/most mentioned places; absolute or relative frequency; aggregation on national, regional or city level). It is also possible to overlay historical maps that show the borders of selected years (1880, 1914, 1920, 1994), and download and share the data/results (Figure 2). This offers users the possibility to analyse the results in an intuitive, interactive, and reproduceable way as well as providing great flexibility to researchers working in spatial humanities, particularly from a historical perspective.</p>
<p>One potential application of GNM is for example the possibility to reconstruct the “geographical agenda” of historical newspapers by analysing the changing geographical bias of the press, an issue urgent to fields such as media studies, cultural history and international relations (McCombs 2014; Craine 2014; Reese and Lee 2012; Wanta, Golan, and Lee 2004; Gans 2004; Beaudoin and Thorson 2001; Ginneken 1998; Gitlin 2003). As a preliminary data exploration, for instance, the tool shows that references to geographical locations in both Italy and the United States stay remarkably stable over the period that includes the First World War.[3] The full documentation of GNM is made available to the research community to facilitate transparency, reproducibility and replicability (Viola 2020).[4]  The app has much to recommend particularly to humanities scholars who are more and more confronted with the challenge of exploring collections larger than before and in a digital format.</p>
<br clear=""all"" />
<p>[1] <em>GeoNewsMiner</em> is a project by Lorella Viola and Jaap Verheul. This project was funded by the <em>Utrecht University</em> <em>Innovation Fund for Research in IT</em> and received support from the Research Engineering team of Utrecht University. The technical implementation was provided by Jonathan de Bruin and Casper Caandorp. The Shiny app was developed by Kees van Eijden</p>

<p>[2] We would like to thank DH2020 reviewers for their helpful comments</p>

<p>[3] GNM is available as an open access app at https://utrecht-university.shinyapps.io/GeoNewsMiner/</p>

<p>[4] https://github.com/lorellav/GeoNewsMiner</p>

","The GeoNewsMiner[1]: An interactive spatial humanities tool to visualize geographical references in historical newspapers[2]
Lorella Violaa and Jaap Verheulb
aLuxembourg Centre for Contemporary and Digital History (C2DH), University of Luxembourg
bDepartment of History and Art History, Utrecht University, Utrecht, the Netherlands
The GeoNewsMiner (GNM) is an interactive tool that maps and visualizes geographical references in historical newspapers. As a use case, we analysed Italian immigrant newspapers published in the United States from 1898 to 1920, as collected in the corpus ChroniclItaly (Viola 2018). Immigrant newspapers form a rich source that adds a historical dimension to the study of both the migration of the past century and the migratory experiences of migrant communities (Viola and Verheul 2019). They for instance enable researchers to compare references to the homeland and the host land (Vellon 2010; Forlenza and Thomassen 2016), thus offering an indication of the way diasporic media negotiate processes of assimilation and ethnic identification (Park 1922; Rhodes 2010; Viola and Musolff 2019, Viola and Verheul 2019), a topic that bears great relevance in the global age of satellite dishes and internet connectivity (Dhoest et al. 2012; Hickerson and Gustafson 2016; Parks 2005; Matsaganis, Katz, and Ball-Rokeach 2011; Appadurai 2008).
In order to offer new perspectives on the geographies of the past, we employed a state-of-the-art deep learning method to extract and disambiguate place names from historical newspapers. Deep learning outperforms the state-of-the-art of place name extraction and disambiguation based on static lists in gazetteers or ensembles of NER-tools (Canale, Lisena, and Troncy 2018; Won, Murrieta-Flores, and Martins 2018; Mariona Coll Ardanuy and Sporleder 2017; Maria Coll Ardanuy 2017, Yadav & Bethard 2019). The two major advantages lie in its potential for text enriching: 1) they may be based on the historical context of a historical corpus; 2) they are able to recognize toponyms in a dynamic way, for instance as as a geographical concept (Viola and Verheul 2020). For the development of the GNM, we the deep learning sequence tagging tool developed by Riedl and Padó (2018). The sequence tagging retrieved 1,369 unique locations which occurred 214,110 times throughout the whole corpus. Because each individual document is timestamped, it was possible to quantify the number of references to each location was at any given time within the timeframe of ChroniclItaly, that is 1898-1920. Afterwards, locations were geocoded by using the Google API which identifies a place as it is stored in the Google Places database and in Google Maps. The tagged version of ChroniclItaly is available as an open access resource (ChroniclItaly 2.0, Viola 2019).
Finally, to visualise and explore the data, we developed the GNM App (Figure 1). Unique to this tool is the possibility to aggregate the data according to a wide range of parameters (time; newspaper’s title; least/most mentioned places; absolute or relative frequency; aggregation on national, regional or city level). It is also possible to overlay historical maps that show the borders of selected years (1880, 1914, 1920, 1994), and download and share the data/results (Figure 2). This offers users the possibility to analyse the results in an intuitive, interactive, and reproduceable way as well as providing great flexibility to researchers working in spatial humanities, particularly from a historical perspective.
One potential application of GNM is for example the possibility to reconstruct the “geographical agenda” of historical newspapers by analysing the changing geographical bias of the press, an issue urgent to fields such as media studies, cultural history and international relations (McCombs 2014; Craine 2014; Reese and Lee 2012; Wanta, Golan, and Lee 2004; Gans 2004; Beaudoin and Thorson 2001; Ginneken 1998; Gitlin 2003). As a preliminary data exploration, for instance, the tool shows that references to geographical locations in both Italy and the United States stay remarkably stable over the period that includes the First World War.[3] The full documentation of GNM is made available to the research community to facilitate transparency, reproducibility and replicability (Viola 2020).[4] The app has much to recommend particularly to humanities scholars who are more and more confronted with the challenge of exploring collections larger than before and in a digital format.
[1] GeoNewsMiner is a project by Lorella Viola and Jaap Verheul. This project was funded by the Utrecht University Innovation Fund for Research in IT and received support from the Research Engineering team of Utrecht University. The technical implementation was provided by Jonathan de Bruin and Casper Caandorp. The Shiny app was developed by Kees van Eijden
[2] We would like to thank DH2020 reviewers for their helpful comments
[3] GNM is available as an open access app at https://utrecht-university.shinyapps.io/GeoNewsMiner/
[4] https://github.com/lorellav/GeoNewsMiner","lorella.viola@uni.lu, j.verheul@uu.nl",Short Presentation
"Burkette, Allison (1);
Kretzschmar, William (2)","1: University of Kentucky, United States of America;
2: University of Georgia, United States of America",Web 2.0 for an Authoritative Web Site,"Linguistic Atlas Project, Web 2.0, digital collaboration","English
North America
Contemporary
curricular and pedagogical development and analysis
public humanities collaborations and methods
Linguistics",English,North America,Contemporary,"curricular and pedagogical development and analysis
public humanities collaborations and methods",Linguistics,"<p>As we prepare to move into the next generation of the Linguistic Atlas Project website, we now must engage in theoretical discussion about a critical issue in DH: how to democratize online information in accordance with what has become known as Web 2.0. The LAP has long-standing authority within academic discussions of the development and characteristics of the different varieties of American English. One of the goals of the LAP editors is the expansion of the use of LAP data by non-linguists. In order to reach a new, wider audience we plan to create a ""Teaching and Sharing"" extension of the LAP website, which raises the question: how do we, as sponsors of the LAP website, negotiate authority with participation; in short, how do we let people participate without letting go of the authoritative nature of the website? This presentation addresses this question along with some possible answers.</p>
","As we prepare to move into the next generation of the Linguistic Atlas Project website, we now must engage in theoretical discussion about a critical issue in DH: how to democratize online information in accordance with what has become known as Web 2.0. The LAP has long-standing authority within academic discussions of the development and characteristics of the different varieties of American English. One of the goals of the LAP editors is the expansion of the use of LAP data by non-linguists. In order to reach a new, wider audience we plan to create a ""Teaching and Sharing"" extension of the LAP website, which raises the question: how do we, as sponsors of the LAP website, negotiate authority with participation; in short, how do we let people participate without letting go of the authoritative nature of the website? This presentation addresses this question along with some possible answers.","allison.burkette@uky.edu, kretzsch@uga.edu",Short Presentation
"Desjardins, Renée","Université de Saint-Boniface, Canada","How can science and knowledge be created for all and by all without #linguisticjustice?: Findings from a two-year study on the intersections between citizen science, social media, crowdsourcing, and Translation Studies.","Translation Studies, linguistic justice, knowledge dissemination, Citizen Science","Global
English
Contemporary
meta-criticism (reflections on digital humanities and humanities computing)
social media analysis and methods
Humanities computing
Translation studies",English,Global,Contemporary,"meta-criticism (reflections on digital humanities and humanities computing)
social media analysis and methods","Humanities computing
Translation studies","<p>This paper presents the findings of a 2-year study examining the role of translation and language diversity in online citizen science initiatives. In the last 10 years, the proliferation of mobile technologies, the popularity of participatory culture and social media, as well as the uptick in crowdsourced models for conducting large-scale tasks has  impacted the academic landscape. Specifically, this project considers two platforms, Zooniverse and the Canadian Citizen Science Portal. The overarching framework is an amalgamation of methods and theories, situating the project in the transdisciplinary Digital Humanities. Because the project’s mandate is, in part, to informmore equitable exchange/dissemination of citizen science capitals in online and digital spaces, the underpinning philosophical worldview is one that is transformative. Within the purview of Translation Studies, this study falls under the umbrella of descriptive product-oriented research and context-oriented research. Social network analysis (data visualization) and social media analysis (qualitative) further supplements this framework.</p>
","This paper presents the findings of a 2-year study examining the role of translation and language diversity in online citizen science initiatives. In the last 10 years, the proliferation of mobile technologies, the popularity of participatory culture and social media, as well as the uptick in crowdsourced models for conducting large-scale tasks has impacted the academic landscape. Specifically, this project considers two platforms, Zooniverse and the Canadian Citizen Science Portal. The overarching framework is an amalgamation of methods and theories, situating the project in the transdisciplinary Digital Humanities. Because the project’s mandate is, in part, to informmore equitable exchange/dissemination of citizen science capitals in online and digital spaces, the underpinning philosophical worldview is one that is transformative. Within the purview of Translation Studies, this study falls under the umbrella of descriptive product-oriented research and context-oriented research. Social network analysis (data visualization) and social media analysis (qualitative) further supplements this framework.",rdesjardins@ustboniface.ca,Long Presentation
"Gorman Jr., Daniel James","University of Rochester, United States of America",“Digitizing Rochester’s Religions: Piloting a Community–University Partnership in the Digital Humanities.”,"Religion, Urban, Public, History, Archive","English
North America
19th Century
20th Century
Contemporary
digital archiving
public humanities collaborations and methods
History
Theology and religious studies",English,North America,"19th Century
20th Century
Contemporary","digital archiving
public humanities collaborations and methods","History
Theology and religious studies","<p>Launched by Dr. Margarita Guillory at the University of Rochester in fall 2016, <em>Digitizing Rochester’s Religions </em>documents the evolution of religion in Western New York after the Second Great Awakening (1800–1850) ended. Western New York and the city of Rochester were renowned for revivals and new religious movements during the Second Great Awakening, so that the region became known as the “Burned-over District.” However, Western New York’s religious history after 1850 has not received equal scholarly attention. Dr. Guillory and the graduate and undergraduate students who worked on <em>DRR</em> sought to fill this gap. (I served as the lead graduate student researcher.) The team wrote essays about past and present religious sites, visited religious sites and archives, and digitized sources from community archives. We sought to collaborate with local religious communities, so that <em>DRR</em> would build a meaningful relationship between the University and surrounding neighborhoods.</p>
<p>Bruce Lincoln’s definition of religion, namely that a religion consists of a “discourse,” a “set of practices,” a “community,” and “an institution,” guides <em>DRR</em>. [Source: Bruce Lincoln, <em>Holy Terrors: Thinking about Religion after September 11</em> (Chicago: University of Chicago Press, 2003), 5–7; see 5 for “discourse,” 6 for “set of practices” and “community,” and 7 for the full quote of “an institution.”] In its attention to religious spaces and its collaborative approach to scholarship, <em>DRR</em> draws inspiration from Dr. Courtney Bender’s <em>Sacred Gotham</em>, which tasked students with mapping religious spaces in New York City. It also builds on Dr. David H. Day’s 2003 web project “Encountering Old Faiths in New Places: Mapping Religious Diversity in the Rochester, New York Area,” based at Monroe Community College (https://web.archive.org/web/20071102133941/http://www.monroecc.edu/depts/sociology/pluralism/overview.htm). “Encountering Old Faiths” featured students’ ethnographic observations of current religious sites. <em>DRR</em>, by contrast, profiles past as well as present religious sites, so that it is more historical than anthropological in its orientation. Overall, <em>DRR</em> contributes to a growing field of public-facing projects about lived religion in U.S. cities; notable examples include <em>Boston’s Hidden Sacred Spaces</em> (http://www.hiddensacredspaces.org/) and the <em>American Religious Sounds Project</em> (https://religioussounds.osu.edu/).</p>
<p>When Dr. Guillory moved to Boston University in 2018, I took over <em>DRR</em> and completed it as a pilot project, documenting religious communities in Rochester’s southwester quadrant, instead of the whole city as originally planned. The website (http://digrocreligions.org/) provides a template for historians, religionists, and students to pursue this work on a larger scale. Taken together, the essays featured on <em>DRR</em> detail how, beginning in the 1960s, the loss of Rochester’s industrial base exacerbated racial and economic segregation. Religious organizations in the economically distressed southwestern neighborhoods filled the gap left by the withdrawal of tax dollars and government services. By launching job programs, soup kitchens, and clinics, religious groups in southwest Rochester tried to meet the physical and material as well as spiritual needs of residents.</p>
<p>Had COVID-19 not required the cancellation of DH2020, my lightning talk would have reviewed <em>DRR</em>’s public and digital history aspects and provided a tour of the website. I would have discussed the importance of developing reciprocal relationships with community partners (in our case, religious congregations), although we were not fully successful. After I took over the project, the priority became finishing it, so that I only used publicly available documents instead of archival sources to finish several essays. As for digital humanities technology, I would have discussed the workflow of scanning, formatting, cataloguing, and compressing 80 gigabytes’ worth of primary sources for the website. Finally, I would have presented <em>DRR</em> as an example of successful project-based learning, since the students who worked on <em>DRR</em> gained hands-on archival, ethnographic, and digital humanities experience.</p>
","Launched by Dr. Margarita Guillory at the University of Rochester in fall 2016, Digitizing Rochester’s Religions documents the evolution of religion in Western New York after the Second Great Awakening (1800–1850) ended. Western New York and the city of Rochester were renowned for revivals and new religious movements during the Second Great Awakening, so that the region became known as the “Burned-over District.” However, Western New York’s religious history after 1850 has not received equal scholarly attention. Dr. Guillory and the graduate and undergraduate students who worked on DRR sought to fill this gap. (I served as the lead graduate student researcher.) The team wrote essays about past and present religious sites, visited religious sites and archives, and digitized sources from community archives. We sought to collaborate with local religious communities, so that DRR would build a meaningful relationship between the University and surrounding neighborhoods.
Bruce Lincoln’s definition of religion, namely that a religion consists of a “discourse,” a “set of practices,” a “community,” and “an institution,” guides DRR. [Source: Bruce Lincoln, Holy Terrors: Thinking about Religion after September 11 (Chicago: University of Chicago Press, 2003), 5–7; see 5 for “discourse,” 6 for “set of practices” and “community,” and 7 for the full quote of “an institution.”] In its attention to religious spaces and its collaborative approach to scholarship, DRR draws inspiration from Dr. Courtney Bender’s Sacred Gotham, which tasked students with mapping religious spaces in New York City. It also builds on Dr. David H. Day’s 2003 web project “Encountering Old Faiths in New Places: Mapping Religious Diversity in the Rochester, New York Area,” based at Monroe Community College (https://web.archive.org/web/20071102133941/http://www.monroecc.edu/depts/sociology/pluralism/overview.htm). “Encountering Old Faiths” featured students’ ethnographic observations of current religious sites. DRR, by contrast, profiles past as well as present religious sites, so that it is more historical than anthropological in its orientation. Overall, DRR contributes to a growing field of public-facing projects about lived religion in U.S. cities; notable examples include Boston’s Hidden Sacred Spaces (http://www.hiddensacredspaces.org/) and the American Religious Sounds Project (https://religioussounds.osu.edu/).
When Dr. Guillory moved to Boston University in 2018, I took over DRR and completed it as a pilot project, documenting religious communities in Rochester’s southwester quadrant, instead of the whole city as originally planned. The website (http://digrocreligions.org/) provides a template for historians, religionists, and students to pursue this work on a larger scale. Taken together, the essays featured on DRR detail how, beginning in the 1960s, the loss of Rochester’s industrial base exacerbated racial and economic segregation. Religious organizations in the economically distressed southwestern neighborhoods filled the gap left by the withdrawal of tax dollars and government services. By launching job programs, soup kitchens, and clinics, religious groups in southwest Rochester tried to meet the physical and material as well as spiritual needs of residents.
Had COVID-19 not required the cancellation of DH2020, my lightning talk would have reviewed DRR’s public and digital history aspects and provided a tour of the website. I would have discussed the importance of developing reciprocal relationships with community partners (in our case, religious congregations), although we were not fully successful. After I took over the project, the priority became finishing it, so that I only used publicly available documents instead of archival sources to finish several essays. As for digital humanities technology, I would have discussed the workflow of scanning, formatting, cataloguing, and compressing 80 gigabytes’ worth of primary sources for the website. Finally, I would have presented DRR as an example of successful project-based learning, since the students who worked on DRR gained hands-on archival, ethnographic, and digital humanities experience.",dgormanj@ur.rochester.edu,Lightning
"Tobón Restrepo, Irene","Banco de la República de Colombia, Colombia","La Enciclopedia de Banrepcultural: Una enciclopedia digital de patrimonio cultural colombiano, que nace de la colaboración de los visitantes de las Bibliotecas del Banco de la República ","Colombia, cultural heritage, crowdsourcing","South America
English
Spanish
19th Century
20th Century
Contemporary
crowdsourcing
digital biography, personography, and prosopography
Art history
History","English
Spanish",South America,"19th Century
20th Century
Contemporary","crowdsourcing
digital biography, personography, and prosopography","Art history
History","<p><strong>La Enciclopedia de banrepcultural.org </strong>es una publicación digital de conocimiento sobre el patrimonio cultural colombiano y en español. Es de libre acceso, libre uso y gran parte de su contenido nace de una creación colaborativa a través de campañas de <em>crowdsourcing</em>.</p>
<p>Inició en el año 2000 con respuestas a las consultas académicas de los visitantes a la Biblioteca Luis Ángel Arango y se creó un listado de cerca de 700 artículos que respondían a estas preguntas. En 2017 se decidió publicar estos contenidos en un gestor MediaWiki y se crearon nuevas convocatorias para la generación de información.</p>
<p>En esta nueva fase queremos aprender a integrar La Enciclopedia con Wikipedia y Wikidata, así como ampliar los proyectos colaborativos para traducir los artículos a otros idiomas y a las lenguas indígenas del país. Nos preocupa la poca presencia de conocimiento sobre Colombia y Suramérica que se encuentra disponible en internet. </p>
","La Enciclopedia de banrepcultural.org es una publicación digital de conocimiento sobre el patrimonio cultural colombiano y en español. Es de libre acceso, libre uso y gran parte de su contenido nace de una creación colaborativa a través de campañas de crowdsourcing.
Inició en el año 2000 con respuestas a las consultas académicas de los visitantes a la Biblioteca Luis Ángel Arango y se creó un listado de cerca de 700 artículos que respondían a estas preguntas. En 2017 se decidió publicar estos contenidos en un gestor MediaWiki y se crearon nuevas convocatorias para la generación de información.
En esta nueva fase queremos aprender a integrar La Enciclopedia con Wikipedia y Wikidata, así como ampliar los proyectos colaborativos para traducir los artículos a otros idiomas y a las lenguas indígenas del país. Nos preocupa la poca presencia de conocimiento sobre Colombia y Suramérica que se encuentra disponible en internet.",itobonre@banrep.gov.co,Lightning
"Palladino, Chiara (2);
Karimi, Farimah (3);
Mathiak, Brigitte (1)","1: University of Cologne, Institute of Digital Humanities;
2: Furman University, Classics Department;
3: GESIS Leibniz Institute for the Social Sciences",NER on Ancient Greek texts with minimal annotation,"Named Entity Recognition, Herodot, Conditional Random Fields","Europe
English
BCE-4th Century
natural language processing
text mining and analysis
Computer science
Philology",English,Europe,BCE-4th Century,"natural language processing
text mining and analysis","Computer science
Philology","<p>This paper presents the results in the adaptation of a new workflow of Named Entity Recognition and classification applied to primary sources in Ancient Greek. We used a model of language-independent data extraction and pattern discovery based on machine learning algorithms, which allowed the extraction of a dataset of automatically classified place-names and ethnonyms starting from a small manually annotated dataset. The idea is that we should be able to train the machine to recognize an entity from recurring elements in the context, without providing a long annotated training dataset in advance, working on the assumption that premodern textual sources display a recognized systematicity in their linguistic encoding of space, which provides a test-case for automatic and semi-automatic methods of pattern discovery and extraction.</p>
","This paper presents the results in the adaptation of a new workflow of Named Entity Recognition and classification applied to primary sources in Ancient Greek. We used a model of language-independent data extraction and pattern discovery based on machine learning algorithms, which allowed the extraction of a dataset of automatically classified place-names and ethnonyms starting from a small manually annotated dataset. The idea is that we should be able to train the machine to recognize an entity from recurring elements in the context, without providing a long annotated training dataset in advance, working on the assumption that premodern textual sources display a recognized systematicity in their linguistic encoding of space, which provides a test-case for automatic and semi-automatic methods of pattern discovery and extraction.","chiara.palladino@furman.edu, karimi.farimah@gmail.com, bmathiak@uni-koeln.de",Short Presentation
"Yamada, Taizo;
Inoue, Satoshi","The University of Tokyo, Japan",A Flow for Digitizing Japanese Historical Materials and their Long-Term Use,"Digitalization, Japanese History, Digital Preservation, OAIS","Asia
English
5th-14th Century
15th-17th Century
data, object, and artefact preservation
digital archiving
Asian studies
History",English,Asia,"5th-14th Century
15th-17th Century","data, object, and artefact preservation
digital archiving","Asian studies
History","<p>We formulated a flow and a rule for digitizing historical documents as organizations, not individuals or projects. It is the rule for supplying data, not going to disappear even when projects that create data will end. The flow consists of following processes: the flow has investigating materials and shooting by digital camera, screening and sorting of images, data registration into the database, image registration, security setting. Each process has a responsible person or department in our institution. As a result, images of historical materials of pre-modern Japanese history are acceleratingly concentrated in our storage, and it is growing as ""image cloud for Japanese historical material"". In order to further develop it, now we have been embedding function of the digital preservation[2] like an Open Archival Information System (OAIS)[3] into our system.</p>
","We formulated a flow and a rule for digitizing historical documents as organizations, not individuals or projects. It is the rule for supplying data, not going to disappear even when projects that create data will end. The flow consists of following processes: the flow has investigating materials and shooting by digital camera, screening and sorting of images, data registration into the database, image registration, security setting. Each process has a responsible person or department in our institution. As a result, images of historical materials of pre-modern Japanese history are acceleratingly concentrated in our storage, and it is growing as ""image cloud for Japanese historical material"". In order to further develop it, now we have been embedding function of the digital preservation[2] like an Open Archival Information System (OAIS)[3] into our system.","t_yamada@hi.u-tokyo.ac.jp, inoue@hi.u-tokyo.ac.jp",Poster
"Vignale, Francois (1);
Antonini, Alessio (2);
Gravier, Guillaume (3)","1: Le Mans Université, France;
2: Open University, UK;
3: CNRS, France", THE READING EXPERIENCES ONTOLOGY (REO): REUSING AND EXTENDING CIDOC CRM ,"Ontology management, CIDOC-CRM, digital heritage, history of reading","Global
Europe
French
19th Century
20th Century
Contemporary
data modeling
linked (open) data
Book and print history
Cultural studies",French,"Global
Europe","19th Century
20th Century
Contemporary","data modeling
linked (open) data","Book and print history
Cultural studies","<p align=""justify"">This paper aims to present the development strategy of the ontology proposed in the READ-IT project (https://readit-project.eu) and the contributions it makes to the conceptual description of reading experiences and, more broadly, to the description of intangible heritage and other ""experiential"" phenomena. Its development relied on a data-driven approach with the active participation of a representative panel of reading experts from HSS disciplines. The process was iterative to converge towards a consensus balancing ICT requirements and HSS scholar needs. In this regard, we will focus on the general framework and the way in which both alignments of CIDOC CRM (and some of its extensions) with READ-IT’s data model and creations of classes have been carried out, as well as the benefits derived from a pragmatic and cost-efficient approach, allowing us to offer REO as an extension of CIDOC CRM, which will guarantee its reusability, improvement and maintenance over time.</p>
","This paper aims to present the development strategy of the ontology proposed in the READ-IT project (https://readit-project.eu) and the contributions it makes to the conceptual description of reading experiences and, more broadly, to the description of intangible heritage and other ""experiential"" phenomena. Its development relied on a data-driven approach with the active participation of a representative panel of reading experts from HSS disciplines. The process was iterative to converge towards a consensus balancing ICT requirements and HSS scholar needs. In this regard, we will focus on the general framework and the way in which both alignments of CIDOC CRM (and some of its extensions) with READ-IT’s data model and creations of classes have been carried out, as well as the benefits derived from a pragmatic and cost-efficient approach, allowing us to offer REO as an extension of CIDOC CRM, which will guarantee its reusability, improvement and maintenance over time.","francois.vignale@univ-lemans.fr, alessio.antonini@open.ac.uk, guig@irisa.fr",Short Presentation
"Watson, Jada Emily","University of Ottawa, Canada",From Public Humanities to Social Remembering: Big Data and the Digital Redlining of Women in Country Music Culture ,"social remembering, country music, gender representation, big data, digital redlining","English
North America
Contemporary
digital biography, personography, and prosopography
public humanities collaborations and methods
Feminist studies
Musicology",English,North America,Contemporary,"digital biography, personography, and prosopography
public humanities collaborations and methods","Feminist studies
Musicology","<p>Theories of social remembering (Misztal 2003; Strong 2011) and digital redlining (Noble 2018) offer a critical framework for considering the credibility of big data within cultures that disadvantage and systematically ignore women. Reflecting on results of a data-driven analysis of Mediabase’s country airplay reports from 2000 to 2018, this paper considers the role of data in the process of shaping country music culture, and reframes our understanding of these reports as an instrument that systematically “remembers” some artists, while “casting away” or “forgetting” others. Over the course of this period, the number of songs by women played on country format radio declined 41.3% (Fig. 1). These reports map the evolving terrain of country music’s cultural space and have resulted in a system that pushes women to the margins: songs by women are played infrequently on country format radio (Fig. 2), with the majority of their airplay occurring in the overnights (Fig. 3) (Watson 2019a/b). As a result, songs by women are charting in declining numbers, peaking in the bottom positions of the weekly charts, and barely heard in radio’s peak daytime hours. Such practices impact black women more significantly (Watson 2020). In this way, the reports reveal a digital redlining of women, wherein programming practices are perpetuating inequalities by refusing high traffic times of day to already marginalized artists. More critically, this paper addresses the challenges of critiquing social remembering through public scholarship and reflects (as Marcia Chatelain [2016] does in her work) on my experiences of thinking and working in public digital spaces.</p>
<p>Gender has been a central dynamic of country music history and culture (Pecknold & McCusker 2016), wherein masculinity and femininity are invoked to define class boundaries, cultural tastes, institutional hierarchies, performance styles, and the evolution socially prescribed roles. With strong ties to conservatism and religion, country’s first female artists often appeared on stage with their husband or male family members, a constant reassurance for record-buyers that the social order in which females performed familial roles and habits of constancy and tradition endured in the genre (McCusker 2017). Following WWII, as female artists began taking a more prominent role on stage and behind the scenes, cultural institutions actively sought to censor lyrics in women’s songs if they were too “suggestive”, aggressive, or politically charged—a trend that has continued throughout the genre’s history (Bufwack & Oermann 2004; Keel 2004; Watson & Burns 2010). Behind the scenes, the country music industry has employed a strict quota system for female artists on radio playlists and label rosters (Penuell 2015), which has, in turn, limited their opportunities for participating within the mainstream of the industry as performer and songwriters.</p>
<p>Adopting methods for data-driven studies of popular music charts (Wells 2001; Lafrance et al 2011) and influenced by the concept of prosopography (Keats-Rohan 2007; Crompton & Schwartz 2018), this project has developed an approach for collecting and organizing music industry data in order to study how the biography of individuals shapes and is shaped by the genre’s cultural constructs. In order to address complex socio-cultural issues of equity and diversity in country music, it has developed a comprehensive dataset of all of the singles played on country format radio between 2000 and 2018, enhanced with biographic information about the lead and featured artists involved in performing the recorded tracks played on country radio to facilitate a vast range of queries about programming practices and their impact on weekly charts. In so doing, this project deconstructs the gender politics that have governed the genre and shows how big data has created and perpetuated gender inequalities and contributed to the continued marginalization and “forgetting” of female narrative voices within country music culture.</p>
<p>  </p>
<p>Figure 1. Distribution of unique songs by men, women and male-female ensembles played on country format radio between 2002 and 2018.</p>
<p>Figure 2. Distribution of spins for songs by men, women and male-female ensembles between 2002 and 2018 reveals a 40.2% increase in spins for male artists against a 44.8% decline in spins for songs by women.</p>
<p>Figure 3. Distribution of spins for songs by men, women and male-female ensembles across the five dayparts in country format radio programming in 2002 (left) and 2018 (right).</p>
<p>Appendix A</p>
<p>Bibliography</p>
<p>Bufwack, Mary A., and Robert K. Oermann. 2003. <em>Finding Her Voice: Women in Country Music, 1800-2000</em>. Nashville: Vanderbilt University Press; Country Music Founcation Press.</p>
<p>Chatelain, Marcia. 2018. “Is Twitter Any Place for a [Black Academic] Lady?” In <em>Bodies of Information: Intersectional Feminism and Digital Humanities</em>, eds Elizabeth Losh and Jacqueline Wernimont, 173-184. Minneapolis: University of Minnesota Press.</p>
<p>Crompton, Constance, and Michelle Schwartz. 2018. “Remaking HIstory: Lesbian Feminist Historical Methods in the Digital Humanities.” In <em>Bodies of Information: Intersectional Feminism and Digital Humanities</em>, eds Elizabeth Losh and Jacqueline Wernimont, 131-56. Minneapolis: University of Minnesota Press.</p>
<p>Keel, Beverly. 2004. “Between Riot Grrrl and Quiet Girl.” In <em>A Boy Named Sue: Gender and Country Music</em>, eds Kristine McCusker and Diane Pecknold, 155-77. Jackson: University of Mississippi Press.</p>
<p>Keats-Rohan, Katherine S.B.. 2007. <em>Prosopography Approaches and Applications: A Handbook</em>. Oxford: Oxford University Press.</p>
<p>Lafrance, Marc, et al. “Gender and the Billboard Top 40 Charts between 1997 and 2007.” <em>Popular Music & Society</em> 35/5: 557-70.</p>
<p>McCusker, Kristine M. 2017. “Gendered Stages: Country Music, Authenticity, and the Performance of Gender.” In <em>The Oxford Handbook to Country Music</em>, ed Travis D. Stimeling, 355-74. Oxford: Oxford University Press.</p>
<p>Misztal, Barbra A. 2003. <em>Theories of Social Remembering</em>. Philadelphia: Open University Press.</p>
<p>Noble, Safiya Umoja. 2018. <em>Algorithms of Oppression: How Search Engines Reinforce Racism</em>. New York: New York University Press.</p>
<p>Pecknold, Diane, and Kristine M. McCusker, eds. <em>Country Boys and Redneck Women: New Essays in Gender and Country Music</em>. Jackson: University of Mississippi Press, 2016.</p>
<p>Penuell, Russ. 2015. “On Music and Scheduling.” <em>Country Aircheck</em>, 449: 1, 8, https://www.countryaircheck.com/pdfs/current052615.pdf.</p>
<p>Strong, Catherine. 2011. “Grunge, Riot Grrrl and the Forgetting of Women in Popular culture.” <em>The Journal of Popular Music Culture</em> 44/2: 398-416.</p>
<p>Watson, Jada. 2019a. “Gender Representation on Country Format Radio: A Study of Published Reports from 2000 to 2018.” Report prepared in consultation with WOMAN Nashville, https://bit.ly/gender-country-radio.</p>
<p>Watson, Jada. 2019b. “Gender Representation on Country Format Radio: A Study of Spins Across Dayparts (2002-2018).” Report prepared in consultation with WOMAN Nashville, https://bit.ly/gender-country-radio-tod-airplay.</p>
<p>Watson, Jada. 2020. “Inequality on Country Radio: 2019 in Review.” Report prepared in partnership with CMT’s EqualPlay Campaign, https://bit.ly/gender-country-radio-equalplay.</p>
<p>Watson, Jada, and Lori Burns. 2010. “Resisting Exile and Asserting Musical Voice: The Dixie Chicks Are ‘Not Ready to Make Nice’.” <em>Popular Music</em> 29/4: 325-50.</p>
<p>Wells, Allan. 2001. “Nationality, Race and Gender on the American Pop Charts: what Happened in the 90s?” <em>Popular Music & Society</em> 25: 221-31.</p>
","Theories of social remembering (Misztal 2003; Strong 2011) and digital redlining (Noble 2018) offer a critical framework for considering the credibility of big data within cultures that disadvantage and systematically ignore women. Reflecting on results of a data-driven analysis of Mediabase’s country airplay reports from 2000 to 2018, this paper considers the role of data in the process of shaping country music culture, and reframes our understanding of these reports as an instrument that systematically “remembers” some artists, while “casting away” or “forgetting” others. Over the course of this period, the number of songs by women played on country format radio declined 41.3% (Fig. 1). These reports map the evolving terrain of country music’s cultural space and have resulted in a system that pushes women to the margins: songs by women are played infrequently on country format radio (Fig. 2), with the majority of their airplay occurring in the overnights (Fig. 3) (Watson 2019a/b). As a result, songs by women are charting in declining numbers, peaking in the bottom positions of the weekly charts, and barely heard in radio’s peak daytime hours. Such practices impact black women more significantly (Watson 2020). In this way, the reports reveal a digital redlining of women, wherein programming practices are perpetuating inequalities by refusing high traffic times of day to already marginalized artists. More critically, this paper addresses the challenges of critiquing social remembering through public scholarship and reflects (as Marcia Chatelain [2016] does in her work) on my experiences of thinking and working in public digital spaces.
Gender has been a central dynamic of country music history and culture (Pecknold & McCusker 2016), wherein masculinity and femininity are invoked to define class boundaries, cultural tastes, institutional hierarchies, performance styles, and the evolution socially prescribed roles. With strong ties to conservatism and religion, country’s first female artists often appeared on stage with their husband or male family members, a constant reassurance for record-buyers that the social order in which females performed familial roles and habits of constancy and tradition endured in the genre (McCusker 2017). Following WWII, as female artists began taking a more prominent role on stage and behind the scenes, cultural institutions actively sought to censor lyrics in women’s songs if they were too “suggestive”, aggressive, or politically charged—a trend that has continued throughout the genre’s history (Bufwack & Oermann 2004; Keel 2004; Watson & Burns 2010). Behind the scenes, the country music industry has employed a strict quota system for female artists on radio playlists and label rosters (Penuell 2015), which has, in turn, limited their opportunities for participating within the mainstream of the industry as performer and songwriters.
Adopting methods for data-driven studies of popular music charts (Wells 2001; Lafrance et al 2011) and influenced by the concept of prosopography (Keats-Rohan 2007; Crompton & Schwartz 2018), this project has developed an approach for collecting and organizing music industry data in order to study how the biography of individuals shapes and is shaped by the genre’s cultural constructs. In order to address complex socio-cultural issues of equity and diversity in country music, it has developed a comprehensive dataset of all of the singles played on country format radio between 2000 and 2018, enhanced with biographic information about the lead and featured artists involved in performing the recorded tracks played on country radio to facilitate a vast range of queries about programming practices and their impact on weekly charts. In so doing, this project deconstructs the gender politics that have governed the genre and shows how big data has created and perpetuated gender inequalities and contributed to the continued marginalization and “forgetting” of female narrative voices within country music culture.
  
Figure 1. Distribution of unique songs by men, women and male-female ensembles played on country format radio between 2002 and 2018.
Figure 2. Distribution of spins for songs by men, women and male-female ensembles between 2002 and 2018 reveals a 40.2% increase in spins for male artists against a 44.8% decline in spins for songs by women.
Figure 3. Distribution of spins for songs by men, women and male-female ensembles across the five dayparts in country format radio programming in 2002 (left) and 2018 (right).
Appendix A
Bibliography
Bufwack, Mary A., and Robert K. Oermann. 2003. Finding Her Voice: Women in Country Music, 1800-2000. Nashville: Vanderbilt University Press; Country Music Founcation Press.
Chatelain, Marcia. 2018. “Is Twitter Any Place for a [Black Academic] Lady?” In Bodies of Information: Intersectional Feminism and Digital Humanities, eds Elizabeth Losh and Jacqueline Wernimont, 173-184. Minneapolis: University of Minnesota Press.
Crompton, Constance, and Michelle Schwartz. 2018. “Remaking HIstory: Lesbian Feminist Historical Methods in the Digital Humanities.” In Bodies of Information: Intersectional Feminism and Digital Humanities, eds Elizabeth Losh and Jacqueline Wernimont, 131-56. Minneapolis: University of Minnesota Press.
Keel, Beverly. 2004. “Between Riot Grrrl and Quiet Girl.” In A Boy Named Sue: Gender and Country Music, eds Kristine McCusker and Diane Pecknold, 155-77. Jackson: University of Mississippi Press.
Keats-Rohan, Katherine S.B.. 2007. Prosopography Approaches and Applications: A Handbook. Oxford: Oxford University Press.
Lafrance, Marc, et al. “Gender and the Billboard Top 40 Charts between 1997 and 2007.” Popular Music & Society 35/5: 557-70.
McCusker, Kristine M. 2017. “Gendered Stages: Country Music, Authenticity, and the Performance of Gender.” In The Oxford Handbook to Country Music, ed Travis D. Stimeling, 355-74. Oxford: Oxford University Press.
Misztal, Barbra A. 2003. Theories of Social Remembering. Philadelphia: Open University Press.
Noble, Safiya Umoja. 2018. Algorithms of Oppression: How Search Engines Reinforce Racism. New York: New York University Press.
Pecknold, Diane, and Kristine M. McCusker, eds. Country Boys and Redneck Women: New Essays in Gender and Country Music. Jackson: University of Mississippi Press, 2016.
Penuell, Russ. 2015. “On Music and Scheduling.” Country Aircheck, 449: 1, 8, https://www.countryaircheck.com/pdfs/current052615.pdf.
Strong, Catherine. 2011. “Grunge, Riot Grrrl and the Forgetting of Women in Popular culture.” The Journal of Popular Music Culture 44/2: 398-416.
Watson, Jada. 2019a. “Gender Representation on Country Format Radio: A Study of Published Reports from 2000 to 2018.” Report prepared in consultation with WOMAN Nashville, https://bit.ly/gender-country-radio.
Watson, Jada. 2019b. “Gender Representation on Country Format Radio: A Study of Spins Across Dayparts (2002-2018).” Report prepared in consultation with WOMAN Nashville, https://bit.ly/gender-country-radio-tod-airplay.
Watson, Jada. 2020. “Inequality on Country Radio: 2019 in Review.” Report prepared in partnership with CMT’s EqualPlay Campaign, https://bit.ly/gender-country-radio-equalplay.
Watson, Jada, and Lori Burns. 2010. “Resisting Exile and Asserting Musical Voice: The Dixie Chicks Are ‘Not Ready to Make Nice’.” Popular Music 29/4: 325-50.
Wells, Allan. 2001. “Nationality, Race and Gender on the American Pop Charts: what Happened in the 90s?” Popular Music & Society 25: 221-31.",jwatso4@uottawa.ca,Short Presentation
"Homburg, Timo",Mainz University Of Applied Sciences,Mind the gap: Filling gaps in cuneiform tablets using Machine Learning Algorithms,"Cuneiform, Text","Asia
English
BCE-4th Century
artificial intelligence and machine learning
semantic analysis
Computer science
Linguistics",English,Asia,BCE-4th Century,"artificial intelligence and machine learning
semantic analysis","Computer science
Linguistics","<p>Cuneiform tablets, when excavated may exhibit a lot of damage, as fragements of cuneiform tablets can be broken over the centuries, making a text harder to interpret for scholars. This poster shows first efforts to use Machine Learning algorithms to overcome this obstacle. Using data of the cuneiform digital library intiative (CDLI), the poster shows the results of rulebased, dictionary-based and statistical approaches and discusses features which may allow for an accurate classification of missing cuneiform characters. The goal of those efforts is to create a suggestion system for cuneiform scholars which give meaningful suggestions to fill gaps. The system is developed in an ongoing research project and may be presented as prototype at the conference.</p>
","Cuneiform tablets, when excavated may exhibit a lot of damage, as fragements of cuneiform tablets can be broken over the centuries, making a text harder to interpret for scholars. This poster shows first efforts to use Machine Learning algorithms to overcome this obstacle. Using data of the cuneiform digital library intiative (CDLI), the poster shows the results of rulebased, dictionary-based and statistical approaches and discusses features which may allow for an accurate classification of missing cuneiform characters. The goal of those efforts is to create a suggestion system for cuneiform scholars which give meaningful suggestions to fill gaps. The system is developed in an ongoing research project and may be presented as prototype at the conference.",timo.homburg@gmx.de,Poster
"Esten, Emily (1);
Blickhan, Samantha (2);
Noel, Will (1);
Rustow, Marina (3)","1: University of Pennsylvania Libraries, United States of America;
2: Adler Planetarium;
3: Princeton University","Scribes, Scholars, & Scripts: creating a Digital Humanities community through crowdsourcing","open data, crowdsourcing, multilingual DH, text analysis, project management","Global
English
5th-14th Century
15th-17th Century
18th Century
crowdsourcing
public humanities collaborations and methods
History
Library & information science",English,Global,"5th-14th Century
15th-17th Century
18th Century","crowdsourcing
public humanities collaborations and methods","History
Library & information science","<p dir=""ltr"">How do you organize a research project around manuscript fragments? Digital collections can allow teams to create a shared online space in which images can be hosted, but the types of research questions that individual scholars may want to ask can be as fragmented as the objects of interest themselves. In this panel, we will discuss Scribes of the Cairo Geniza (https://www.scribesofthecairogeniza.org), a collaboration between the University of Pennsylvania Libraries and the Zooniverse (https://www.zooniverse.org), the world’s largest platform for online crowdsourced research. The project invites the public to help classify and transcribe fragments from the Cairo Geniza, a corpus of 350,000 fragments primarily from the 10th-13th centuries, found in a storeroom (or ‘geniza’) of the Ben Ezra synagogue in Fustat. The panelists will discuss the process of designing, developing, and running a large, multi-institution online crowdsourcing project from the following perspectives: project manager, web developer, content specialist, and data specialist.</p>
","How do you organize a research project around manuscript fragments? Digital collections can allow teams to create a shared online space in which images can be hosted, but the types of research questions that individual scholars may want to ask can be as fragmented as the objects of interest themselves. In this panel, we will discuss Scribes of the Cairo Geniza (https://www.scribesofthecairogeniza.org), a collaboration between the University of Pennsylvania Libraries and the Zooniverse (https://www.zooniverse.org), the world’s largest platform for online crowdsourced research. The project invites the public to help classify and transcribe fragments from the Cairo Geniza, a corpus of 350,000 fragments primarily from the 10th-13th centuries, found in a storeroom (or ‘geniza’) of the Ben Ezra synagogue in Fustat. The panelists will discuss the process of designing, developing, and running a large, multi-institution online crowdsourcing project from the following perspectives: project manager, web developer, content specialist, and data specialist.","estenemily@gmail.com, samantha@zooniverse.org, wgnoel@gmail.com, mrustow@princeton.edu",Panel
"Howard-Sukhil, Christian","Bucknell University, United States of America","Project Twitter Literature: Scraping, Analyzing, and Archiving Twitter Data in Literary Research","social media, data curation, data preservation, globality","Global
English
Contemporary
digital archiving
social media analysis and methods
Literary studies
Media studies",English,Global,Contemporary,"digital archiving
social media analysis and methods","Literary studies
Media studies","<p><em>Project Twitter Literature </em>(<em>TwitLit</em>),<em> </em>seeks to address a growing gap in the literary-historical record[1] by establishing a consistent, rigorous, and ethical method for scraping and cleaning up Twitter data for the use of humanities scholars. In particular, my project explores the growing community of amateur writers who are using Twitter as a means of publication and dissemination for their literary output. There are three parts to my project: the research findings related to the global literary community on Twitter, the tools and resources developed as part of the project and made openly available to other scholars, and partnership with a university library to ensure the long-term preservation of the collected data.</p>
<p>The data that I have collected shows that social media is altering literary practices by providing a space for amateur writers to publish, disseminate, and receive feedback from a global community of writers. Preliminary figures put the number of active Anglophone writers using Twitter as a publication platform for their literary output at over 1 million users per year since 2015, and writers working in non-English languages on Twitter raise these numbers even higher. This practice is changing how literature is produced, published, and shared. Readerships too are changing, for rather than being tied to print subscriptions or access to physical books, audiences of social media literature are based on online communities and tied to the costs of physical devices and internet access.</p>
<p>My presentation will showcase these research findings in order to highlight the importance and necessity of social media archival work. In so doing, I will discuss how I collected the data using a Python script (co-developed by myself and several other scholars), challenges of cleaning up and visualizing this data (using ArcGIS and tools developed by Documenting the Now), and ethical best-practices for using social media data in research. Information relating to this process – including detailed instructions, the Python scripts used to collect Twitter data, and a list of resources – are free and openly accessible on the project’s website (www.twit-lit.com) and GitHub repository (https://github.com/TwitLit/TwitLitSource). Other scholars are invited to use these scripts and other resources to collect their own social media data.</p>
<p>Additionally, my project has attempted to plan for the long-term preservation of the over eight million tweets that I have collected. This preservation has been made difficult by Twitter’s strict Developer Policy and Agreement, which prevents individuals from keeping or disseminating large data sets for more than 30 days. The only exception to this policy is made on behalf of academic institutions, which may store Twitter data for unlimited amounts of time on behalf of academic research.[2] The <em>Project TwitLit </em>project thus presents best-practices for establishing a working relationship with university libraries for storing and disseminating Twitter data in a way that is both in accord with Twitter’s legal restrictions and responsive to the needs of scholars. In short, <em>Project TwitLit </em>provides a case-study of a growing community on Twitter while simultaneously developing a set of tools and guidelines for other scholars seeking to engage in similar work.</p>
<p>[1] In December 2017, the Library of Congress, which began archiving Twitter in 2010, announced that it would no longer collect all Tweets; instead, Tweets produced after December 2017 would only be collected on a selective basis. There are no other ongoing, systematic efforts to collect and preserve this digital material. See Library of Congress, “Update on the Twitter Archive at the Library of Congress” (December 2017).</p>
<p>[2] For more information related to the challenges of collecting and storing Twitter data, please see Christian Howard, “Studying and Preserving the Global Networks of Twitter Literature,” in <em>Post-45</em>.</p>
","Project Twitter Literature (TwitLit), seeks to address a growing gap in the literary-historical record[1] by establishing a consistent, rigorous, and ethical method for scraping and cleaning up Twitter data for the use of humanities scholars. In particular, my project explores the growing community of amateur writers who are using Twitter as a means of publication and dissemination for their literary output. There are three parts to my project: the research findings related to the global literary community on Twitter, the tools and resources developed as part of the project and made openly available to other scholars, and partnership with a university library to ensure the long-term preservation of the collected data.
The data that I have collected shows that social media is altering literary practices by providing a space for amateur writers to publish, disseminate, and receive feedback from a global community of writers. Preliminary figures put the number of active Anglophone writers using Twitter as a publication platform for their literary output at over 1 million users per year since 2015, and writers working in non-English languages on Twitter raise these numbers even higher. This practice is changing how literature is produced, published, and shared. Readerships too are changing, for rather than being tied to print subscriptions or access to physical books, audiences of social media literature are based on online communities and tied to the costs of physical devices and internet access.
My presentation will showcase these research findings in order to highlight the importance and necessity of social media archival work. In so doing, I will discuss how I collected the data using a Python script (co-developed by myself and several other scholars), challenges of cleaning up and visualizing this data (using ArcGIS and tools developed by Documenting the Now), and ethical best-practices for using social media data in research. Information relating to this process – including detailed instructions, the Python scripts used to collect Twitter data, and a list of resources – are free and openly accessible on the project’s website (www.twit-lit.com) and GitHub repository (https://github.com/TwitLit/TwitLitSource). Other scholars are invited to use these scripts and other resources to collect their own social media data.
Additionally, my project has attempted to plan for the long-term preservation of the over eight million tweets that I have collected. This preservation has been made difficult by Twitter’s strict Developer Policy and Agreement, which prevents individuals from keeping or disseminating large data sets for more than 30 days. The only exception to this policy is made on behalf of academic institutions, which may store Twitter data for unlimited amounts of time on behalf of academic research.[2] The Project TwitLit project thus presents best-practices for establishing a working relationship with university libraries for storing and disseminating Twitter data in a way that is both in accord with Twitter’s legal restrictions and responsive to the needs of scholars. In short, Project TwitLit provides a case-study of a growing community on Twitter while simultaneously developing a set of tools and guidelines for other scholars seeking to engage in similar work.
[1] In December 2017, the Library of Congress, which began archiving Twitter in 2010, announced that it would no longer collect all Tweets; instead, Tweets produced after December 2017 would only be collected on a selective basis. There are no other ongoing, systematic efforts to collect and preserve this digital material. See Library of Congress, “Update on the Twitter Archive at the Library of Congress” (December 2017).
[2] For more information related to the challenges of collecting and storing Twitter data, please see Christian Howard, “Studying and Preserving the Global Networks of Twitter Literature,” in Post-45.",cfh008@bucknell.edu,Short Presentation
"BLILID, Abdelaziz","University of Tours, France",«La planète numérique» d'un peuple autochtone transnational: Une analyse des liens hypertextes des sites web amazighs,peuples autochtones - culture amazighe - activisme culturel - communauté imaginée,"Africa
French
Contemporary
cultural analytics
linked (open) data
African and African American Studies
First nations and indigenous studies",French,Africa,Contemporary,"cultural analytics
linked (open) data","African and African American Studies
First nations and indigenous studies","<p>This research processing on the Internet using the Aboriginal Nations in the North America: les Berbères ou les Amazighs. Ce peuple a Investi Internet Internet dès les années 1990, a déclaré que le texte était représenté dans la toile numérique afin de permettre la transmission de la culture entre eux et d'associations et de faire en sorte que leurs revendications politiques (Almasude, 1994). Ils ont créé des sites Web pour codifier et transmettre leur patrimoine culturel. Ainsi, le Web leur a offert un moyen singulier de transmission: en transposant dans l'univers numérique, l'identité culturelle s'est redéfinie. This is this subject of this study at the people on the amazigh for cultural protection and activism is one one jou.</p>
","This research processing on the Internet using the Aboriginal Nations in the North America: les Berbères ou les Amazighs. Ce peuple a Investi Internet Internet dès les années 1990, a déclaré que le texte était représenté dans la toile numérique afin de permettre la transmission de la culture entre eux et d'associations et de faire en sorte que leurs revendications politiques (Almasude, 1994). Ils ont créé des sites Web pour codifier et transmettre leur patrimoine culturel. Ainsi, le Web leur a offert un moyen singulier de transmission: en transposant dans l'univers numérique, l'identité culturelle s'est redéfinie. This is this subject of this study at the people on the amazigh for cultural protection and activism is one one jou.",a.blilid@gmail.com,Lightning
"Okuda, Nozomu (1);
Kinnison, Jeffery (2);
Coffee, Neil (1);
Scheirer, Walter (2)","1: Department of Classics, The University at Buffalo, SUNY;
2: Department of Computer Science & Engineering, University of Notre Dame",Integrating Intertextual Search into Your Web Application: The Tesserae Intertext Service API,"intertextuality, API, web services, digital classics","English
North America
BCE-4th Century
5th-14th Century
Contemporary
software development, systems, analysis and methods
text mining and analysis
Literary studies
Philology",English,North America,"BCE-4th Century
5th-14th Century
Contemporary","software development, systems, analysis and methods
text mining and analysis","Literary studies
Philology","<p>The Tesserae Project presents the TIS API, a REST-based web service that allows partner collections to integrate Tesserae intertext search directly into their web applications.</p>
","The Tesserae Project presents the TIS API, a REST-based web service that allows partner collections to integrate Tesserae intertext search directly into their web applications.","nozomuok@buffalo.edu, jkinniso@nd.edu, ncoffee@buffalo.edu, walter.scheirer@nd.edu",Poster
"Scheirer, Walter (1);
Forstall, Christopher (2)","1: University of Notre Dame, United States of America;
2: Mount Allison University, Canada",Quantitative Intertextuality: Analyzing the Markers of Information Reuse,"Intertextuality, Text Analysis, Cultural Analytics, Computer Vision, Memes","English
North America
Contemporary
artificial intelligence and machine learning
text mining and analysis
Cultural studies
Literary studies",English,North America,Contemporary,"artificial intelligence and machine learning
text mining and analysis","Cultural studies
Literary studies","<p>A remarkable amount of information crosses our eyes and ears each day, yet we adeptly identify what is familiar with seemingly no effort at all. In many cases, what we see or hear has been shaped by recognizable prior sources. Such instances of intertextuality reveal a wealth of data about authorship, influence, and style, making them attractive targets for automatic identification. This lightning talk introduces quantitative intertextuality [Forstall-and-Scheirer-2019], a new approach for the algorithmic study of information reuse in text, sound and images. Using a variety of tools drawn from machine learning, natural language processing, and computer vision, we will describe how to trace patterns of reuse across diverse sources for scholarly work and practical applications.</p>
","A remarkable amount of information crosses our eyes and ears each day, yet we adeptly identify what is familiar with seemingly no effort at all. In many cases, what we see or hear has been shaped by recognizable prior sources. Such instances of intertextuality reveal a wealth of data about authorship, influence, and style, making them attractive targets for automatic identification. This lightning talk introduces quantitative intertextuality [Forstall-and-Scheirer-2019], a new approach for the algorithmic study of information reuse in text, sound and images. Using a variety of tools drawn from machine learning, natural language processing, and computer vision, we will describe how to trace patterns of reuse across diverse sources for scholarly work and practical applications.","walter.scheirer@nd.edu, cforstall@mta.ca",Lightning
"Day, Kevin",University of British Columbia,“Beyond the logic of commensurability: a cultural analysis of media artworks and digital media in information capitalism”,"media art, philosophy of technology, media studies, information capitalism, big data","Global
English
Contemporary
digital art production and analysis
Art history
Media studies",English,Global,Contemporary,digital art production and analysis,"Art history
Media studies","<p>The proposed presentation will examine the pedagogical potential of media artworks that interrogate the big data economy within contemporary information society. The study begins by establishing the socio-political landscape within which it is situated, one that recognizes the pervasive utopic myth and exploitative algorithmic activities of informatics, and asserts that media art needs to address digital media by examining the underpinning logic of information within the wider landscape of information capitalism. Guided by a framework that pulls together theories of media and art, the paper argues that media art has the capacity to subvert normalized and entrenched ways of knowing through its potential to foster ways of 'knowing differently' in relation to information and communication technology. To substantiate the argument, the paper will position information as an epistemic model through which one comes to make sense of the world – and precisely that which visual/media art should tackle and question.</p>
","The proposed presentation will examine the pedagogical potential of media artworks that interrogate the big data economy within contemporary information society. The study begins by establishing the socio-political landscape within which it is situated, one that recognizes the pervasive utopic myth and exploitative algorithmic activities of informatics, and asserts that media art needs to address digital media by examining the underpinning logic of information within the wider landscape of information capitalism. Guided by a framework that pulls together theories of media and art, the paper argues that media art has the capacity to subvert normalized and entrenched ways of knowing through its potential to foster ways of 'knowing differently' in relation to information and communication technology. To substantiate the argument, the paper will position information as an epistemic model through which one comes to make sense of the world – and precisely that which visual/media art should tackle and question.",kevin.t.day@gmail.com,Short Presentation
"Kelber, Nathan",ITHAKA,The Plant Humanities Workbench: Using Linked Open Data to Discover Early Modern Plant History,"plants, linked open data, early modern, book history","Comparative (2 or more geographical areas)
Europe
English
North America
15th-17th Century
Contemporary
linked (open) data
software development, systems, analysis and methods
Book and print history
Environmental, ocean, and waterway studies",English,"Comparative (2 or more geographical areas)
Europe
North America","15th-17th Century
Contemporary","linked (open) data
software development, systems, analysis and methods","Book and print history
Environmental, ocean, and waterway studies","<p dir=""ltr"">In 2017 with the support of the Mellon Foundation, the experimental wing of JSTOR known as JSTOR Labs began a three year partnership with Dumbarton Oaks, a Harvard-owned research library and collection in Washington DC, to create a new digital humanities tool focused on the study of plants from a humanities perspective. The Plant Humanities Workbench will enable new research by intergenerational teams of students, advanced researchers, and professionals. Drawing on the technical and design expertise of the JSTOR Labs team, the digital tool will be interactive, iterative, and scalable. This presentation will demonstrate the tool after its first year of development, summarizing key findings in the project that may be relevant to a variety of digital humanists.</p>
","In 2017 with the support of the Mellon Foundation, the experimental wing of JSTOR known as JSTOR Labs began a three year partnership with Dumbarton Oaks, a Harvard-owned research library and collection in Washington DC, to create a new digital humanities tool focused on the study of plants from a humanities perspective. The Plant Humanities Workbench will enable new research by intergenerational teams of students, advanced researchers, and professionals. Drawing on the technical and design expertise of the JSTOR Labs team, the digital tool will be interactive, iterative, and scalable. This presentation will demonstrate the tool after its first year of development, summarizing key findings in the project that may be relevant to a variety of digital humanists.",NKELBER@GMAIL.COM,Short Presentation
"Kelber, Nathan","JSTOR Labs, United States of America",Algorithms of Resistance: Using OCR and AI for Social Justice,"jim crow, algorithmic bias, ocr, tdm","English
North America
19th Century
20th Century
optical character recognition and handwriting recognition
text mining and analysis
African and African American Studies
Law and legal studies",English,North America,"19th Century
20th Century","optical character recognition and handwriting recognition
text mining and analysis","African and African American Studies
Law and legal studies","<p dir=""ltr"">This long presentation will share key findings from the final report of <em>On the Books: Jim Crow and Algorithms of Resistance</em>, a Mellon-funded Collections as Data project using machine learning to systematically discover racism within North Carolina laws. The project will make North Carolina legal history accessible to researchers by creating a corpus of over one hundred years of North Carolina public, private, and local session laws and resolutions from the end of civil war through the civil rights movement (1865-1968). This project represents the intersection of many subject areas including critical race theory, American history, text and data mining, machine learning, and public humanities. It will be of interest to subject specialists, data science researchers, educators, librarians, and other information professionals.</p>
","This long presentation will share key findings from the final report of On the Books: Jim Crow and Algorithms of Resistance, a Mellon-funded Collections as Data project using machine learning to systematically discover racism within North Carolina laws. The project will make North Carolina legal history accessible to researchers by creating a corpus of over one hundred years of North Carolina public, private, and local session laws and resolutions from the end of civil war through the civil rights movement (1865-1968). This project represents the intersection of many subject areas including critical race theory, American history, text and data mining, machine learning, and public humanities. It will be of interest to subject specialists, data science researchers, educators, librarians, and other information professionals.",NKELBER@GMAIL.COM,Long Presentation
"Hunter, Elizabeth",San Francisco State University,Bitter Wind: Adapting Greek Tragedy for Spatial Computing,"spatial computing, theatre studies, Greek tragedy, mixed reality, interactivity","English
North America
BCE-4th Century
Contemporary
meta-criticism (reflections on digital humanities and humanities computing)
virtual and augmented reality creation, systems, and analysis
Literary studies
Performance Studies: Dance, Theatre",English,North America,"BCE-4th Century
Contemporary","meta-criticism (reflections on digital humanities and humanities computing)
virtual and augmented reality creation, systems, and analysis","Literary studies
Performance Studies: Dance, Theatre","<p>This presentation will report on the completed research project <em>Bitter Wind</em>, my adaptation of an ancient Greek tragedy for Microsoft’s spatial computing HoloLens headset. The presentation will demonstrate how emerging embodied technologies like spatial computing (1) offer new possibilities for qualitative interpretation and (2) promote greater theatre studies engagement with digital humanities. In addition to outlining <em>Bitter Wind’s</em> technological elements, the presentation will explain how I integrated the affordances of spatial computing with theories of audience participation to mount a new dramaturgical and historiographic analysis of a canonical source. This presentation will also argue that a stronger theatre studies presence in DH has become urgent, as DH inquiry begins to expand from two-dimensional screens to the immersive, 360° environment created by embodied technologies like augmented/mixed/virtual reality, wearables, and the Internet of Things.</p>
","This presentation will report on the completed research project Bitter Wind, my adaptation of an ancient Greek tragedy for Microsoft’s spatial computing HoloLens headset. The presentation will demonstrate how emerging embodied technologies like spatial computing (1) offer new possibilities for qualitative interpretation and (2) promote greater theatre studies engagement with digital humanities. In addition to outlining Bitter Wind’s technological elements, the presentation will explain how I integrated the affordances of spatial computing with theories of audience participation to mount a new dramaturgical and historiographic analysis of a canonical source. This presentation will also argue that a stronger theatre studies presence in DH has become urgent, as DH inquiry begins to expand from two-dimensional screens to the immersive, 360° environment created by embodied technologies like augmented/mixed/virtual reality, wearables, and the Internet of Things.",ebh@sfsu.edu,Short Presentation
"Blackwell, Christopher William;
Palladino, Chiara;
Greico, MacKense;
Bolton, Allie","Furman University, United States of America",DUCAT: Passage/Translation Alignment with the CITE Architecture,"citation, translation, prose, poetry, alignment","Comparative (2 or more geographical areas)
Europe
English
North America
Contemporary
linked (open) data
semantic analysis
Literary studies
Translation studies",English,"Comparative (2 or more geographical areas)
Europe
North America",Contemporary,"linked (open) data
semantic analysis","Literary studies
Translation studies","<p>DUCAT is a tool that uses the CITE Architecture, allowing alignments among texts. It is a zero-infrastructure HTML app that exports data in a plain-text CEX format. It allows two kinds of alignments among any number of texts, in any language:</p>
<p>**Citation Alignment** A poem, cited line-by-line, is translated into prose, cited by paragraph. A paragraph of the translation might correspond to several lines of poetry; a range of lines of poetry might overlap with seveal paragraphs of the prose translation.</p>
<p>**Translation Alignment** A text and its translation will not align word-by-word, but will inevitably consist of one-to-one, one-to-zero, many-to-one, one-to-many, many-to-many, many-to-zero, or zero-to-many alignments. In the case of ""many"" the words may be discontiguous. When we align more than one translation with an original text, these problems multiply.</p>
<p>While DUCAT is a standalone tool, the data it produces is generic, plain-text, and can be re-imported, shared, and reused.</p>
","DUCAT is a tool that uses the CITE Architecture, allowing alignments among texts. It is a zero-infrastructure HTML app that exports data in a plain-text CEX format. It allows two kinds of alignments among any number of texts, in any language:
**Citation Alignment** A poem, cited line-by-line, is translated into prose, cited by paragraph. A paragraph of the translation might correspond to several lines of poetry; a range of lines of poetry might overlap with seveal paragraphs of the prose translation.
**Translation Alignment** A text and its translation will not align word-by-word, but will inevitably consist of one-to-one, one-to-zero, many-to-one, one-to-many, many-to-many, many-to-zero, or zero-to-many alignments. In the case of ""many"" the words may be discontiguous. When we align more than one translation with an original text, these problems multiply.
While DUCAT is a standalone tool, the data it produces is generic, plain-text, and can be re-imported, shared, and reused.","cwblackwell@gmail.com, chiara.palladino@furman.edu, mackense.greico@furman.edu, allie.bolton@furman.edu",Short Presentation
"Bermúdez Sabel, Helena;
Dell'Oro, Francesca;
Marongiu, Paola",Université de Lausanne,Visualization of semantic shifts: the case of modal markers,"Latin, Diachronic linguistics, Modality, Semantic maps, Visualization","Global
Europe
English
BCE-4th Century
Contemporary
data modeling
semantic analysis
Humanities computing
Linguistics",English,"Global
Europe","BCE-4th Century
Contemporary","data modeling
semantic analysis","Humanities computing
Linguistics","<p dir=""ltr"">This proposal examines the importance of visual representations to convey semantic shifts. This study stems from a project studying modality in Latin from a diachronic perspective; thus, we sketch this proposal with a particular use case in mind: the development of modal maps for Latin. We review a brief portion of the literature on this topic in order to present the proposal we will use as a model. The discussion leads to the presentation of the critical aspects that our own visualization must address and how, by building on previous models, we introduce various enhancements.</p>
","This proposal examines the importance of visual representations to convey semantic shifts. This study stems from a project studying modality in Latin from a diachronic perspective; thus, we sketch this proposal with a particular use case in mind: the development of modal maps for Latin. We review a brief portion of the literature on this topic in order to present the proposal we will use as a model. The discussion leads to the presentation of the critical aspects that our own visualization must address and how, by building on previous models, we introduce various enhancements.","helena.bermudezsabel@unil.ch, francesca.delloro@unil.ch, paola.marongiu@unil.ch",Poster
"Smithies, James (1);
Balaawi, Fadi (2);
Flohr, Pascal (3);
Rababeh, Shaher (2);
Idwan, Sahar (2);
Palmer, Carol (4);
Mubaideen, Shatha (4);
Esposito, Alessandra (1);
Ciula, Arianna (1)","1: King's College London;
2: Hashemite University;
3: University of Oxford;
4: Council for British Research in the Levant",MaDiH (مديح): Mapping Digital Cultural Heritage in Jordan,"Jordan, archaeology, infrastructure, data, analysis","Asia
English
BCE-4th Century
5th-14th Century
15th-17th Century
data publishing projects, systems, and methods
database creation, management, and analysis
Archaeology
Library & information science",English,Asia,"BCE-4th Century
5th-14th Century
15th-17th Century","data publishing projects, systems, and methods
database creation, management, and analysis","Archaeology
Library & information science","<p>MaDiH (مديح): Mapping Digital Cultural Heritage in Jordan, is a collaborative project between King’s Digital Lab (KDL), the Hashemite University, the Council for British Research in the Levant (CBRL), the Department of Antiquities of Jordan, the Jordanian Open Source Association, and the Endangered Archaeology in the Middle East and North Africa (EAMENA) project. It is scheduled to run for two years, from February 2019 - February 2021. The project will contribute to the long-term sustainable development of Jordan’s digital cultural heritage, identifying key systems, datasets, standards, and policies, and aligning them to government digital infrastructure capabilities and strategies. Defining a robust technical and operational architecture for digital cultural heritage will assist the Department of Antiquities in their planning processes, help product development teams develop their systems, facilitate the aggregation of valuable datasets held in disparate repositories, and ensure data generated from research activity is properly stored and widely accessible.</p>
","MaDiH (مديح): Mapping Digital Cultural Heritage in Jordan, is a collaborative project between King’s Digital Lab (KDL), the Hashemite University, the Council for British Research in the Levant (CBRL), the Department of Antiquities of Jordan, the Jordanian Open Source Association, and the Endangered Archaeology in the Middle East and North Africa (EAMENA) project. It is scheduled to run for two years, from February 2019 - February 2021. The project will contribute to the long-term sustainable development of Jordan’s digital cultural heritage, identifying key systems, datasets, standards, and policies, and aligning them to government digital infrastructure capabilities and strategies. Defining a robust technical and operational architecture for digital cultural heritage will assist the Department of Antiquities in their planning processes, help product development teams develop their systems, facilitate the aggregation of valuable datasets held in disparate repositories, and ensure data generated from research activity is properly stored and widely accessible.","james.smithies@kcl.ac.uk, fadi.balaawi@hu.edu.jo, pascal.flohr@arch.ox.ac.uk, srababeh@hu.edu.jo, sahar@hu.edu.jo, director@bi-amman.org.uk, shathamubaideen@bi-amman.org.uk, alessandra.g.esposito@kcl.ac.uk, arianna.ciula@kcl.ac.uk",Short Presentation
"Helling, Patrick","University of Cologne, Germany",Modelling Consultation Workflows for Research Data Management in the Humanities ,"Research Data, Research Data Management, RDM, Data Center, Workflow","Europe
English
Contemporary
digital research infrastructures development and analysis
sustainable procedures, systems, and methods
Library & information science",English,Europe,Contemporary,"digital research infrastructures development and analysis
sustainable procedures, systems, and methods",Library & information science,"<p><strong>Abstract</strong><br />Research data management (RDM) is becoming more important for researchers and research institutions like universities and non-university centers. Due to the heterogeneity of methods in the Humanities, RDM in the Humanities is very complex. Several data centers and contact points have been established to handle this complexity, support researchers and conduct active RDM. Nevertheless, there are few best practices and policies on how RDM is done in a sustainable and goal-oriented way. In this paper I present a consultation-protocol-template we developed at the Data Center for the Humanities (DCH) at the Faculty of Arts and Humanities at the University of Cologne (UoC) to structure and measure RDM in a comprehensive way. I illustrate generic RDM-consultation-workflows developed on the basis of our consultations.</p>
<p><strong>1.   Introduction and Background</strong> <br />Research data is an essential ingredient and facilitator of scientific progress across all disciplines and includes heterogenous data (Bryant, Lavoie, Malpas, 2017). The developments of the European Open Science Cloud (EOSC) and OpenAIRE, and the plans to establish a national research data infrastructure (NFDI) for the whole German research landscape (RfII, 2016) illustrate that the relevance of RDM has reached a high political level, nationally and internationally.<sup>1 </sup>However, this is a rather new development and practical implementation has not yet reached maturity.</p>
<p>The Data Center for the Humanities (DCH) is the first contact point for researchers in the Humanities at the University of Cologne (UoC) on questions concerning we (Blumtritt, Helling, Mathiak et. al., 2018).<sup>2</sup> Through open consultation hours the DCH advise researchers at the Faculty and beyond, attend to their projects and support them as far as RDM is concerned. In addition, we run the Language Archive Cologne (LAC), a repository for audio-visual data, we do active data management and curate, archive and publish research data.<sup>4</sup><br />In this paper, I seek to further the professionalization of RDM, by looking closer at the consultation process and the experiences made with it at our institution in 6 years of consultation practice.<br /><strong>2.   RDM consultation protocols</strong> <br />In order to identify goal-oriented solutions for RDM-needs and as a basis for active RDM, we developed a consultation-workflow (see Figure 1). Through semi-structured interviews and structured consulting protocols, we are offering sustainable RDM-services and -solutions (Helling, Blumtritt and Mathiak, 2018). With more than 70 consulting protocols, the DCH has a wealth of knowledge.</p>
<p>Figure 1: The core consulting-workflow at the DCH.</p>
<p>On the basis of a comparative analysis of the structured consulting protocols, I was able to identify the core information required for active RDM. The information can be subsumed under the categories (red) context information of a process, (yellow) RDM request-categories with a concrete description of the categories in the context of a consultation and (green) recommendations (see Figure 2).</p>
<p>Figure 2: anonymous example of the consulting protocol template: (A) consists of meta information about a process, (B) and (C) are both consulting protocols of a process.</p>
<p><strong>3.   Modelling of RDM-consultation workflows</strong><br />In 2019, I identified 28 demand-categories in 36 consultations. I started modelling generic workflows for each demand-category fitting with every consultation, where the demand was given with the help of the Business Process Model and Notation (BPMN).<sup>4</sup> I will only illustrate on two examples in this abstract, but plan to show more in the presentation.</p>
<p><strong>3.1.   Support with a proposal</strong><br />Many funding institutions ask for statements on how researchers will deal with the digital research data representing the output of an applied project. If a consultation is needed in such a case, we conduct an interview with the researchers and try to receive as much information as possible about the project, which we translate into our consultation-protocol-template.</p>
<p>Figure 3: Consultation workflow for support with a proposal.</p>
<p>Based on the information in the protocol, we then follow the steps as detailed in Fig. 3, always taking the context information into account. Based on similar cases, we suggest a text to describe possible strategies for data management and define fundamental steps of data handling, archiving and publishing.<br />These formulations get integrated into the overall application by the researchers and thus become part of the proposal.</p>
<p><strong>3.2.   Data archiving</strong><br />Another common request is the archiving (see Figure 4) of data at the end of a project. After the generic steps of a workflow described already, we start to search for a fitting repository with respect to the conditions of the actual data and the whole project.</p>
<p>Figure 4: Consultation workflow for data archiving.</p>
<p>We developed a system of priorities that we work through step by step: at first, we check if we can identify a (1) domain-specific repository that fits with the given needs and conditions. In our case, this can be the LAC, or an external repository. If there is no solution, we check if there is a fitting (2) generic repository. In our case, this can only be an external repository, but, in the context of another data center, it could be a generic repository driven by the data center.</p>
<p>In any case the results of the investigation need to be passed to the researchers: either we mediate a contact to an external repository, or we present information about the conditions of archiving data in the LAC.</p>
<p><strong>Conclusion</strong> <br />The formal description of our workflows optimizes both consultations and active RDM. The workflows follow acknowledged standards as well as know-how and experiences made by data managers since 2013. Thanks to these procedures, we are able to deal with the diversity of RDM-requirements in the Humanities in an effective and standardized way and we can handle different consultations with the same or a similar request equally. This makes RDM measurable and comprehensible. Because of the abstract level of modelling, the workflows are generic and can be adapted in different contexts.<br />In my talk, I will present more workflow models and in a more detailed way. Contextually, I will define RDM-requirements and -services based on active data management in a comprehensive way and show synergies between different needs and service structures. This will lead me to a plea for the necessity of measurable RDM-structures based on RDM done in expert centers by data manager, as opposed to the common approach of building and establishing contact points and training a mediating data manager based on recommendations of political and scientific committees and umbrella organizations that are not closely related to daily research and RDM.</p>
<p><strong>Footnotes</strong></p>
<p><sup><sup>[1]</sup></sup> European Open Science Cloud (EOSC), Online: https://ec.europa.eu/research/openscience/index.cfm?pg=open-science-cloud (last request: 2nd October 2019); OpenAIRE, Online: https://www.openaire.eu/ (last request: 2nd October 2019); National Research Data Infrastructure (NFDI), Online: https://www.dfg.de/foerderung/programme/nfdi/ (last request: 2nd October 2019).</p>
<p><sup>[2]</sup> Data Center for the Humanities (DCH), Online: http://dch.phil-fak.uni-koeln.de/index.html (last request: 2nd October 2019).</p>
<p><sup><sup>[3]</sup></sup> Language Archive Cologne (LAC), Online: https://lac.uni-koeln.de/ (last request: 2nd October 2019).</p>
<p><sup><sup>[4]</sup></sup> cf. Silvere, Bruce. <em>BPMN method and style.</em> Cody-Cassidy Press, Aptos, CA 2009.</p>
<p><strong>Bibliography</strong><br />Blumtritt, J., Helling, P., Mathiak, B. et. al. (2018): Forschungsdatenmanagement in den Geisteswissenschaften an der Universität zu Köln, in: o-bib, Das offene Bibliotheksjournal, P. 104-117. DOI: https://doi.org/10.5282/o-bib/2018H3S104-117.<br />Bryant, R., Lavoie, B., Malpas, C. (2017): A Tour of the Research Data Management (RDM) Service Space. The Realities of Research Data Management, Part 1. Dublin, Ohio: OCLC Research. DOI: https://doi.org/10.25333/C3PG8J.<br />Helling, P., Blumtritt, J., Mathiak, B. (2018): Der Beratungs-workflow des Data Center for the Humanities an der Universität zu Köln, in: o-bib, Das offene Bibliotheksjournal, P. 248-261. DOI: https://doi.org/10.5282/o-bib/2018H4S248-261. <br />RfII (2016): Leistung aus Vielfalt. Empfehlungen zu Strukturen, Prozessen und Finanzierung des Forschungsdatenmanagements in Deutschland, Göttingen, 160 S. Online: http://www.rfii.de/?p=1998 (last request: 2nd of October 2019).</p>
","Abstract
Research data management (RDM) is becoming more important for researchers and research institutions like universities and non-university centers. Due to the heterogeneity of methods in the Humanities, RDM in the Humanities is very complex. Several data centers and contact points have been established to handle this complexity, support researchers and conduct active RDM. Nevertheless, there are few best practices and policies on how RDM is done in a sustainable and goal-oriented way. In this paper I present a consultation-protocol-template we developed at the Data Center for the Humanities (DCH) at the Faculty of Arts and Humanities at the University of Cologne (UoC) to structure and measure RDM in a comprehensive way. I illustrate generic RDM-consultation-workflows developed on the basis of our consultations.
1.   Introduction and Background 
Research data is an essential ingredient and facilitator of scientific progress across all disciplines and includes heterogenous data (Bryant, Lavoie, Malpas, 2017). The developments of the European Open Science Cloud (EOSC) and OpenAIRE, and the plans to establish a national research data infrastructure (NFDI) for the whole German research landscape (RfII, 2016) illustrate that the relevance of RDM has reached a high political level, nationally and internationally.1 However, this is a rather new development and practical implementation has not yet reached maturity.
The Data Center for the Humanities (DCH) is the first contact point for researchers in the Humanities at the University of Cologne (UoC) on questions concerning we (Blumtritt, Helling, Mathiak et. al., 2018).2 Through open consultation hours the DCH advise researchers at the Faculty and beyond, attend to their projects and support them as far as RDM is concerned. In addition, we run the Language Archive Cologne (LAC), a repository for audio-visual data, we do active data management and curate, archive and publish research data.4
In this paper, I seek to further the professionalization of RDM, by looking closer at the consultation process and the experiences made with it at our institution in 6 years of consultation practice.
2.   RDM consultation protocols 
In order to identify goal-oriented solutions for RDM-needs and as a basis for active RDM, we developed a consultation-workflow (see Figure 1). Through semi-structured interviews and structured consulting protocols, we are offering sustainable RDM-services and -solutions (Helling, Blumtritt and Mathiak, 2018). With more than 70 consulting protocols, the DCH has a wealth of knowledge.
Figure 1: The core consulting-workflow at the DCH.
On the basis of a comparative analysis of the structured consulting protocols, I was able to identify the core information required for active RDM. The information can be subsumed under the categories (red) context information of a process, (yellow) RDM request-categories with a concrete description of the categories in the context of a consultation and (green) recommendations (see Figure 2).
Figure 2: anonymous example of the consulting protocol template: (A) consists of meta information about a process, (B) and (C) are both consulting protocols of a process.
3.   Modelling of RDM-consultation workflows
In 2019, I identified 28 demand-categories in 36 consultations. I started modelling generic workflows for each demand-category fitting with every consultation, where the demand was given with the help of the Business Process Model and Notation (BPMN).4 I will only illustrate on two examples in this abstract, but plan to show more in the presentation.
3.1.   Support with a proposal
Many funding institutions ask for statements on how researchers will deal with the digital research data representing the output of an applied project. If a consultation is needed in such a case, we conduct an interview with the researchers and try to receive as much information as possible about the project, which we translate into our consultation-protocol-template.
Figure 3: Consultation workflow for support with a proposal.
Based on the information in the protocol, we then follow the steps as detailed in Fig. 3, always taking the context information into account. Based on similar cases, we suggest a text to describe possible strategies for data management and define fundamental steps of data handling, archiving and publishing.
These formulations get integrated into the overall application by the researchers and thus become part of the proposal.
3.2.   Data archiving
Another common request is the archiving (see Figure 4) of data at the end of a project. After the generic steps of a workflow described already, we start to search for a fitting repository with respect to the conditions of the actual data and the whole project.
Figure 4: Consultation workflow for data archiving.
We developed a system of priorities that we work through step by step: at first, we check if we can identify a (1) domain-specific repository that fits with the given needs and conditions. In our case, this can be the LAC, or an external repository. If there is no solution, we check if there is a fitting (2) generic repository. In our case, this can only be an external repository, but, in the context of another data center, it could be a generic repository driven by the data center.
In any case the results of the investigation need to be passed to the researchers: either we mediate a contact to an external repository, or we present information about the conditions of archiving data in the LAC.
Conclusion 
The formal description of our workflows optimizes both consultations and active RDM. The workflows follow acknowledged standards as well as know-how and experiences made by data managers since 2013. Thanks to these procedures, we are able to deal with the diversity of RDM-requirements in the Humanities in an effective and standardized way and we can handle different consultations with the same or a similar request equally. This makes RDM measurable and comprehensible. Because of the abstract level of modelling, the workflows are generic and can be adapted in different contexts.
In my talk, I will present more workflow models and in a more detailed way. Contextually, I will define RDM-requirements and -services based on active data management in a comprehensive way and show synergies between different needs and service structures. This will lead me to a plea for the necessity of measurable RDM-structures based on RDM done in expert centers by data manager, as opposed to the common approach of building and establishing contact points and training a mediating data manager based on recommendations of political and scientific committees and umbrella organizations that are not closely related to daily research and RDM.
Footnotes
[1] European Open Science Cloud (EOSC), Online: https://ec.europa.eu/research/openscience/index.cfm?pg=open-science-cloud (last request: 2nd October 2019); OpenAIRE, Online: https://www.openaire.eu/ (last request: 2nd October 2019); National Research Data Infrastructure (NFDI), Online: https://www.dfg.de/foerderung/programme/nfdi/ (last request: 2nd October 2019).
[2] Data Center for the Humanities (DCH), Online: http://dch.phil-fak.uni-koeln.de/index.html (last request: 2nd October 2019).
[3] Language Archive Cologne (LAC), Online: https://lac.uni-koeln.de/ (last request: 2nd October 2019).
[4] cf. Silvere, Bruce. BPMN method and style. Cody-Cassidy Press, Aptos, CA 2009.
Bibliography
Blumtritt, J., Helling, P., Mathiak, B. et. al. (2018): Forschungsdatenmanagement in den Geisteswissenschaften an der Universität zu Köln, in: o-bib, Das offene Bibliotheksjournal, P. 104-117. DOI: https://doi.org/10.5282/o-bib/2018H3S104-117.
Bryant, R., Lavoie, B., Malpas, C. (2017): A Tour of the Research Data Management (RDM) Service Space. The Realities of Research Data Management, Part 1. Dublin, Ohio: OCLC Research. DOI: https://doi.org/10.25333/C3PG8J.
Helling, P., Blumtritt, J., Mathiak, B. (2018): Der Beratungs-workflow des Data Center for the Humanities an der Universität zu Köln, in: o-bib, Das offene Bibliotheksjournal, P. 248-261. DOI: https://doi.org/10.5282/o-bib/2018H4S248-261. 
RfII (2016): Leistung aus Vielfalt. Empfehlungen zu Strukturen, Prozessen und Finanzierung des Forschungsdatenmanagements in Deutschland, Göttingen, 160 S. Online: http://www.rfii.de/?p=1998 (last request: 2nd of October 2019).",patrick.helling@uni-koeln.de,Short Presentation
"Meneses, Luis (1);
Martin, Jonathan (2);
Furuta, Richard (1);
Siemens, Ray (3)","1: Electronic Textual Cultures Lab, University of Victoria;
2: King’s College London;
3: Center for the Study of Digital Libraries, Texas A&M University",Analyzing Link Topology to Quantify the Degree of Planned Obsolesce in Online Digital Humanities Projects,"degradation, abandonment, preservation, online digital humanities projects","Global
English
Contemporary
data, object, and artefact preservation
digital archiving
Computer science
Humanities computing",English,Global,Contemporary,"data, object, and artefact preservation
digital archiving","Computer science
Humanities computing","<p>Many of the online projects in the digital humanities have an implied planned obsolesce –which means that they will degrade over time once they cease to receive updates in their content and software libraries (Fitzpatrick 2011). We presented papers at Digital Humanities 2017, 2018, and 2019 that explored the abandonment and the average lifespan of online projects in the digital humanities (Meneses and Furuta 2017), contrasted how things have changed over the course of a year (Meneses et al. 2018), and introduced a strategy for preservation by creating standalone software executables (Meneses et al. 2019). However, managing and characterizing the degradation of online digital humanities projects is a complex and pressing problem that demands further analysis.</p>
<p>In this sense “planned obsolescence” is a nuanced designation —as there are many cases of successful projects in digital humanities that are shifting their focus from active development to data management (for example: http://cervantes.dh.tamu.edu). These are cases where a project’s online presence has not received updates for some time but its online tools are stable and continue to be accessed by its users. However, if updates are not applied to the infrastructure or content of a project over time web requests will eventually start generating errors on the server or the client —affecting the overall user experience (Nowviskie and Porter 2010). These are examples of why the rules for traditional resources do not fully apply and new metrics are needed to identify issues concerning online projects in the digital humanities.</p>
<p>In this study we dive deeper into exploring the distinctive signs of abandonment to quantify the planned obsolesce of online digital humanities projects. In our workflow, we use each project included in the Book of Abstracts that is published after each Digital Humanities conference from 2006 to 2019. We then proceed to periodically create a set of WARC files for each project, which are processed using Python (van Rossum 1995) and Apache Spark (Apache Software Foundation 2017) to statistically analyze the retrieved HTTP response codes, number of redirects, DNS metadata and detailed examination of the contents and links returned by traversing the base node. This combination of metrics and techniques has allowed us to assess the degree of change of a project over time. As one of the results from our 2019 presentation, we claimed that the most important signature for degradation comes from the assessing the validity and overall health of the topology of links in a project. Thus, the focus of our study is analyzing this key signature.</p>
<p>We acknowledge that research on the preservation of projects in the digital humanities is also carried out by other groups (Larrousse and Marchand 2019) (Arneil, Holmes, and Newton 2019). However, our study is different as it focuses on two points: first, identifying the signals of abandoned projects using computational methods; and second, quantifying their degree of abandonment. In the end, we intend this study to be a step forward towards better preservation strategies for the planned obsolesce of online digital humanities projects.</p>
<p><strong>References</strong></p>
<p>Apache Software Foundation. 2017. “Apache Spark: Lightning-Fast Cluster Computing.” April 11. http://spark.apache.org.</p>
<p>Arneil, Stewart, Martin Holmes, and Greg Newton. 2019. “Project Endings: Early Impressions From Our Recent Survey On Project Longevity In DH.” presented at the Digital Humanities 2019, Utrecht, The Netherlands, July 9.</p>
<p>Fitzpatrick, Kathleen. 2011. <em>Planned Obsolescence: Publishing, Technology, and the Future of the Academy</em>. New York: NYU Press.</p>
<p>Larrousse, Nicolas, and Joel Marchand. 2019. “A Techno-Human Mesh for Humanities in France: Dealing with Preservation Complexity.” presented at the Digital Humanities 2019, Utrecht, The Netherlands, July 9.</p>
<p>Meneses, Luis, and Richard Furuta. 2017. “Shelf Life: Identifying the Abandonment of Online Digital Humanities Projects.” presented at the Digital Humanities 2017, Montreal, Canada, August 8.</p>
<p>Meneses, Luis, Jonathan Martin, Richard Furuta, and Ray Siemens. 2018. “Part Deux: Exploring the Signs of Abandonment of Online Digital Humanities Projects.” presented at the Digital Humanities 2018, Mexico City, June 26.</p>
<p>———. 2019. “A Framework to Quantify the Signs of Abandonment in Online Digital Humanities Projects.” presented at the Digital Humanities 2019, Utrecht, The Netherlands, July 9.</p>
<p>Nowviskie, Bethany, and Dot Porter. 2010. “The Graceful Degradation Survey: Managing Digital Humanities Projects Through Times of Transition and Decline.” presented at the 2010 Conference of the Alliance of Digital Humanities Organizations. http://dh2010.cch.kcl.ac.uk/academic-programme/abstracts/papers/html/ab-722.html.</p>
<p>van Rossum, Guido. 1995. “Python Tutorial, Technical Report CS-R9526.” Amsterdam: Centrum voor Wikunde en Informatica (CWI). https://ir.cwi.nl/pub/5007/05007D.pdf.</p>
","Many of the online projects in the digital humanities have an implied planned obsolesce –which means that they will degrade over time once they cease to receive updates in their content and software libraries (Fitzpatrick 2011). We presented papers at Digital Humanities 2017, 2018, and 2019 that explored the abandonment and the average lifespan of online projects in the digital humanities (Meneses and Furuta 2017), contrasted how things have changed over the course of a year (Meneses et al. 2018), and introduced a strategy for preservation by creating standalone software executables (Meneses et al. 2019). However, managing and characterizing the degradation of online digital humanities projects is a complex and pressing problem that demands further analysis.
In this sense “planned obsolescence” is a nuanced designation —as there are many cases of successful projects in digital humanities that are shifting their focus from active development to data management (for example: http://cervantes.dh.tamu.edu). These are cases where a project’s online presence has not received updates for some time but its online tools are stable and continue to be accessed by its users. However, if updates are not applied to the infrastructure or content of a project over time web requests will eventually start generating errors on the server or the client —affecting the overall user experience (Nowviskie and Porter 2010). These are examples of why the rules for traditional resources do not fully apply and new metrics are needed to identify issues concerning online projects in the digital humanities.
In this study we dive deeper into exploring the distinctive signs of abandonment to quantify the planned obsolesce of online digital humanities projects. In our workflow, we use each project included in the Book of Abstracts that is published after each Digital Humanities conference from 2006 to 2019. We then proceed to periodically create a set of WARC files for each project, which are processed using Python (van Rossum 1995) and Apache Spark (Apache Software Foundation 2017) to statistically analyze the retrieved HTTP response codes, number of redirects, DNS metadata and detailed examination of the contents and links returned by traversing the base node. This combination of metrics and techniques has allowed us to assess the degree of change of a project over time. As one of the results from our 2019 presentation, we claimed that the most important signature for degradation comes from the assessing the validity and overall health of the topology of links in a project. Thus, the focus of our study is analyzing this key signature.
We acknowledge that research on the preservation of projects in the digital humanities is also carried out by other groups (Larrousse and Marchand 2019) (Arneil, Holmes, and Newton 2019). However, our study is different as it focuses on two points: first, identifying the signals of abandoned projects using computational methods; and second, quantifying their degree of abandonment. In the end, we intend this study to be a step forward towards better preservation strategies for the planned obsolesce of online digital humanities projects.
References
Apache Software Foundation. 2017. “Apache Spark: Lightning-Fast Cluster Computing.” April 11. http://spark.apache.org.
Arneil, Stewart, Martin Holmes, and Greg Newton. 2019. “Project Endings: Early Impressions From Our Recent Survey On Project Longevity In DH.” presented at the Digital Humanities 2019, Utrecht, The Netherlands, July 9.
Fitzpatrick, Kathleen. 2011. Planned Obsolescence: Publishing, Technology, and the Future of the Academy. New York: NYU Press.
Larrousse, Nicolas, and Joel Marchand. 2019. “A Techno-Human Mesh for Humanities in France: Dealing with Preservation Complexity.” presented at the Digital Humanities 2019, Utrecht, The Netherlands, July 9.
Meneses, Luis, and Richard Furuta. 2017. “Shelf Life: Identifying the Abandonment of Online Digital Humanities Projects.” presented at the Digital Humanities 2017, Montreal, Canada, August 8.
Meneses, Luis, Jonathan Martin, Richard Furuta, and Ray Siemens. 2018. “Part Deux: Exploring the Signs of Abandonment of Online Digital Humanities Projects.” presented at the Digital Humanities 2018, Mexico City, June 26.
———. 2019. “A Framework to Quantify the Signs of Abandonment in Online Digital Humanities Projects.” presented at the Digital Humanities 2019, Utrecht, The Netherlands, July 9.
Nowviskie, Bethany, and Dot Porter. 2010. “The Graceful Degradation Survey: Managing Digital Humanities Projects Through Times of Transition and Decline.” presented at the 2010 Conference of the Alliance of Digital Humanities Organizations. http://dh2010.cch.kcl.ac.uk/academic-programme/abstracts/papers/html/ab-722.html.
van Rossum, Guido. 1995. “Python Tutorial, Technical Report CS-R9526.” Amsterdam: Centrum voor Wikunde en Informatica (CWI). https://ir.cwi.nl/pub/5007/05007D.pdf.","ldmm@uvic.ca, jonathan.d.martin@kcl.ac.uk, furuta@cse.tamu.edu, siemens@uvic.ca",Poster
"Miller, Yitzchak;
Prebor, Gila","Bar Ilan University, Israel",From Metadata to linked Open Data and Wikidata : Yemenite Hebrew Manuscripts and Wikidata,"linked Open Data, Wikidata, Hebrew Manuscripts, Metadata","Asia
English
Contemporary
linked (open) data
manuscripts description, representation, and analysis
Humanities computing
Library & information science",English,Asia,Contemporary,"linked (open) data
manuscripts description, representation, and analysis","Humanities computing
Library & information science","<p>The Hebrew Manuscripts catalogue in the National Library of Israel provides metadata for most Hebrew manuscripts in the world.  Traditionally, library catalogues have served as a tool to manage library collections causing library catalogues to be data silos. In order to break down these metadata silos, information must be accessible and free. The semantic web, and in particular, linked open data, are initiatives that can turn library catalogues into a real part of the Internet. In order to explore the potential of linked data to this type of data we plan to convert a small part of the catalogue’s metadata of Hebrew Manuscripts in the NLI to Wikidata and create Wikidata items for each of the chosen manuscripts. We hope this will lead to an enrichment of the data and easy access to tools for querying and visualizing the collection. As a case study we have chosen Yemenite script manuscripts. </p>
","The Hebrew Manuscripts catalogue in the National Library of Israel provides metadata for most Hebrew manuscripts in the world. Traditionally, library catalogues have served as a tool to manage library collections causing library catalogues to be data silos. In order to break down these metadata silos, information must be accessible and free. The semantic web, and in particular, linked open data, are initiatives that can turn library catalogues into a real part of the Internet. In order to explore the potential of linked data to this type of data we plan to convert a small part of the catalogue’s metadata of Hebrew Manuscripts in the NLI to Wikidata and create Wikidata items for each of the chosen manuscripts. We hope this will lead to an enrichment of the data and easy access to tools for querying and visualizing the collection. As a case study we have chosen Yemenite script manuscripts.","Isaac.Miller@biu.ac.il, gila.prebor@biu.ac.il",Poster
"Ciula, Arianna;
Caton, Paul;
Ferraro, Ginestra;
Maher, Brian;
Noël, Geoffroy;
Vieira, Miguel","King's College London, United Kingdom",The place of models and modelling in Digital Humanities: Intersections with a Research Software Engineering perspective,"Research Software Engineering, models, critical modelling","Comparative (2 or more geographical areas)
English
Contemporary
data modeling
project design, organization, management
Humanities computing",English,Comparative (2 or more geographical areas),Contemporary,"data modeling
project design, organization, management",Humanities computing,"<p>This paper<sup>1</sup> aims to bridge Digital Humanities (DH) and Research Software Engineering (RSE) communities. It argues that the production of models is the core contribution of RSE to the epistemology of DH. We adopt an inclusive definition of models and modelling (see Ciula et al. 2018) which spans the whole range from ‘deformative’ to empirical modelling (see Smithies 2017: 168), including formal or predictive modelling (Joslyn and Turchin 1993), and the technical solutions produced in the process as well as the know-how, languages and documentation which accompany this production. From this wide perspective, models are also artefacts which can be studied across the history of science and of the humanities tradition (see Bod 2018) and in comparison with other modelling practices in science. RSE practice is grounded on a strong conscience of the experimental apparatus and the iterative critique of models built (and often deflated) for a purpose. The challenge is to recognise the idiosyncrasy and situatedness of modelling practices and artefacts while devising methods to expose the scalability of the underlying workflows and modelling processes.</p>
<p>We reflect on the epistemology of DH from the practical perspective of our RSE lab - King’s Digital Lab (KDL)<sup>2</sup> - and the research processes embedded in our Software Development Lifecycle (fig.1). The human element is at the core of the technical ecosystem we research and operate in. We acknowledge that KDL models and modelling are co-constitutive of human expertise, technical systems and operational methods, all aspiring to an environment conducive of open knowledge. This is not only in terms of development and management approaches via adoption of open standards, open source and exposure of open data but also, more fundamentally, in sharing (achievements and struggles around) our processes and in promoting open models. We will use project-specific examples, including data modelling and knowledge representation practices, to demonstrate how some of our research into model-making processes challenge the perception of the technical work of RSE within DH as a stale, mechanistic and uncritical procedural activity.</p>
<p dir=""ltr""><em>Figure 1</em> KDL Software Development Lifecycle (SDLC) (King’s Digital Lab 2019) mapped to Agile DSDM project phases (Agile Business Consortium 2014).</p>
<p dir=""ltr"">KDL operates within a rather unique context. It claims its origins in the pioneering work of colleagues at King’s College London working in applied computing in the Humanities already in the 1970s (see Short et al. 2012). However, the crossings between RSE and DH communities at King’s and internationally are only recently being highlighted and explored (e.g. Gold 2009; Smithies 2019; DHTech Group 2019). We argue that the study and building of models is one of the dimensions via which these crossings emerge more vividly with substantial epistemological implications and innovative ramifications for the field of DH as a whole.</p>
<p dir=""ltr"">The core practice of research in DH is modelling (e.g. cf. McCarty 2005: 20–72; Buzzetti 2002; Beynon and McCarty 2006; Flanders and Jannidis 2015, 2018), which implies the translation of complex systems of knowledge or conceptual frameworks into computationally processable models or operational frameworks.</p>
<p dir=""ltr"">Gooding (2003) claims that in experimental settings computational approaches are analogues to other processes of abstraction, measurement and contextual interpretation, whereby reduction of complexity is followed by expansion in the guise of a double funnel-shaped process (Gooding 2003: fig 13.4). We can trace these processes of reduction and expansion also in the RSE context, where, for example, operationalisation makes models formalised into snippets of code or software components. Other languages than translation into code play a role in the process, however.<sup>3</sup></p>
<p dir=""ltr"">The paper will reflect on models as artefacts of different kind expressed via a variety of languages, including but not limited to computational models, produced during several phases of the SDLC (see Ciula and Smithies, forthcoming) such as:</p>
<ul><li>negotiations around the meaning of the project units of analysis documented in diagrams and definitions which shape requirements and an agreed project language;</li>
<li>paper or whiteboard sketches used to draft the solution architecture for a project in its feasibility assessment;</li>
<li>wireframes and static mockups of user journeys;</li>
<li>data models implementing the logical structure of a database;</li>
<li>statistical models implemented with <em>ad hoc</em> algorithms and code or relying on tested formulas and existing libraries.</li>
</ul><p>While specific instantiations of models (for example in relational databases) can have a rather short life, in the RSE context and projects where they were designed and developed, they often represented innovative solutions which have a longstanding effect. Indeed, while models are temporary pragmatic solutions to address specific project challenges yet, at the scale KDL operates, they are the backbone of the team's tacit knowledge as well as the building blocks towards more generalisable and re-usable approaches. They mediate and bridge the layers of the Lab’s socio-technical system: team expertise, data and technical systems (fig. 2).</p>
<p><em>Figure 2 </em>Multilayered socio-technical system of the Lab where concentric circles denote co-constitution of team expertise, data and technical systems (Ciula and Smithies, forthcoming: fig. 5).</p>
<p dir=""ltr"">While we can refine existing approaches to sustain and expose modelling efforts in SDLC cycles which rely on RSE best practices such as attention to documentation and re-use,<sup>4</sup> there is also room for more innovative approaches, including a design first culture, workflow integration across RSE roles, the assessment and potential adoption of a set of modelling notation languages for the exposure mechanisms for our models.</p>
<p>In alignment with a “critical modelling” approach (see Bode 2020), but also with a material culture and media literacy perspective, in this paper we aim to reflect upon models by looking at the wide epistemological implications of their production and use, at the responsibilities of modellers,<sup>5</sup> at how models come to be and what effect they have in the resources they contribute to instantiate and hence in interpretative processes of expansion as well as reduction.</p>
<p><strong>Notes</strong></p>
<p>[1] A preliminary version of this paper was presented at the symposium <em>Computational Text Analysis and Historical Change</em>, held at Humlab, Umeå University (Sweden), 4-6 September 2019.</p>
<p>[2] The authors currently cover different roles at King’s Digital Lab (KDL), a Research Software Engineering (RSE) team hosted by the Faculty of Arts & Humanities at King’s College London, which provides software development and infrastructure to departments in the Faculty of Arts & Humanities while also collaborating with Social Science & Public Policy as well as a range of external partners in the higher education and cultural heritage sectors.</p>
<p>[3] Data modelling, for example, is informed if not driven by communication and collaborative reasoning around more or less standardised graphical representations and notations in phases of reverse engineering as well as design methods. Note that KDL intend and use design methods in a wide sense ranging from techniques of requirements elicitation in pre-project analysis to data modelling and wireframing in evolutionary development (see Bennet et al. 2005). Equally, the re-integration or expansion of modelling efforts into interpretative frameworks usually rely on verbal and visual language to document code, or to explain the results of an experiment (Ciula and Marras 2019: 39).</p>
<p>[4] With this respect see, for example, the KDL checklist for assessment of digital outputs within the UK Research Framework Exercise (Ciula 2019).</p>
<p>[5] Models contribute to define and redefine objects of study which come charged with layers of scholarship and analysis, with previous selections, bias and political as well as ethical responsibilities. As the creators of new memory regimes and intermediaries to the past engaged in modelling efforts which interact and affect the materiality of our objects of study (Ciula 2017a), we bear responsibilities (Ciula 2017b). In line with ongoing discussions around the representativeness and constraints of the digital archive (e.g. Dahlström 2010; Hitchcock 2013; Prescott 2015), Bode (2018, 2020) presented some lucid analysis around modellers’ responsibilities in digital literary studies by exposing the gaps that propagate from produced literary works we know of to material preserved in the analogue archive, to selections of works that make it into digital archives to further reductions in the creation of a corpus of analysis and, last but not least, in the application of statistical modelling techniques which dictate additional powerful yet limiting constraints if not contextualised critically within an interlocked chain of bias.</p>
<p><strong>References</strong></p>
<ul><li dir=""ltr"">
<p dir=""ltr"">Agile Business Consortium. The DSDM Agile Project Framework Handbook. DSDM Consortium, 2014. https://www.agilebusiness.org/resources/dsdm-handbooks.</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">Bennett, Simon, Steve McRobb, and Ray Farmer. Object-Oriented Systems Analysis and Design Using UML. McGraw Hill Higher Education, 2005.</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">Beynon, Meurig, Steve Russ, and Willard McCarty. ‘Human Computing—Modelling with Meaning’. Literary and Linguistic Computing 21, no. 2 (6 January 2006): 141–57. https://doi.org/10.1093/llc/fql015.</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">Bod, Rens. ‘Modelling in the Humanities: Linking Patterns to Principles’. Historical Social Research, Supplement, no. 31 (2018): 78–95. https://doi.org/10.12759/hsr.suppl.31.2018.78-95.</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">Bode, Katherine. A World of Fiction: Digital Collections and the Future of Literary History. University of Michigan Press, 2018.</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">———. ‘Why You Can’t Model Away Bias’. Modern Language Quarterly 81, no. 1 (1 March 2020): 95–124. https://doi.org/10.1215/00267929-7933102. preprint: https://katherinebode.files.wordpress.com/2019/08/mlq2019_preprintbode_why.pdf.</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">Buzzetti, Dino. ‘Digital Representation and the Text Model’. New Literary History 33, no. 1 (2002): 61–88. https://doi.org/10.1353/nlh.2002.0003.</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">Ciula, Arianna. ‘Digital Humanities and Practical Memory: Modelling Textuality’. Estudos Em Comunicação 25 (21 December 2017): 7–17.</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">———. ‘Modelling Textuality: A Material Culture Framework’. In Advances in Digital Scholarly Editing. Papers Presented at the DiXiT Conferences in The Hague, Cologne, and Antwerp, edited by Edited by Peter Boot, Anna Cappellotto, Wout Dillen, Franz Fischer, Aodhán Kelly, Andreas Mertgens, Anna-Maria Sichani, and Elena Spadini & Dirk van Hulle, 91–97. Leiden: Sidestone Press, 2017. https://www.sidestone.com/books/advances-in-digital-scholarly-editing.</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">———. ‘KDL Checklist for Digital Outputs Assessment (Version 2.0)’. Zenodo, 2019. http://doi.org/10.5281/zenodo.3361580.</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">Ciula, Arianna, Øyvind Eide, Cristina Marras, and Patrick Sahle, eds. ‘Models and Modelling between Digital & Humanities: A Multidisciplinary Perspective’. Historical Social Research Supplement, no. 31 (2018). https://doi.org/10.12759/hsr.43.2018.4.343-361.</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">Ciula, Arianna, and Cristina Marras. ‘Exploring a Semiotic Conceptualisation of Modelling in Digital Humanities Practices’. In Meanings & Co: The Interdisciplinarity of Communication, Semiotics and Multimodality, edited by Alin Olteanu, Andrew Stables, and Dimitru Bortun, 6:33–52. Numanities - Arts and Humanities in Progress. Springer International Publishing, 2019. https://www.springer.com/us/book/9783319919850.</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">Ciula, Arianna, and James Smithies. ‘Sustainability and Modelling at King’s Digital Lab: Between Tradition and Innovation’. In On Making in the Digital Humanities: Essays on the Scholarship of Digital Humanities Development in Honour of John Bradley, edited by Julianne Nyhan, Geoffroy Rockwell and Stefan Sinclair. London: University College London Press, Forthcoming.</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">Dahlström, Mats. ‘Critical Editing and Critical Digitisation’. In Text Comparison and Digital Creativity: The Production of Presence and Meaning in Digital Text Scholarship, edited by Wido van Peursen, Ernst D. Thoutenhoofd, and Adriaan van der Weel, 79–97. Brill Academic Pub, 2010.</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">DHTech Group. ‘DH Research Software Engineers - For We Are Many’. DHTech, 2019. https://dh-tech.github.io/dhrse-whitepaper/.</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">Flanders, Julia, and Fotis Jannidis. ‘Knowledge Organization and Data Modeling in the Humanities’. White paper, 2015. http://www.wwp.northeastern.edu/outreach/conference/kodm2012/flanders_jannidis_datamodeling.pdf.</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">———, eds. The Shape of Data in Digital Humanities: Modeling Texts and Text-Based Resources. Digital Research in the Arts and Humanities. London: Routledge, Taylor & Francis Group, 2018.</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">Gold, Nicolas. ‘Service-Oriented Software in the Humanities: A Software Engineering Perspective’. Digital Humanities Quarterly, no. 3.4 (2009). http://digitalhumanities.org:8081/dhq/vol/3/4/000072/000072.html.</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">Gooding, David. ‘Varying the Cognitive Span: Experimentation, Visualization, And Computation’. In The Philosophy of Scientific Experimentation, edited by Hans Radder, 255–84. Pittsburgh, Pa.: University of Pittsburgh Press, 2003.</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">Hitchcock, Tim. ‘Confronting the Digital’. Cultural and Social History 10, no. 1 (1 March 2013): 9–23. https://doi.org/10.2752/147800413X13515292098070.</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">Joslyn, Cliff A., and Valentin Turchin. ‘Model’. In Principia Cybernetica Web, edited by Francis Heylighen, Cliff A. Joslyn, and Valentin Turchin. Brussels, 1993. http://pespmc1.vub.ac.be/MODEL.html.</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">King’s Digital Lab. ‘Frequently Asked Questions for Project Partners | King’s Digital Lab’. King’s Digital Lab, 2019. https://www.kdl.kcl.ac.uk/how-we-work/faq-partners/.</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">———. ‘What Is KDL | King’s Digital Lab’. King’s Digital Lab, 2018. https://www.kdl.kcl.ac.uk/how-we-work/what-is-kdl/.</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">McCarty, Willard. Humanities Computing. Basingstoke [England]; New York: Palgrave Macmillan, 2005.</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">Prescott, Andrew. ‘I’d Rather Be a Librarian: A Response to Tim Hitchcock, “Confronting the Digital”’. Cultural and Social History 11, no. 3 (1 September 2014): 335–41. https://doi.org/10.2752/147800414X13983595303192.</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">Short, Harold, Julianne Nyhan, Anne Welsh, and Jessica Salmon. ‘“Collaboration Must Be Fundamental or It’s Not Going to Work”: An Oral History Conversation between Harold Short and Julianne Nyhan’. Digital Humanities Quarterly 6, no. 3 (2012). http://www.digitalhumanities.org/dhq/vol/6/3/000133/000133.html.</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">Smithies, James. ‘The Continuum Approach to Career Development: Research Software Careers in King’s Digital Lab’. King’s Digital Lab - Thoughts and Reflections from the Lab (blog), 7 February 2019. https://www.kdl.kcl.ac.uk/blog/rse-career-development/.</p>
</li>
<li dir=""ltr"">
<p dir=""ltr"">———. The Digital Humanities and the Digital Modern. Basingstoke: Palgrave Macmillan, 2017.</p>
</li>
</ul>","This paper1 aims to bridge Digital Humanities (DH) and Research Software Engineering (RSE) communities. It argues that the production of models is the core contribution of RSE to the epistemology of DH. We adopt an inclusive definition of models and modelling (see Ciula et al. 2018) which spans the whole range from ‘deformative’ to empirical modelling (see Smithies 2017: 168), including formal or predictive modelling (Joslyn and Turchin 1993), and the technical solutions produced in the process as well as the know-how, languages and documentation which accompany this production. From this wide perspective, models are also artefacts which can be studied across the history of science and of the humanities tradition (see Bod 2018) and in comparison with other modelling practices in science. RSE practice is grounded on a strong conscience of the experimental apparatus and the iterative critique of models built (and often deflated) for a purpose. The challenge is to recognise the idiosyncrasy and situatedness of modelling practices and artefacts while devising methods to expose the scalability of the underlying workflows and modelling processes.
We reflect on the epistemology of DH from the practical perspective of our RSE lab - King’s Digital Lab (KDL)2 - and the research processes embedded in our Software Development Lifecycle (fig.1). The human element is at the core of the technical ecosystem we research and operate in. We acknowledge that KDL models and modelling are co-constitutive of human expertise, technical systems and operational methods, all aspiring to an environment conducive of open knowledge. This is not only in terms of development and management approaches via adoption of open standards, open source and exposure of open data but also, more fundamentally, in sharing (achievements and struggles around) our processes and in promoting open models. We will use project-specific examples, including data modelling and knowledge representation practices, to demonstrate how some of our research into model-making processes challenge the perception of the technical work of RSE within DH as a stale, mechanistic and uncritical procedural activity.
Figure 1 KDL Software Development Lifecycle (SDLC) (King’s Digital Lab 2019) mapped to Agile DSDM project phases (Agile Business Consortium 2014).
KDL operates within a rather unique context. It claims its origins in the pioneering work of colleagues at King’s College London working in applied computing in the Humanities already in the 1970s (see Short et al. 2012). However, the crossings between RSE and DH communities at King’s and internationally are only recently being highlighted and explored (e.g. Gold 2009; Smithies 2019; DHTech Group 2019). We argue that the study and building of models is one of the dimensions via which these crossings emerge more vividly with substantial epistemological implications and innovative ramifications for the field of DH as a whole.
The core practice of research in DH is modelling (e.g. cf. McCarty 2005: 20–72; Buzzetti 2002; Beynon and McCarty 2006; Flanders and Jannidis 2015, 2018), which implies the translation of complex systems of knowledge or conceptual frameworks into computationally processable models or operational frameworks.
Gooding (2003) claims that in experimental settings computational approaches are analogues to other processes of abstraction, measurement and contextual interpretation, whereby reduction of complexity is followed by expansion in the guise of a double funnel-shaped process (Gooding 2003: fig 13.4). We can trace these processes of reduction and expansion also in the RSE context, where, for example, operationalisation makes models formalised into snippets of code or software components. Other languages than translation into code play a role in the process, however.3
The paper will reflect on models as artefacts of different kind expressed via a variety of languages, including but not limited to computational models, produced during several phases of the SDLC (see Ciula and Smithies, forthcoming) such as:
negotiations around the meaning of the project units of analysis documented in diagrams and definitions which shape requirements and an agreed project language;
paper or whiteboard sketches used to draft the solution architecture for a project in its feasibility assessment;
wireframes and static mockups of user journeys;
data models implementing the logical structure of a database;
statistical models implemented with ad hoc algorithms and code or relying on tested formulas and existing libraries.
While specific instantiations of models (for example in relational databases) can have a rather short life, in the RSE context and projects where they were designed and developed, they often represented innovative solutions which have a longstanding effect. Indeed, while models are temporary pragmatic solutions to address specific project challenges yet, at the scale KDL operates, they are the backbone of the team's tacit knowledge as well as the building blocks towards more generalisable and re-usable approaches. They mediate and bridge the layers of the Lab’s socio-technical system: team expertise, data and technical systems (fig. 2).
Figure 2 Multilayered socio-technical system of the Lab where concentric circles denote co-constitution of team expertise, data and technical systems (Ciula and Smithies, forthcoming: fig. 5).
While we can refine existing approaches to sustain and expose modelling efforts in SDLC cycles which rely on RSE best practices such as attention to documentation and re-use,4 there is also room for more innovative approaches, including a design first culture, workflow integration across RSE roles, the assessment and potential adoption of a set of modelling notation languages for the exposure mechanisms for our models.
In alignment with a “critical modelling” approach (see Bode 2020), but also with a material culture and media literacy perspective, in this paper we aim to reflect upon models by looking at the wide epistemological implications of their production and use, at the responsibilities of modellers,5 at how models come to be and what effect they have in the resources they contribute to instantiate and hence in interpretative processes of expansion as well as reduction.
Notes
[1] A preliminary version of this paper was presented at the symposium Computational Text Analysis and Historical Change, held at Humlab, Umeå University (Sweden), 4-6 September 2019.
[2] The authors currently cover different roles at King’s Digital Lab (KDL), a Research Software Engineering (RSE) team hosted by the Faculty of Arts & Humanities at King’s College London, which provides software development and infrastructure to departments in the Faculty of Arts & Humanities while also collaborating with Social Science & Public Policy as well as a range of external partners in the higher education and cultural heritage sectors.
[3] Data modelling, for example, is informed if not driven by communication and collaborative reasoning around more or less standardised graphical representations and notations in phases of reverse engineering as well as design methods. Note that KDL intend and use design methods in a wide sense ranging from techniques of requirements elicitation in pre-project analysis to data modelling and wireframing in evolutionary development (see Bennet et al. 2005). Equally, the re-integration or expansion of modelling efforts into interpretative frameworks usually rely on verbal and visual language to document code, or to explain the results of an experiment (Ciula and Marras 2019: 39).
[4] With this respect see, for example, the KDL checklist for assessment of digital outputs within the UK Research Framework Exercise (Ciula 2019).
[5] Models contribute to define and redefine objects of study which come charged with layers of scholarship and analysis, with previous selections, bias and political as well as ethical responsibilities. As the creators of new memory regimes and intermediaries to the past engaged in modelling efforts which interact and affect the materiality of our objects of study (Ciula 2017a), we bear responsibilities (Ciula 2017b). In line with ongoing discussions around the representativeness and constraints of the digital archive (e.g. Dahlström 2010; Hitchcock 2013; Prescott 2015), Bode (2018, 2020) presented some lucid analysis around modellers’ responsibilities in digital literary studies by exposing the gaps that propagate from produced literary works we know of to material preserved in the analogue archive, to selections of works that make it into digital archives to further reductions in the creation of a corpus of analysis and, last but not least, in the application of statistical modelling techniques which dictate additional powerful yet limiting constraints if not contextualised critically within an interlocked chain of bias.
References
Agile Business Consortium. The DSDM Agile Project Framework Handbook. DSDM Consortium, 2014. https://www.agilebusiness.org/resources/dsdm-handbooks.
Bennett, Simon, Steve McRobb, and Ray Farmer. Object-Oriented Systems Analysis and Design Using UML. McGraw Hill Higher Education, 2005.
Beynon, Meurig, Steve Russ, and Willard McCarty. ‘Human Computing—Modelling with Meaning’. Literary and Linguistic Computing 21, no. 2 (6 January 2006): 141–57. https://doi.org/10.1093/llc/fql015.
Bod, Rens. ‘Modelling in the Humanities: Linking Patterns to Principles’. Historical Social Research, Supplement, no. 31 (2018): 78–95. https://doi.org/10.12759/hsr.suppl.31.2018.78-95.
Bode, Katherine. A World of Fiction: Digital Collections and the Future of Literary History. University of Michigan Press, 2018.
———. ‘Why You Can’t Model Away Bias’. Modern Language Quarterly 81, no. 1 (1 March 2020): 95–124. https://doi.org/10.1215/00267929-7933102. preprint: https://katherinebode.files.wordpress.com/2019/08/mlq2019_preprintbode_why.pdf.
Buzzetti, Dino. ‘Digital Representation and the Text Model’. New Literary History 33, no. 1 (2002): 61–88. https://doi.org/10.1353/nlh.2002.0003.
Ciula, Arianna. ‘Digital Humanities and Practical Memory: Modelling Textuality’. Estudos Em Comunicação 25 (21 December 2017): 7–17.
———. ‘Modelling Textuality: A Material Culture Framework’. In Advances in Digital Scholarly Editing. Papers Presented at the DiXiT Conferences in The Hague, Cologne, and Antwerp, edited by Edited by Peter Boot, Anna Cappellotto, Wout Dillen, Franz Fischer, Aodhán Kelly, Andreas Mertgens, Anna-Maria Sichani, and Elena Spadini & Dirk van Hulle, 91–97. Leiden: Sidestone Press, 2017. https://www.sidestone.com/books/advances-in-digital-scholarly-editing.
———. ‘KDL Checklist for Digital Outputs Assessment (Version 2.0)’. Zenodo, 2019. http://doi.org/10.5281/zenodo.3361580.
Ciula, Arianna, Øyvind Eide, Cristina Marras, and Patrick Sahle, eds. ‘Models and Modelling between Digital & Humanities: A Multidisciplinary Perspective’. Historical Social Research Supplement, no. 31 (2018). https://doi.org/10.12759/hsr.43.2018.4.343-361.
Ciula, Arianna, and Cristina Marras. ‘Exploring a Semiotic Conceptualisation of Modelling in Digital Humanities Practices’. In Meanings & Co: The Interdisciplinarity of Communication, Semiotics and Multimodality, edited by Alin Olteanu, Andrew Stables, and Dimitru Bortun, 6:33–52. Numanities - Arts and Humanities in Progress. Springer International Publishing, 2019. https://www.springer.com/us/book/9783319919850.
Ciula, Arianna, and James Smithies. ‘Sustainability and Modelling at King’s Digital Lab: Between Tradition and Innovation’. In On Making in the Digital Humanities: Essays on the Scholarship of Digital Humanities Development in Honour of John Bradley, edited by Julianne Nyhan, Geoffroy Rockwell and Stefan Sinclair. London: University College London Press, Forthcoming.
Dahlström, Mats. ‘Critical Editing and Critical Digitisation’. In Text Comparison and Digital Creativity: The Production of Presence and Meaning in Digital Text Scholarship, edited by Wido van Peursen, Ernst D. Thoutenhoofd, and Adriaan van der Weel, 79–97. Brill Academic Pub, 2010.
DHTech Group. ‘DH Research Software Engineers - For We Are Many’. DHTech, 2019. https://dh-tech.github.io/dhrse-whitepaper/.
Flanders, Julia, and Fotis Jannidis. ‘Knowledge Organization and Data Modeling in the Humanities’. White paper, 2015. http://www.wwp.northeastern.edu/outreach/conference/kodm2012/flanders_jannidis_datamodeling.pdf.
———, eds. The Shape of Data in Digital Humanities: Modeling Texts and Text-Based Resources. Digital Research in the Arts and Humanities. London: Routledge, Taylor & Francis Group, 2018.
Gold, Nicolas. ‘Service-Oriented Software in the Humanities: A Software Engineering Perspective’. Digital Humanities Quarterly, no. 3.4 (2009). http://digitalhumanities.org:8081/dhq/vol/3/4/000072/000072.html.
Gooding, David. ‘Varying the Cognitive Span: Experimentation, Visualization, And Computation’. In The Philosophy of Scientific Experimentation, edited by Hans Radder, 255–84. Pittsburgh, Pa.: University of Pittsburgh Press, 2003.
Hitchcock, Tim. ‘Confronting the Digital’. Cultural and Social History 10, no. 1 (1 March 2013): 9–23. https://doi.org/10.2752/147800413X13515292098070.
Joslyn, Cliff A., and Valentin Turchin. ‘Model’. In Principia Cybernetica Web, edited by Francis Heylighen, Cliff A. Joslyn, and Valentin Turchin. Brussels, 1993. http://pespmc1.vub.ac.be/MODEL.html.
King’s Digital Lab. ‘Frequently Asked Questions for Project Partners King’s Digital Lab’. King’s Digital Lab, 2019. https://www.kdl.kcl.ac.uk/how-we-work/faq-partners/.
———. ‘What Is KDL King’s Digital Lab’. King’s Digital Lab, 2018. https://www.kdl.kcl.ac.uk/how-we-work/what-is-kdl/.
McCarty, Willard. Humanities Computing. Basingstoke [England]; New York: Palgrave Macmillan, 2005.
Prescott, Andrew. ‘I’d Rather Be a Librarian: A Response to Tim Hitchcock, “Confronting the Digital”’. Cultural and Social History 11, no. 3 (1 September 2014): 335–41. https://doi.org/10.2752/147800414X13983595303192.
Short, Harold, Julianne Nyhan, Anne Welsh, and Jessica Salmon. ‘“Collaboration Must Be Fundamental or It’s Not Going to Work”: An Oral History Conversation between Harold Short and Julianne Nyhan’. Digital Humanities Quarterly 6, no. 3 (2012). http://www.digitalhumanities.org/dhq/vol/6/3/000133/000133.html.
Smithies, James. ‘The Continuum Approach to Career Development: Research Software Careers in King’s Digital Lab’. King’s Digital Lab - Thoughts and Reflections from the Lab (blog), 7 February 2019. https://www.kdl.kcl.ac.uk/blog/rse-career-development/.
———. The Digital Humanities and the Digital Modern. Basingstoke: Palgrave Macmillan, 2017.","arianna.ciula@kcl.ac.uk, paul.caton@kcl.ac.uk, ginestra.ferraro@kcl.ac.uk, brian.maher@kcl.ac.uk, geoffroy.noel@kcl.ac.uk, jose.m.vieira@kcl.ac.uk",Long Presentation
"Hörmann, Richard;
Schlager, Daniel","University of Salzburg, Austria",Enhanced Stand-off TEI Annotation with StoReCo: A generic approach with the use of RDF.,Stand off markup RDF Annotation,"Europe
English
Contemporary
data modeling
text encoding and markup language creation, deployment, and analysis
Humanities computing
Informatics",English,Europe,Contemporary,"data modeling
text encoding and markup language creation, deployment, and analysis","Humanities computing
Informatics","<p>TEI-XML is the standard in digital text editing. The classic inline annotation with TEI quickly reaches its limits when it comes to complex annotations. In order to realize DH projects with such requirements on the dhPLUS platform currently developed at the University of Salzburg, a special form of stand-off markup is presented: It saves the original TEI file, converts the markup into RDF instructions, keeps the text and links the triples to an unlimited number of annotation levels. A unique identifier connects the different stages and facilitates presentation and search processes.</p>
","TEI-XML is the standard in digital text editing. The classic inline annotation with TEI quickly reaches its limits when it comes to complex annotations. In order to realize DH projects with such requirements on the dhPLUS platform currently developed at the University of Salzburg, a special form of stand-off markup is presented: It saves the original TEI file, converts the markup into RDF instructions, keeps the text and links the triples to an unlimited number of annotation levels. A unique identifier connects the different stages and facilitates presentation and search processes.","richard.hoermann@sbg.ac.at, daniel.schlager@sbg.ac.at",Poster
"van Zaanen, Menno;
Trollip, Benito;
Ramukhadi, Phathuthsedzo;
Mlambo, Respect","South African Centre for Digital Language Resources, South Africa","Identifying relations between characters in Afrikaans, Tshivenḓa, and Xitsonga book ","African languages, literary analysis, named entity recognition, network analysis","Africa
English
Contemporary
electronic literature production and analysis
natural language processing
Literary studies",English,Africa,Contemporary,"electronic literature production and analysis
natural language processing",Literary studies,"<p align=""justify"">The usefulness of computational linguistic tools, such as named entity recognition (NER) systems, in linguistic or literary studies of under-resourced languages is an area that is still relatively unexplored. We applied NER systems to one Afrikaans novel and two scanned dramas, one in Tshivenḓa and one in Xitsonga. Personal relations are identified through character name co-occurence in sentences and these relationships are visualized using Gephi, following the approach by Van de Ven <em>et al.</em> (2018). The research identified several practical problems: low quality OCR, low quality NER, limited amounts of NE and language specific issues.</p>
","The usefulness of computational linguistic tools, such as named entity recognition (NER) systems, in linguistic or literary studies of under-resourced languages is an area that is still relatively unexplored. We applied NER systems to one Afrikaans novel and two scanned dramas, one in Tshivenḓa and one in Xitsonga. Personal relations are identified through character name co-occurence in sentences and these relationships are visualized using Gephi, following the approach by Van de Ven et al. (2018). The research identified several practical problems: low quality OCR, low quality NER, limited amounts of NE and language specific issues.","menno.vanzaanen@nwu.ac.za, benito.trollip@nwu.ac.za, Phathutshedzo.Ramukhadi@nwu.ac.za, respect.mlambo@nwu.ac.za",Short Presentation
"Rominger, Gian Duri;
O'Leary, John;
Budak, Nick","Princeton University, United States of America",DIRECT - Digital Intertextual Resonances in Early Chinese Texts,"china, phonology, text, tool","Asia
English
BCE-4th Century
concordancing and indexing
natural language processing
Asian studies
Linguistics",English,Asia,BCE-4th Century,"concordancing and indexing
natural language processing","Asian studies
Linguistics","<p>In this talk, the creators will provide an introduction and demonstration of DIRECT (Digital Intertextual Resonances in Early Chinese Texts), a new digital tool designed to simplify phonological analysis of early Chinese texts. DIRECT enables users to quickly and easily identify homophonous and near-homophonous passages in ancient Chinese texts that may be obscured by graphic variation, uncovering hidden resonances and patterns.</p>
","In this talk, the creators will provide an introduction and demonstration of DIRECT (Digital Intertextual Resonances in Early Chinese Texts), a new digital tool designed to simplify phonological analysis of early Chinese texts. DIRECT enables users to quickly and easily identify homophonous and near-homophonous passages in ancient Chinese texts that may be obscured by graphic variation, uncovering hidden resonances and patterns.","gianr@princeton.edu, jo10@princeton.edu, nbudak@princeton.edu",Lightning
"Winter, Caroline;
El Khatib, Randa;
Arbuckle, Alyssa;
Siemens, Ray","Electronic Textual Cultures Lab, University of Victoria","The Open Knowledge Program: Creating Space for Digital, Public Scholarship","open scholarship, open knowledge, social knowledge creation, digital humanities, public humanities","English
North America
Contemporary
digital research infrastructures development and analysis
public humanities collaborations and methods
Humanities computing",English,North America,Contemporary,"digital research infrastructures development and analysis
public humanities collaborations and methods",Humanities computing,"<p><strong>DH2020 Proposal</strong></p>
<p>Caroline Winter</p>
<p>Randa El Khatib</p>
<p>Alyssa Arbuckle</p>
<p>Ray Siemens</p>
<p><strong> </strong></p>
<p align=""center""><strong>The Open Knowledge Program: Creating Space for Digital, Public Scholarship</strong></p>

<p>In <em>Generous Thinking: A Radical Approach to Saving the University </em>(2019), Kathleen Fitzpatrick argues that academics must reconsider themselves within the “larger ‘us’ that we together form,” rather than holding themselves apart from the wider community (8). How to enact more open, public work is not always obvious, however. Researchers often face barriers to engaging in open and public-facing scholarship, including lack of training, infrastructure, and technical and community support. The Open Knowledge Program at the University of Victoria’s Electronic Textual Cultures Lab (ETCL) facilitates intersections between the scholarly and public communities by supporting university and community researchers in creating open knowledge: “what open data becomes when it’s useful, usable and used” (Open Knowledge Foundation n.d.). In this paper, we discuss the trajectory of the program so far, share examples of participants’ contributions, and invite feedback and discussion about adapting the program for other contexts and its next steps.</p>

<p><strong>The Program</strong></p>

<p>The Open Knowledge Program is based in the ETCL, a collaborative humanities research lab with a focus on open scholarship and a mandate that includes research, training, and service.</p>
<p>The Program comprises three initiatives: the Open Knowledge Practicum (OKP), the Open Knowledge Practicum at the Digital Humanities Summer Institute (OKP@DHSI), and the Open Knowledge Residency (OKR). The OKP is a term-long program that welcomes researchers from UVic and the wider community into the lab to work on an open knowledge project, including contributing to Wikipedia. The OKP@DHSI supports the ETCL’s global community by inviting DHSI students and instructors into the lab for a condensed, three-day version of the OKP. The OKR offers graduate students in any discipline at UVic an intensive, week-long residency in the lab to conduct thesis or doctoral research and share findings in an open venue.</p>
<p><strong>The Intervention</strong></p>
<p>Just as digital scholarship has moved from the periphery of the Humanities towards its centre, open scholarship is increasingly recognized as the new scholarly mode. This transition from closed to open is driven partly by necessity — the costs of the current subscription model of scholarly communication being unsustainable for many research libraries — and partly by researchers driven to engage with the broader community and universities striving to fulfill their public missions (CARL–ABRC 2010; O’Gara 2019; Suber 2019). It is also driven by opportunity, since digital technologies have made it possible to share scholarly work widely with academic and public readers, greatly extending its “reach” (Maxwell 2015, 2).</p>
<p>Digital tools allow researchers to engage with new types of research materials, new tools and methodologies, and new modes of communication, but require skills that are not part of traditional Humanities curricula, including collaboration. These digital skills are increasingly recognized as essential for scholarly work and beyond, particularly for emerging scholars (Brier 2012; El Khatib, Arbuckle, and Siemens 2019; IPLAI 2013; Jakacki and Faull 2016; Lewis et al. 2015; MLA 2014; NEH 2016; Reid 2012). The Open Knowledge Practicum draws on the pedagogical model of the practicum, in which skills and knowledge are applied in practice. Practicums are common in Education and professional fields including nursing and clinical psychology, but much less so in the Humanities and Social Sciences. Although practicums are usually part of a larger curriculum, the OKP employs this model as a standalone program, in which researchers plan, develop, and create their own research projects. In doing so, participants put their subject matter expertise and digital skills into action by creating open knowledge resources while gaining experience working in a DH lab as part of a research team. Participants develop their own projects in consultation with the lab team and our colleagues in the Library and across campus as needed. The lab team also provides just-in-time collaborative learning as needed in the use of specific software applications, digital tools, and project management. Honorariums for participants comprise registration in a DHSI course, providing further opportunity for learning, putting learning into practice, and building community.</p>
<p>In its focus on open scholarship as praxis, the Open Knowledge Program complements the “apprenticeship model that dominates graduate training and socialization in the humanities and elsewhere across the university,” which focuses tightly on skills necessary to institutional frameworks (Bartha and Burgett 2015, 33­–4). These frameworks, however, leave little space for digital and public scholarship: review, promotion, and tenure guidelines, for example, tend to discourage open scholarship in favour of more traditional forms of scholarly communication (Alperin et al. 2018). In their study of best practices for supporting digital scholarship, Lewis, Spiro, Wang, and Cawthorne note that physical space in which to work, collaborate, and learn is key (2015, 2). Lewis et al. also find that “collaborative competencies” and “learning mindsets,” comprising “creativity, curiosity, and an enthusiasm for learning” are as important for digital scholarly work as technical skills (2015, 2). Building regular lab hours into the Program’s structure provides participants with a shared physical workspace as well as a highly collaborative community of practice that facilitates social knowledge creation (Burke 2000). Participants in the Program determine the subject and scope of their project, pursuing interests that intersect with their academic work or study, or not. In this way, the Program creates intellectual space and scheduled time for curiosity, exploration, and creativity, space that is often difficult to find within a university’s institutional structure (Bartha and Burgett 2015). Many projects focus on social justice issues such as wealth inequality, Indigenous knowledge systems, international LGBTQ rights, and histories of oppressed groups (ETCL 2020; El Khatib et al. 2019).</p>
<p>The Open Knowledge Program has evolved over the past few years to support standalone projects as well as multi-term endeavours and components of large, grant-funded research initiatives with faculty partners and their student research assistants. We anticipate that describing the structure and goals of the Open Knowledge Program and highlighting the work of its participants will open a discussion of the Program’s future directions and provide a model for other digital scholarship centres that are invested in public scholarship and open knowledge production.</p>
<p align=""center"">References</p>
<p>Alperin, Juan Pablo, Carol Muñoz Nieves, Lesley Schimanski, Gustavo E. Fischman, Meredith T. Niles, and Erin C. McKiernan. 2018. “How Significant are the Public Dimensions of Faculty Work in Review, Promotion, and Tenure Documents?” <em>Humanities Commons</em> [preprint]. http://dx.doi.org/10.17613/M6W950N35.</p>
<p>Bartha, Miriam, and Bruce Burgett. 2015. “Why Public Scholarship Matters for Graduate Education.” <em>Pedagogy</em> 15, no. 1: 31–43. https://doi.org/10.1215/15314200-2799148.</p>
<p>Brier, Stephen. 2012. “Where’s the Pedagogy? The Role of Teaching and Learning in the Digital Humanities.” In <em>Debates in Digital Humanities, </em>edited by Matthew K. Gold, 390–401. Minneapolis: University of Minnesota Press. https://doi.org/10.5749/9781452963754.</p>
<p>Burke, Peter. 2000. <em>A Social History of Knowledge: From Gutenberg to Diderot</em>. Cambridge: Polity Press.</p>
<p>CARL–ABRC (Canadian Association of Research Libraries–Association des bibliothèques de recherche du Canada). 2010. <em>CARL Open Access Backgrounder</em>. http://www.carl-abrc.ca/doc/CARL_OA_Backgrounder_EN.pdf.</p>
<p>El Khatib, Randa, Alyssa Arbuckle, and Ray Siemens. 2019. “Foundations for On-Campus Open Social Scholarship Activities.” <em>KULA: Knowledge Creation, Dissemination, and Preservation Studies</em> 3, no. 1. http://doi.org/10.5334/kula.14.</p>
<p>El Khatib, Randa, Lindsey Seatter, Tracy El Hajj, Conrad Leibel, Alyssa Arbuckle, Ray Siemens, Caroline Winter, and the ETCL and INKE Research Groups. 2019. “Open Social Scholarship Annotated Bibliography.” <em>KULA: Knowledge Creation, Dissemination, and Preservation Studies</em> 3, no. 1. http://doi.org/10.5334/kula.58.</p>
<p>ETCL (Electronic Textual Cultures Lab). 2020. “Open Knowledge Practicum.” https://etcl.uvic.ca/open-knowledge-practicum-okp/.</p>
<p>Fitzpatrick, Kathleen. 2019. <em>Generous Thinking: A Radical Approach to Saving the University</em>. Baltimore: Johns Hopkins University Press.</p>
<p>IPLAI (Institute for the Public Life of Arts and Ideas). 2013. “White Paper on the Future of the PhD in the Humanities.” 2013. Institute for the Public Life of Arts and Ideas, McGill University. http://iplai.ca/wp-content/uploads/2015/04/white_paper_on_the_future_of_the_phd_in_the_humanities_dec_2013_1.pdf.</p>
<p>Jakacki, Diane, and Katherine Faull. 2016. “Doing DH in the Classroom: Transforming the Humanities Curriculum through Digital Engagement.” In <em>Doing Digital Humanities, </em>edited by Constance Crompton, Richard Lane, and Ray Siemens, 358–72. London: Routledge.</p>
<p>Lewis, Vivian, Lisa Spiro, Xuemao Wang, and Jon E. Cawthorne. 2015. <em>Building Expertise to Support Digital Scholarship: A Global Perspective</em>. Council on Library and Information Resources. https://www.clir.org/pubs/reports/pub168/.</p>
<p>Maxwell, John W. 2015. “Beyond Open Access to Open Publication and Open Scholarship.” <em>Scholarly and Research Communication</em> 6, no. 3. https://doi.org/10.22230/src.2015v6n3a202.</p>
<p>MLA (Modern Language Association). 2014. Report of the Task Force on Doctoral Study in Modern Language and Literature. https://www.mla.org/content/download/25437/1164354/taskforcedocstudy2014.pdf.</p>
<p>NEH (National Endowment for the Humanities). 2016. “NEH Announces $1.7 Million for ‘Next Generation PhD.’” August 9, 2016. https://www.neh.gov/news/press-release/2016-08-10.</p>
<p>O’Gara, Genya, and Anne C. Osterman. 2019. “Negotiating on Our Terms: Harnessing the Collective Power of the Consortium to Transform the Journal Subscription Model.” <em>Collection Management</em> 44, no. 2–4: 176–194. https://doi.org/10.1080/01462679.2018.1564716.</p>
<p>Open Knowledge Foundation. n.d. “What is Open?” https://okfn.org/opendata/. Accessed October 9, 2019.</p>
<p>Reid, Alexander. 2012. “Graduate Education and the Ethics of the Digital Humanities.” In <em>Debates in the Digital Humanities</em>, edited by Matthew K. Gold, 350–67. Minneapolis: University of Minnesota Press. https://doi.org/10.5749/9781452963754.</p>
<p>Suber, Peter. 2019. “2. Motivation.” In <em>Open Access</em>, by Suber, https://openaccesseks.mitpress.mit.edu/pub/ktf344br.</p>
","DH2020 Proposal
Caroline Winter
Randa El Khatib
Alyssa Arbuckle
Ray Siemens
 
The Open Knowledge Program: Creating Space for Digital, Public Scholarship
In Generous Thinking: A Radical Approach to Saving the University (2019), Kathleen Fitzpatrick argues that academics must reconsider themselves within the “larger ‘us’ that we together form,” rather than holding themselves apart from the wider community (8). How to enact more open, public work is not always obvious, however. Researchers often face barriers to engaging in open and public-facing scholarship, including lack of training, infrastructure, and technical and community support. The Open Knowledge Program at the University of Victoria’s Electronic Textual Cultures Lab (ETCL) facilitates intersections between the scholarly and public communities by supporting university and community researchers in creating open knowledge: “what open data becomes when it’s useful, usable and used” (Open Knowledge Foundation n.d.). In this paper, we discuss the trajectory of the program so far, share examples of participants’ contributions, and invite feedback and discussion about adapting the program for other contexts and its next steps.
The Program
The Open Knowledge Program is based in the ETCL, a collaborative humanities research lab with a focus on open scholarship and a mandate that includes research, training, and service.
The Program comprises three initiatives: the Open Knowledge Practicum (OKP), the Open Knowledge Practicum at the Digital Humanities Summer Institute (OKP@DHSI), and the Open Knowledge Residency (OKR). The OKP is a term-long program that welcomes researchers from UVic and the wider community into the lab to work on an open knowledge project, including contributing to Wikipedia. The OKP@DHSI supports the ETCL’s global community by inviting DHSI students and instructors into the lab for a condensed, three-day version of the OKP. The OKR offers graduate students in any discipline at UVic an intensive, week-long residency in the lab to conduct thesis or doctoral research and share findings in an open venue.
The Intervention
Just as digital scholarship has moved from the periphery of the Humanities towards its centre, open scholarship is increasingly recognized as the new scholarly mode. This transition from closed to open is driven partly by necessity — the costs of the current subscription model of scholarly communication being unsustainable for many research libraries — and partly by researchers driven to engage with the broader community and universities striving to fulfill their public missions (CARL–ABRC 2010; O’Gara 2019; Suber 2019). It is also driven by opportunity, since digital technologies have made it possible to share scholarly work widely with academic and public readers, greatly extending its “reach” (Maxwell 2015, 2).
Digital tools allow researchers to engage with new types of research materials, new tools and methodologies, and new modes of communication, but require skills that are not part of traditional Humanities curricula, including collaboration. These digital skills are increasingly recognized as essential for scholarly work and beyond, particularly for emerging scholars (Brier 2012; El Khatib, Arbuckle, and Siemens 2019; IPLAI 2013; Jakacki and Faull 2016; Lewis et al. 2015; MLA 2014; NEH 2016; Reid 2012). The Open Knowledge Practicum draws on the pedagogical model of the practicum, in which skills and knowledge are applied in practice. Practicums are common in Education and professional fields including nursing and clinical psychology, but much less so in the Humanities and Social Sciences. Although practicums are usually part of a larger curriculum, the OKP employs this model as a standalone program, in which researchers plan, develop, and create their own research projects. In doing so, participants put their subject matter expertise and digital skills into action by creating open knowledge resources while gaining experience working in a DH lab as part of a research team. Participants develop their own projects in consultation with the lab team and our colleagues in the Library and across campus as needed. The lab team also provides just-in-time collaborative learning as needed in the use of specific software applications, digital tools, and project management. Honorariums for participants comprise registration in a DHSI course, providing further opportunity for learning, putting learning into practice, and building community.
In its focus on open scholarship as praxis, the Open Knowledge Program complements the “apprenticeship model that dominates graduate training and socialization in the humanities and elsewhere across the university,” which focuses tightly on skills necessary to institutional frameworks (Bartha and Burgett 2015, 33­–4). These frameworks, however, leave little space for digital and public scholarship: review, promotion, and tenure guidelines, for example, tend to discourage open scholarship in favour of more traditional forms of scholarly communication (Alperin et al. 2018). In their study of best practices for supporting digital scholarship, Lewis, Spiro, Wang, and Cawthorne note that physical space in which to work, collaborate, and learn is key (2015, 2). Lewis et al. also find that “collaborative competencies” and “learning mindsets,” comprising “creativity, curiosity, and an enthusiasm for learning” are as important for digital scholarly work as technical skills (2015, 2). Building regular lab hours into the Program’s structure provides participants with a shared physical workspace as well as a highly collaborative community of practice that facilitates social knowledge creation (Burke 2000). Participants in the Program determine the subject and scope of their project, pursuing interests that intersect with their academic work or study, or not. In this way, the Program creates intellectual space and scheduled time for curiosity, exploration, and creativity, space that is often difficult to find within a university’s institutional structure (Bartha and Burgett 2015). Many projects focus on social justice issues such as wealth inequality, Indigenous knowledge systems, international LGBTQ rights, and histories of oppressed groups (ETCL 2020; El Khatib et al. 2019).
The Open Knowledge Program has evolved over the past few years to support standalone projects as well as multi-term endeavours and components of large, grant-funded research initiatives with faculty partners and their student research assistants. We anticipate that describing the structure and goals of the Open Knowledge Program and highlighting the work of its participants will open a discussion of the Program’s future directions and provide a model for other digital scholarship centres that are invested in public scholarship and open knowledge production.
References
Alperin, Juan Pablo, Carol Muñoz Nieves, Lesley Schimanski, Gustavo E. Fischman, Meredith T. Niles, and Erin C. McKiernan. 2018. “How Significant are the Public Dimensions of Faculty Work in Review, Promotion, and Tenure Documents?” Humanities Commons [preprint]. http://dx.doi.org/10.17613/M6W950N35.
Bartha, Miriam, and Bruce Burgett. 2015. “Why Public Scholarship Matters for Graduate Education.” Pedagogy 15, no. 1: 31–43. https://doi.org/10.1215/15314200-2799148.
Brier, Stephen. 2012. “Where’s the Pedagogy? The Role of Teaching and Learning in the Digital Humanities.” In Debates in Digital Humanities, edited by Matthew K. Gold, 390–401. Minneapolis: University of Minnesota Press. https://doi.org/10.5749/9781452963754.
Burke, Peter. 2000. A Social History of Knowledge: From Gutenberg to Diderot. Cambridge: Polity Press.
CARL–ABRC (Canadian Association of Research Libraries–Association des bibliothèques de recherche du Canada). 2010. CARL Open Access Backgrounder. http://www.carl-abrc.ca/doc/CARL_OA_Backgrounder_EN.pdf.
El Khatib, Randa, Alyssa Arbuckle, and Ray Siemens. 2019. “Foundations for On-Campus Open Social Scholarship Activities.” KULA: Knowledge Creation, Dissemination, and Preservation Studies 3, no. 1. http://doi.org/10.5334/kula.14.
El Khatib, Randa, Lindsey Seatter, Tracy El Hajj, Conrad Leibel, Alyssa Arbuckle, Ray Siemens, Caroline Winter, and the ETCL and INKE Research Groups. 2019. “Open Social Scholarship Annotated Bibliography.” KULA: Knowledge Creation, Dissemination, and Preservation Studies 3, no. 1. http://doi.org/10.5334/kula.58.
ETCL (Electronic Textual Cultures Lab). 2020. “Open Knowledge Practicum.” https://etcl.uvic.ca/open-knowledge-practicum-okp/.
Fitzpatrick, Kathleen. 2019. Generous Thinking: A Radical Approach to Saving the University. Baltimore: Johns Hopkins University Press.
IPLAI (Institute for the Public Life of Arts and Ideas). 2013. “White Paper on the Future of the PhD in the Humanities.” 2013. Institute for the Public Life of Arts and Ideas, McGill University. http://iplai.ca/wp-content/uploads/2015/04/white_paper_on_the_future_of_the_phd_in_the_humanities_dec_2013_1.pdf.
Jakacki, Diane, and Katherine Faull. 2016. “Doing DH in the Classroom: Transforming the Humanities Curriculum through Digital Engagement.” In Doing Digital Humanities, edited by Constance Crompton, Richard Lane, and Ray Siemens, 358–72. London: Routledge.
Lewis, Vivian, Lisa Spiro, Xuemao Wang, and Jon E. Cawthorne. 2015. Building Expertise to Support Digital Scholarship: A Global Perspective. Council on Library and Information Resources. https://www.clir.org/pubs/reports/pub168/.
Maxwell, John W. 2015. “Beyond Open Access to Open Publication and Open Scholarship.” Scholarly and Research Communication 6, no. 3. https://doi.org/10.22230/src.2015v6n3a202.
MLA (Modern Language Association). 2014. Report of the Task Force on Doctoral Study in Modern Language and Literature. https://www.mla.org/content/download/25437/1164354/taskforcedocstudy2014.pdf.
NEH (National Endowment for the Humanities). 2016. “NEH Announces $1.7 Million for ‘Next Generation PhD.’” August 9, 2016. https://www.neh.gov/news/press-release/2016-08-10.
O’Gara, Genya, and Anne C. Osterman. 2019. “Negotiating on Our Terms: Harnessing the Collective Power of the Consortium to Transform the Journal Subscription Model.” Collection Management 44, no. 2–4: 176–194. https://doi.org/10.1080/01462679.2018.1564716.
Open Knowledge Foundation. n.d. “What is Open?” https://okfn.org/opendata/. Accessed October 9, 2019.
Reid, Alexander. 2012. “Graduate Education and the Ethics of the Digital Humanities.” In Debates in the Digital Humanities, edited by Matthew K. Gold, 350–67. Minneapolis: University of Minnesota Press. https://doi.org/10.5749/9781452963754.
Suber, Peter. 2019. “2. Motivation.” In Open Access, by Suber, https://openaccesseks.mitpress.mit.edu/pub/ktf344br.","winterc@uvic.ca, khatib@uvic.ca, alyssaa@uvic.ca, siemens@uvic.ca",Long Presentation
"Olive, Jenn","Georgia State University, United States of America","Never Alone, Never Finished: Defining Never Alone’s World Games Genre as Ethical Alternative to Empathy Games","Never Alone, world games, survivance, empathy games","Global
English
North America
Contemporary
electronic literature production and analysis
mixed-media analysis
Games studies
Literary studies",English,"Global
North America",Contemporary,"electronic literature production and analysis
mixed-media analysis","Games studies
Literary studies","<p>In this paper, I will investigate the genre of world games through the game that launched the genre into the spotlight,  <em>Never Alone. </em>This debut title for the first indigenous-owned game company in the United States, Upper One Games, uses this genre to categorize itself within the rest of game experiences, which is significant given the genre's lack of use in games at large as well as what it signifies in its difference from established genres. Based on my investigation, I will argue that the potential importance of the genre as defined through <em>Never Alone </em>comes from its opportunity to answer to the ethical concerns of the empathy games genre.</p>
","In this paper, I will investigate the genre of world games through the game that launched the genre into the spotlight, Never Alone. This debut title for the first indigenous-owned game company in the United States, Upper One Games, uses this genre to categorize itself within the rest of game experiences, which is significant given the genre's lack of use in games at large as well as what it signifies in its difference from established genres. Based on my investigation, I will argue that the potential importance of the genre as defined through Never Alone comes from its opportunity to answer to the ethical concerns of the empathy games genre.",jolive1@gsu.edu,Short Presentation
"Schöch, Christof (1);
van Dalen-Oskam, Karina (3);
Antoniak, Maria (4);
Jannidis, Fotis (2);
Mimno, David (4)","1: University of Trier, Germany;
2: University of Würzburg, Germany;
3: Huyghens ING and University of Amsterdam, The Netherlands;
4: Cornell University, USA",Replication and Computational Literary Studies,"replication, reproduction, computational literary studies","Global
English
Contemporary
meta-criticism (reflections on digital humanities and humanities computing)
text mining and analysis
Literary studies",English,Global,Contemporary,"meta-criticism (reflections on digital humanities and humanities computing)
text mining and analysis",Literary studies,"<p><strong>Technical Note<br /></strong> <strong></strong> A PDF version of this contribution is available at Humanities Commons: https://hcommons.org/deposits/item/hc:30439 Several versions of this contribution in structured formats, including figures and bibliographical data, are available from Zenodo: http://doi.org/10.5281/zenodo.3893428</p>
<p><strong></strong> <strong>Panel Overview</strong></p>
<p>The ""replication crisis"" that has been raging in fields like Psychology (Open Science Collaboration 2015) or Medicine (Ioannidis 2005) for years has recently reached the field of Artificial Intelligence (Barber 2019). One of the key conferences in the field, NeurIPS, has reacted by appointing 'reproducibility chairs' in their organizing committee <sup>1</sup>. In the Digital Humanities, and particularly in Computational Literary Studies (CLS), there is an increasing awareness of the crucial role played by replication in evidence-based research. Relevant disciplinary developments include the increased importance of evaluation in text analysis and the increased interest in making research transparent through publicly accessible data and code (open source, open data). Specific impulses include Geoffrey Rockwell and Stéfan Sinclair's re-enactments of pre-digital studies (Sinclair and Rockwell 2015) or the recent replication study by Nan Z. Da (Da 2019). The paper has been met by an avalanche of responses that pushed back several of its key claims, including its rather sweeping condemnation of the replicated papers. However, an important point got buried in the process: that replication is indeed a valuable goal and practice. <sup>2</sup>As stated in the Open Science Collaboration paper: ""Replication can increase certainty when findings are reproduced and promote innovation when they are not"" (Open Science Collaboration 2015, 943).</p>
<p>As a consequence, the panel aims to raise a number of issues regarding the place, types, challenges and affordances, both on a practical and on a policy or community level, of replication in CLS. Several impulse papers will address key aspects of the issue: recent experience with attempts at replication of specific papers; policies dealing with replication in fields with more experience in the issue; conceptual and terminological clarification with regard to replication studies; and proposals for a way forward with replication as a community task or a policy issue.</p>
<p><strong>Contribution 1: ""A typology of replication studies"", by Christof Schöch</strong></p>
<p>This contribution aims to provide orientation about the range of existing replication studies, based on a simple typology. The typology describes the relationship between an earlier study and its replication in terms of four key variables: the research question, the method of analysis (including the implementation of that method) and the dataset used. <sup>3</sup> For each of these variables, a replication study can attempt to operate either in the same way as the previous study, or in a different way. Note that the typology is not meant to establish these distinctions in a purely binary fashion: rather, as data or methods are never entirely identical or completely different from the earlier study, the extreme points in the typology are meant to open up a gradient of pratices. In addition (and this aspect might be unique to the Digital Humanities), replication studies can be described as involving crossing the boundary between non-digital and digital research or between qualitative and quantitative research.</p>
<p>[Figure 1: Typology of repeating research]</p>
<p>At the most fundamental level, such a typology structures the field and provides a clearly-defined terminology and systematic relations between the various types. For example, replication vs. reproduction or re-analysis vs. follow-up research. Such a shared understanding is useful because each type of replication study comes with its own objectives, requirements and challenges as well as their own place and function in the research process. A replication study strictly repeating key aspects of an earlier study will be most useful in reviewing and quality assessment, while follow-up research more loosely modeled after an earlier study may instead have important methodological implications or lead to knew domain knowledge. This is true despite the fact that in reality, there is more to consider than a few binary categories when describing a given replication study.</p>
<p>But understanding replication through such a typology can have an impact on the field of DH in a number of additional ways. It can provide guidance when publishing research and help clarify what needs to be provided (in terms of data, code and contextualization in prose) in order for a study to be amenable to a specific type of replication. It can help assess the merits and limitations of a given replication study to assess whether, given the stated objectives of the authors, they have employed a suitable type of replication strategy. And it can support designing a replication study and clarify what data, code and contextual information needs to be obtained or reconstructed in order to perform a specific type of replication. <sup>4</sup></p>
<p>Beyond this, such a typology can contribute to better define the relationship between replication in the strict sense and related efforts like benchmarking and evaluation studies. Finally, because such a typology makes it easier to identify similar studies across disciplinary boundaries, it may help us as a field learn more quickly from other fields with a longer tradition in (specific types of) replication studies. In this way, such a typology of replication studies can contribute to establishing replication as a well-understood part of Computational Literary Studies.</p>
<p><strong>Contribution 2: ""Replication to the Rescue: Funding Strategies"", by Karina Van Dalen-Oskam</strong></p>
<p>The Dutch Research Council (NWO) is the first funding agency to take the initiative for a pilot programme for Replication Studies. Their aim is ""to encourage researchers to carry out replication research. NWO wants to gain experience that can lead to insights into how replication research can be effectively included in research programmes. That experience should also lead to insights into and a reflection on the requirements that NWO sets for research in terms of methodology and transparency."" <sup>5</sup>The first two rounds of funding were in 2017 and 2018, and were aimed at the Social Sciences and Medical Sciences. In the third round in 2019, the Humanities were included.</p>
<p>This was done after a heated discussion in <em>Nature</em> between Rik Peels and Lex Bouter (chair of the Replication Studies Programme Committee) on the one hand, and Bart Pender, Sarah de Rijcke and J. Britt Holbrook on the other. Peels and Bouter (2018) started of with a note titled ""Humanities need a replication drive too"". De Rijke and Penders (2018), both scholars from Science and Technology Studies, countered with the call to ""Resist calls for replicability in the humanities"". They argue that quality criteria are crucially different in the humanities and the sciences. <sup>6</sup></p>
<p>NWO went ahead with including the humanities in the call for replication studies, stating they are aware that not all humanities research is suitable for replication. NWO ""expresses no preference or opinion about the value of various methods of research. Where possible it wants to encourage and facilitate the replication of humanities research: this should certainly be possible in the empirical humanities."" <sup>7</sup>In March 2020, seven proposals were awarded funding, but none of these can be called typical Humanities projects. <sup>8</sup>How many submissions were received from humanities applicants - did scholars indeed resist, as Pender and De Rijcke advised? And in a wider context: How do Dutch Humanities scholars evaluate the new possibility? And does this agree with the reception in the growing and very active Dutch Digital Humanities community?</p>
<p>In my short impulse paper, I will reflect on what we can learn from the explicit invitation to the humanties to apply for funding for replication studies. What does this tell us about the status of humanities research in the Netherlands, and more specifically about the role of the Digital Humanities? I will pay special attention to the opportunities these developments may have for Computational Literary Studies. Should we consider the situation as ""Funding Strategies to the Rescue: Replication"", so a turning around of the title of my talk?</p>
<p><strong>Contribution 3: ""Replication of quantitative and qualitative research - a case study"", by Fotis Jannidis</strong></p>
<p>Literary studies always had an empirical side - 'empirical' in the broader sense, that claims and counterclaims are substantiated by referring to specific parts of texts. These text segments are regarded as indicators which in their sum make a more general point plausible, for example the use of specific terms to validate a hypothesis about a text. Therefore, the concept of replication warrants a wider understanding in Computational Literary Studies. The prototypical center is the quantitative replication of quantitative research, but it also includes quantitative replication of qualitative philological research: Using the same indicators to validate the same hypothesis but moving the research into an empirical framework. Seen in the context of the discussion of mixed methods, this is a specific case of ‘triangulation’. Triangulation refers to “the application of different data analysis methods, different data sets, or different researchers’ perspective to examine the same research question or theme” (Bergin 2018, 29). But here, data sets and data analysis methods overlap strongly, while the research framework is changed from hermeneutic to quantitative.</p>
<p>Our case study is an attempt to replicate research on the complexity of language in German dime novels, published by Peter Nusser (Nusser 1981), and it demands both kinds of replication. Nusser describes the language of dime novels on three levels: vocabulary, syntax, and phrases. The work on vocabulary and syntax is quantitative, while the analysis of phrases is qualitative. The replication of the quantitative parts is made more difficult by the fact that the results which Nusser reports have actually been produced by another author in the context of an unprinted exam thesis which seems to be lost for the moment. So a lot of information is missing, and we can only make educated guesses: the exact corpus design (for high literature only the authors are given and for dime novels only the series), the strategies of tokenization and sentence splitting, the exact formula for calculating specific values, etc. The qualitative research is enumerating many phrases which are seen as examples of clichés and there is no explicit comparison with high literature. So a quantification must try to operationalize the concept of cliché and then compare retrieval results between dime novels and high literature.</p>
<p>As is well known in Computational Literary Studies, operationalization as an instance of formal modeling usually covers some aspects that are part of the intuitive notion, while others are excluded for the time being and it is one goal to reduce the loss (Moretti (2013); for a counterposition see Underwood (2019, 181)). In a replication, the loss may be responsible for the difference in outcome. In view of all these difficulties it could seem an unnecessary endeavor to replicate the research, but Nusser’s study had a huge influence on the assessment and evaluation of popular literature in German studies for almost four decades.</p>
<p><strong>Contribution 4: ""Reliable methods for text analysis"", by Maria Antoniak and David Mimno</strong></p>
<p>If we are to make reproducible computational claims about literary texts, we need methods that lend themselves to robustness and reliability. Here we focus on the case study of word embeddings, which analyze collections of documents and produce numeric representations of words. Although these methods are powerful, they are also at high risk for problems with reproducibility: they are complicated enough to be essentially ""black boxes"", yet they are also known to be highly sensitive to text curation choices, parameter settings, and even random initializations (Antoniak and Mimno 2018). How can we assure researchers and their audiences that seemingly small changes would not alter or even reverse their findings?</p>
<p>Embedding vectors are useful for their ability to operationalize thick cultural concepts. For example, the resulting vectors have been used to measure shifts in word meaning over time and geographic areas (e.g. Hamilton, Lescovec, and Jurafsky (2016); Kulkarni, Perozzi, and Skiena (2016)). Several studies have shown that embeddings can encode gender biases by probing embedding spaces using carefully chosen seed words (Gordon and Van Durme (2013); (Bolukbasi et al. 2016a); Caliskan Islam, Bryson, and Narayanan (2016)). Subsequent work in natural language processing has focused on removing biases from an embedding model (Bolukbasi et al. (2016b); Sutton, Lansdall, and Cristianini (2018)). In this context, the concern is the downstream impact of bias on systems that use embeddings, but similar work can also be motivated from an upstream perspective, as a means of studying bias in collections.</p>
<p>Researchers from the humanities and social sciences use embeddings to provide quantitative answers to otherwise elusive political and social questions about the training corpus and its authors (e.g. Kozlowski, Taddy, and Evans (2019)). These bias detection techniques were originally intended to measure the bias encoded in a trained embedding; they were not originally tested to measure the bias of a corpus and make comparisons between corpora.</p>
<p>We probe the stability of these measurements by testing two popular bias detection methods ( Bolukbasi et al. (2016a); Caliskan Islam, Bryson, and Narayanan (2016)) on sets of automatically constructed seed sets. These sets were constructed by randomly selecting a target term and then including its <em>N</em> nearest neighbors in the set; this process more closely approximates a real seed set, constructed by a scholar interested in a particular concept, than a random set of seeds. We find that bias detection techniques via word embeddings are susceptible to variability in the seed terms, in both their order (alternative pairings of seeds from two sets can significantly change the ability of the method to capture a single bias subspace) and semantic similarity (the more similar seeds set are to each other, the more difficult it is to measure their biases). If done carefully, bias detection using embeddings is feasible even for small, subdivided collections and can provide a promising tool for differential content analysis, but we encourage error analysis of the seed terms.</p>
<p>We further highlight a central inconsistency in these bias detection methods. While these methods seek to measure biases in datasets, the researcher-selected seeds themselves can contain a variety of biases. For example, the seeds used for racial categories often include lists of names that are ""African American"" or ""European."" Such lists can be both reductive and essentializing. In addition, some seed sets contain confounding terms, e.g., contain a gendered term in a seed set for ""domestic work"" that is then used to measure gender bias. If the seed set for ""domestic work"" appears closer to the gender that it contains, it will be impossible to say whether that bias exists because of the training corpus or because of the inclusion of the gendered seed.</p>
<p>This case study highlights the reversal in perspectives when techniques from natural language processing and machine learning are re-purposed for studies of specialized datasets. Some working assumptions from the machine learning community (e.g. large size of training set) are broken in the humanities context, where datasets are non-expandable and are the primary focus of the study, rather than a generalized training set for downstream applications. The stability and robustness of these repurposings should not be assumed but rather should be reanalyzed for the particular new contexts.</p>
<p><strong>Bibliography</strong></p>
<p>Antoniak, Maria, and David Mimno. 2018. “Evaluating the Stability of Embedding-Based Word Similarities.” <em>Transactions of the Association for Computational Linguistics</em> 6. https://transacl.org/ojs/index.php/tacl/article/view/1202.</p>
<p>Barber, Gregory. 2019. “Artificial Intelligence Confronts a ‘Reproducibility’ Crisis.” <em>Wired</em>, 2019. https://www.wired.com/story/artificial-intelligence-confronts-reproducibility-crisis/.</p>
<p>Bergin, Tiffany. 2018. <em>An Introduction to Data Analysis. Quantitative, Qualitative and Mixed Methods</em>. London: Sage.</p>
<p>Bolukbasi, Tolga, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam Tauman Kalai. 2016a. “Man Is to Computer Programmer as Woman Is to Homemaker? Debiasing Word Embeddings.” In <em>NIPS</em>. https://arxiv.org/abs/1607.06520.</p>
<p>Bolukbasi, Tolga, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam Tauman Kalai. 2016b. “Quantifying and Reducing Stereotypes in Word Embeddings.” In <em>Data4Good: Machine Learning in Social Good Applications</em>, edited by Kush R. Varshney. http://arxiv.org/abs/1606.06121.</p>
<p>Caliskan Islam, Aylin, Joanna J. Bryson, and Arvind Narayanan. 2016. “Semantics Derived Automatically from Language Corpora Necessarily Contain Human Biases.” In <em>CoRR</em>. http://arxiv.org/abs/1608.07187.</p>
<p>Da, Nan Z. 2019. “The Computational Case Against Computational Literary Studies.” <em>Critical Inquiry</em> 45 (3): 601–39. https://www.journals.uchicago.edu/doi/abs/10.1086/702594.</p>
<p>De Rijke, Sarah, and Bart Penders. 2018. “Resist Calls for Replicability in the Humanities.” <em>Nature</em> 560 (29). https://www.nature.com/articles/d41586-018-05845-z.</p>
<p>Deckker, Harrison, and Paula Lackie. 2016. “Technical Data Skills for Reproducible Research.” Edited by Lynda M. Kellam and Kristi Thompson. <em>Databrarianship: The Academic Data Librarian in Theory and Practice</em>. https://escholarship.org/uc/item/8qb2q8fk.</p>
<p>Gómez, Omar S., Natalia Juristo, and Sira Vegas. 2010. “Replication, Reproduction and Re-Analysis: Three Ways for Verifying Experimental Findings.” In <em>RESER ’2010 Cape Town</em>.</p>
<p>Gordon, Jonathan, and Benjamin Van Durme. 2013. “Reporting Bias and Knowledge Acquisition.” In <em>Proceedings of the 2013 Workshop on Automated Knowledge Base Construction</em>. https://doi.org/https://doi.org/10.1145/2509558.2509563.</p>
<p>Hamilton, William L., Jure Lescovec, and Dan Jurafsky. 2016. “Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change.” Edited by Hamilton, Jure Lescovec, and Dan Jurafsky. <em>CoRR</em>. http://arxiv.org/abs/1605.09096.</p>
<p>Hüffmeier, Joachim, Jens Mazei, and Thomas Schulte. 2015. “Reconceptualizing Replication as a Sequence of Different Studies: A Replication Typology.” <em>Journal of Experimental Social Psychology</em>, 81–92. https://www.sciencedirect.com/science/article/pii/S0022103115001195.</p>
<p>Ioannidis, J.A. 2005. “Contradicted and Initially Stronger Effects in Highly Cited Clinical Research.” <em>JAMA</em>, no. 294/2: 218–28. https://doi.org/https://doi:10.1001/jama.294.2.218.</p>
<p>Kozlowski, Austin C., Matt Taddy, and James A. Evans. 2019. “The Geometry of Culture: Analyzing Meaning through Word Embeddings.” <em>American Sociological Review</em> 84 (5). https://doi.org/https://doi.org/10.1177%2F0003122419877135.</p>
<p>Kulkarni, Vivek, Bryan Perozzi, and Steven Skiena. 2016. “Freshman or Fresher? Quantifying the Geographic Variation of Internet Language.” In <em>Proceedings of the Tenth International Conference on Web and Social Media</em>. http://www.aaai.org/ocs/index.php/ICWSM/ICWSM16/paper/view/13121.</p>
<p>Moretti, Franco. 2013. “‘Operationalizing’: Or, The Function of Measurement in Modern Literary Theory.” <em>Stanford Literary Lab Pamphlet 6</em>. https://litlab.stanford.edu/LiteraryLabPamphlet6.pdf.</p>
<p>Nusser, Peter. 1981. <em>Romane für die Unterschicht. Der Groschenroman und seine Leser [1973]</em>. Stuttgart: Metzler.</p>
<p>Open Science Collaboration. 2015. “Estimating the Reproducibility of Psychological Science.” <em>Science</em>, no. 349 (6251). https://science.sciencemag.org/content/349/6251/aac4716.</p>
<p>Peels, Rik, and Lex Bouter. 2018. “Humanities Need a Replication Drive Too.” <em>Nature</em> 558 (372). https://www.nature.com/articles/d41586-018-05454-w.</p>
<p>Peels, Rik, and Lex Bouter. 2018a. “Replication Is Both Possible and Desirable in the Humanities, Just as It Is in the Sciences.” <em>LSE Impact Blog</em>, 2018. http://blogs.lse.ac.uk/impactofsocialsciences/2018/10/01/replication-is-both-possible-and-desirable-in-the-humanities-just-as-it-is-in-the-sciences/.</p>
<p>Peels, Rik, and Lex Bouter. 2018b. “The Possibility and Desirability of Replication in the Humanities.” <em>Palgrave Communications</em> 4 (95). https://science.sciencemag.org/content/349/6251/aac4716.</p>
<p>Pender, Bart, J. Britt Holbrook, and Sarah De Rijke. 2019. “Rinse and Repeat: Understanding the Value of Replication across Different Ways of Knowing.” <em>Publications 2019</em> 7 (3): 1–15. https://ideas.repec.org/a/gam/jpubli/v7y2019i3p52-d249307.html.</p>
<p>Schöch, Christof. 2017. “Wiederholende Forschung in Den Digitalen Geisteswissenschaften.” In <em>Konferenzabstracts DHd2017: Digitale Nachhaltigkeit</em>, edited by DHd-Verband. https://doi.org/http://doi.org/10.5281/zenodo.277113.</p>
<p>Sinclair, Stéfan, and Geoffrey Rockwell. 2015. “Epistemologica.” 2015. https://github.com/sgsinclair/epistemologica.</p>
<p>Sutton, Adam, Thomas Lansdall, and Nello Cristianini. 2018. “Biased Embeddings from Wild Data: Measuring, Understanding and Removing.” In <em>Advances in Intelligent Data Analysis XVII</em>. https://doi.org/https://doi.org/10.1007/978-3-030-01768-2\_27.</p>
<p>Underwood, Ted. 2019. <em>Distant Horizons: Digital Evidence and Literary Change</em>. Chicago: The University of Chicago Press.</p>
<p><strong>Notes</strong></p>
<p>1. See: https://nips.cc/Conferences/2019/Committees.</p>
<p>2. For a selection of responses, see relevant contributions to <em>Cultural Analytics</em>: https://culturalanalytics.org/articles?tag=replication</p>
<p>3. For an earlier iteration of the typology, see Schöch (2017). For typologies in other fields, see Gómez, Juristo, and Vegas (2010) and Hüffmeier, Mazei, and Schulte (2015).</p>
<p>4. A practice-oriented guide is Deckker and Lackie (2016).</p>
<p>5. See: https://www.nwo.nl/en/research-and-results/programmes/replication+studies. Call for applications:</p>
<p>https://www.nwo.nl/en/funding/our-funding-instruments/sgw/replication-studies/replication-studies.html.</p>
<p>6. The discussion was continued in (Peels and Bouter 2018b), Peels and Bouter (2018a) and in Pender, Holbrook, and De Rijke (2019).</p>
<p>7. See: https://www.nwo.nl/en/news-and-events/news/2019/03/third-round-in-pilot-replication-studies-now-includes-the-humanities.html.</p>
<p>8. See: https://www.nwo.nl/en/research-and-results/programmes/magw/replication-studies/awards-2019.html.</p>
","Technical Note
 A PDF version of this contribution is available at Humanities Commons: https://hcommons.org/deposits/item/hc:30439 Several versions of this contribution in structured formats, including figures and bibliographical data, are available from Zenodo: http://doi.org/10.5281/zenodo.3893428
 Panel Overview
The ""replication crisis"" that has been raging in fields like Psychology (Open Science Collaboration 2015) or Medicine (Ioannidis 2005) for years has recently reached the field of Artificial Intelligence (Barber 2019). One of the key conferences in the field, NeurIPS, has reacted by appointing 'reproducibility chairs' in their organizing committee 1. In the Digital Humanities, and particularly in Computational Literary Studies (CLS), there is an increasing awareness of the crucial role played by replication in evidence-based research. Relevant disciplinary developments include the increased importance of evaluation in text analysis and the increased interest in making research transparent through publicly accessible data and code (open source, open data). Specific impulses include Geoffrey Rockwell and Stéfan Sinclair's re-enactments of pre-digital studies (Sinclair and Rockwell 2015) or the recent replication study by Nan Z. Da (Da 2019). The paper has been met by an avalanche of responses that pushed back several of its key claims, including its rather sweeping condemnation of the replicated papers. However, an important point got buried in the process: that replication is indeed a valuable goal and practice. 2As stated in the Open Science Collaboration paper: ""Replication can increase certainty when findings are reproduced and promote innovation when they are not"" (Open Science Collaboration 2015, 943).
As a consequence, the panel aims to raise a number of issues regarding the place, types, challenges and affordances, both on a practical and on a policy or community level, of replication in CLS. Several impulse papers will address key aspects of the issue: recent experience with attempts at replication of specific papers; policies dealing with replication in fields with more experience in the issue; conceptual and terminological clarification with regard to replication studies; and proposals for a way forward with replication as a community task or a policy issue.
Contribution 1: ""A typology of replication studies"", by Christof Schöch
This contribution aims to provide orientation about the range of existing replication studies, based on a simple typology. The typology describes the relationship between an earlier study and its replication in terms of four key variables: the research question, the method of analysis (including the implementation of that method) and the dataset used. 3 For each of these variables, a replication study can attempt to operate either in the same way as the previous study, or in a different way. Note that the typology is not meant to establish these distinctions in a purely binary fashion: rather, as data or methods are never entirely identical or completely different from the earlier study, the extreme points in the typology are meant to open up a gradient of pratices. In addition (and this aspect might be unique to the Digital Humanities), replication studies can be described as involving crossing the boundary between non-digital and digital research or between qualitative and quantitative research.
[Figure 1: Typology of repeating research]
At the most fundamental level, such a typology structures the field and provides a clearly-defined terminology and systematic relations between the various types. For example, replication vs. reproduction or re-analysis vs. follow-up research. Such a shared understanding is useful because each type of replication study comes with its own objectives, requirements and challenges as well as their own place and function in the research process. A replication study strictly repeating key aspects of an earlier study will be most useful in reviewing and quality assessment, while follow-up research more loosely modeled after an earlier study may instead have important methodological implications or lead to knew domain knowledge. This is true despite the fact that in reality, there is more to consider than a few binary categories when describing a given replication study.
But understanding replication through such a typology can have an impact on the field of DH in a number of additional ways. It can provide guidance when publishing research and help clarify what needs to be provided (in terms of data, code and contextualization in prose) in order for a study to be amenable to a specific type of replication. It can help assess the merits and limitations of a given replication study to assess whether, given the stated objectives of the authors, they have employed a suitable type of replication strategy. And it can support designing a replication study and clarify what data, code and contextual information needs to be obtained or reconstructed in order to perform a specific type of replication. 4
Beyond this, such a typology can contribute to better define the relationship between replication in the strict sense and related efforts like benchmarking and evaluation studies. Finally, because such a typology makes it easier to identify similar studies across disciplinary boundaries, it may help us as a field learn more quickly from other fields with a longer tradition in (specific types of) replication studies. In this way, such a typology of replication studies can contribute to establishing replication as a well-understood part of Computational Literary Studies.
Contribution 2: ""Replication to the Rescue: Funding Strategies"", by Karina Van Dalen-Oskam
The Dutch Research Council (NWO) is the first funding agency to take the initiative for a pilot programme for Replication Studies. Their aim is ""to encourage researchers to carry out replication research. NWO wants to gain experience that can lead to insights into how replication research can be effectively included in research programmes. That experience should also lead to insights into and a reflection on the requirements that NWO sets for research in terms of methodology and transparency."" 5The first two rounds of funding were in 2017 and 2018, and were aimed at the Social Sciences and Medical Sciences. In the third round in 2019, the Humanities were included.
This was done after a heated discussion in Nature between Rik Peels and Lex Bouter (chair of the Replication Studies Programme Committee) on the one hand, and Bart Pender, Sarah de Rijcke and J. Britt Holbrook on the other. Peels and Bouter (2018) started of with a note titled ""Humanities need a replication drive too"". De Rijke and Penders (2018), both scholars from Science and Technology Studies, countered with the call to ""Resist calls for replicability in the humanities"". They argue that quality criteria are crucially different in the humanities and the sciences. 6
NWO went ahead with including the humanities in the call for replication studies, stating they are aware that not all humanities research is suitable for replication. NWO ""expresses no preference or opinion about the value of various methods of research. Where possible it wants to encourage and facilitate the replication of humanities research: this should certainly be possible in the empirical humanities."" 7In March 2020, seven proposals were awarded funding, but none of these can be called typical Humanities projects. 8How many submissions were received from humanities applicants - did scholars indeed resist, as Pender and De Rijcke advised? And in a wider context: How do Dutch Humanities scholars evaluate the new possibility? And does this agree with the reception in the growing and very active Dutch Digital Humanities community?
In my short impulse paper, I will reflect on what we can learn from the explicit invitation to the humanties to apply for funding for replication studies. What does this tell us about the status of humanities research in the Netherlands, and more specifically about the role of the Digital Humanities? I will pay special attention to the opportunities these developments may have for Computational Literary Studies. Should we consider the situation as ""Funding Strategies to the Rescue: Replication"", so a turning around of the title of my talk?
Contribution 3: ""Replication of quantitative and qualitative research - a case study"", by Fotis Jannidis
Literary studies always had an empirical side - 'empirical' in the broader sense, that claims and counterclaims are substantiated by referring to specific parts of texts. These text segments are regarded as indicators which in their sum make a more general point plausible, for example the use of specific terms to validate a hypothesis about a text. Therefore, the concept of replication warrants a wider understanding in Computational Literary Studies. The prototypical center is the quantitative replication of quantitative research, but it also includes quantitative replication of qualitative philological research: Using the same indicators to validate the same hypothesis but moving the research into an empirical framework. Seen in the context of the discussion of mixed methods, this is a specific case of ‘triangulation’. Triangulation refers to “the application of different data analysis methods, different data sets, or different researchers’ perspective to examine the same research question or theme” (Bergin 2018, 29). But here, data sets and data analysis methods overlap strongly, while the research framework is changed from hermeneutic to quantitative.
Our case study is an attempt to replicate research on the complexity of language in German dime novels, published by Peter Nusser (Nusser 1981), and it demands both kinds of replication. Nusser describes the language of dime novels on three levels: vocabulary, syntax, and phrases. The work on vocabulary and syntax is quantitative, while the analysis of phrases is qualitative. The replication of the quantitative parts is made more difficult by the fact that the results which Nusser reports have actually been produced by another author in the context of an unprinted exam thesis which seems to be lost for the moment. So a lot of information is missing, and we can only make educated guesses: the exact corpus design (for high literature only the authors are given and for dime novels only the series), the strategies of tokenization and sentence splitting, the exact formula for calculating specific values, etc. The qualitative research is enumerating many phrases which are seen as examples of clichés and there is no explicit comparison with high literature. So a quantification must try to operationalize the concept of cliché and then compare retrieval results between dime novels and high literature.
As is well known in Computational Literary Studies, operationalization as an instance of formal modeling usually covers some aspects that are part of the intuitive notion, while others are excluded for the time being and it is one goal to reduce the loss (Moretti (2013); for a counterposition see Underwood (2019, 181)). In a replication, the loss may be responsible for the difference in outcome. In view of all these difficulties it could seem an unnecessary endeavor to replicate the research, but Nusser’s study had a huge influence on the assessment and evaluation of popular literature in German studies for almost four decades.
Contribution 4: ""Reliable methods for text analysis"", by Maria Antoniak and David Mimno
If we are to make reproducible computational claims about literary texts, we need methods that lend themselves to robustness and reliability. Here we focus on the case study of word embeddings, which analyze collections of documents and produce numeric representations of words. Although these methods are powerful, they are also at high risk for problems with reproducibility: they are complicated enough to be essentially ""black boxes"", yet they are also known to be highly sensitive to text curation choices, parameter settings, and even random initializations (Antoniak and Mimno 2018). How can we assure researchers and their audiences that seemingly small changes would not alter or even reverse their findings?
Embedding vectors are useful for their ability to operationalize thick cultural concepts. For example, the resulting vectors have been used to measure shifts in word meaning over time and geographic areas (e.g. Hamilton, Lescovec, and Jurafsky (2016); Kulkarni, Perozzi, and Skiena (2016)). Several studies have shown that embeddings can encode gender biases by probing embedding spaces using carefully chosen seed words (Gordon and Van Durme (2013); (Bolukbasi et al. 2016a); Caliskan Islam, Bryson, and Narayanan (2016)). Subsequent work in natural language processing has focused on removing biases from an embedding model (Bolukbasi et al. (2016b); Sutton, Lansdall, and Cristianini (2018)). In this context, the concern is the downstream impact of bias on systems that use embeddings, but similar work can also be motivated from an upstream perspective, as a means of studying bias in collections.
Researchers from the humanities and social sciences use embeddings to provide quantitative answers to otherwise elusive political and social questions about the training corpus and its authors (e.g. Kozlowski, Taddy, and Evans (2019)). These bias detection techniques were originally intended to measure the bias encoded in a trained embedding; they were not originally tested to measure the bias of a corpus and make comparisons between corpora.
We probe the stability of these measurements by testing two popular bias detection methods ( Bolukbasi et al. (2016a); Caliskan Islam, Bryson, and Narayanan (2016)) on sets of automatically constructed seed sets. These sets were constructed by randomly selecting a target term and then including its N nearest neighbors in the set; this process more closely approximates a real seed set, constructed by a scholar interested in a particular concept, than a random set of seeds. We find that bias detection techniques via word embeddings are susceptible to variability in the seed terms, in both their order (alternative pairings of seeds from two sets can significantly change the ability of the method to capture a single bias subspace) and semantic similarity (the more similar seeds set are to each other, the more difficult it is to measure their biases). If done carefully, bias detection using embeddings is feasible even for small, subdivided collections and can provide a promising tool for differential content analysis, but we encourage error analysis of the seed terms.
We further highlight a central inconsistency in these bias detection methods. While these methods seek to measure biases in datasets, the researcher-selected seeds themselves can contain a variety of biases. For example, the seeds used for racial categories often include lists of names that are ""African American"" or ""European."" Such lists can be both reductive and essentializing. In addition, some seed sets contain confounding terms, e.g., contain a gendered term in a seed set for ""domestic work"" that is then used to measure gender bias. If the seed set for ""domestic work"" appears closer to the gender that it contains, it will be impossible to say whether that bias exists because of the training corpus or because of the inclusion of the gendered seed.
This case study highlights the reversal in perspectives when techniques from natural language processing and machine learning are re-purposed for studies of specialized datasets. Some working assumptions from the machine learning community (e.g. large size of training set) are broken in the humanities context, where datasets are non-expandable and are the primary focus of the study, rather than a generalized training set for downstream applications. The stability and robustness of these repurposings should not be assumed but rather should be reanalyzed for the particular new contexts.
Bibliography
Antoniak, Maria, and David Mimno. 2018. “Evaluating the Stability of Embedding-Based Word Similarities.” Transactions of the Association for Computational Linguistics 6. https://transacl.org/ojs/index.php/tacl/article/view/1202.
Barber, Gregory. 2019. “Artificial Intelligence Confronts a ‘Reproducibility’ Crisis.” Wired, 2019. https://www.wired.com/story/artificial-intelligence-confronts-reproducibility-crisis/.
Bergin, Tiffany. 2018. An Introduction to Data Analysis. Quantitative, Qualitative and Mixed Methods. London: Sage.
Bolukbasi, Tolga, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam Tauman Kalai. 2016a. “Man Is to Computer Programmer as Woman Is to Homemaker? Debiasing Word Embeddings.” In NIPS. https://arxiv.org/abs/1607.06520.
Bolukbasi, Tolga, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam Tauman Kalai. 2016b. “Quantifying and Reducing Stereotypes in Word Embeddings.” In Data4Good: Machine Learning in Social Good Applications, edited by Kush R. Varshney. http://arxiv.org/abs/1606.06121.
Caliskan Islam, Aylin, Joanna J. Bryson, and Arvind Narayanan. 2016. “Semantics Derived Automatically from Language Corpora Necessarily Contain Human Biases.” In CoRR. http://arxiv.org/abs/1608.07187.
Da, Nan Z. 2019. “The Computational Case Against Computational Literary Studies.” Critical Inquiry 45 (3): 601–39. https://www.journals.uchicago.edu/doi/abs/10.1086/702594.
De Rijke, Sarah, and Bart Penders. 2018. “Resist Calls for Replicability in the Humanities.” Nature 560 (29). https://www.nature.com/articles/d41586-018-05845-z.
Deckker, Harrison, and Paula Lackie. 2016. “Technical Data Skills for Reproducible Research.” Edited by Lynda M. Kellam and Kristi Thompson. Databrarianship: The Academic Data Librarian in Theory and Practice. https://escholarship.org/uc/item/8qb2q8fk.
Gómez, Omar S., Natalia Juristo, and Sira Vegas. 2010. “Replication, Reproduction and Re-Analysis: Three Ways for Verifying Experimental Findings.” In RESER ’2010 Cape Town.
Gordon, Jonathan, and Benjamin Van Durme. 2013. “Reporting Bias and Knowledge Acquisition.” In Proceedings of the 2013 Workshop on Automated Knowledge Base Construction. https://doi.org/https://doi.org/10.1145/2509558.2509563.
Hamilton, William L., Jure Lescovec, and Dan Jurafsky. 2016. “Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change.” Edited by Hamilton, Jure Lescovec, and Dan Jurafsky. CoRR. http://arxiv.org/abs/1605.09096.
Hüffmeier, Joachim, Jens Mazei, and Thomas Schulte. 2015. “Reconceptualizing Replication as a Sequence of Different Studies: A Replication Typology.” Journal of Experimental Social Psychology, 81–92. https://www.sciencedirect.com/science/article/pii/S0022103115001195.
Ioannidis, J.A. 2005. “Contradicted and Initially Stronger Effects in Highly Cited Clinical Research.” JAMA, no. 294/2: 218–28. https://doi.org/https://doi:10.1001/jama.294.2.218.
Kozlowski, Austin C., Matt Taddy, and James A. Evans. 2019. “The Geometry of Culture: Analyzing Meaning through Word Embeddings.” American Sociological Review 84 (5). https://doi.org/https://doi.org/10.1177%2F0003122419877135.
Kulkarni, Vivek, Bryan Perozzi, and Steven Skiena. 2016. “Freshman or Fresher? Quantifying the Geographic Variation of Internet Language.” In Proceedings of the Tenth International Conference on Web and Social Media. http://www.aaai.org/ocs/index.php/ICWSM/ICWSM16/paper/view/13121.
Moretti, Franco. 2013. “‘Operationalizing’: Or, The Function of Measurement in Modern Literary Theory.” Stanford Literary Lab Pamphlet 6. https://litlab.stanford.edu/LiteraryLabPamphlet6.pdf.
Nusser, Peter. 1981. Romane für die Unterschicht. Der Groschenroman und seine Leser [1973]. Stuttgart: Metzler.
Open Science Collaboration. 2015. “Estimating the Reproducibility of Psychological Science.” Science, no. 349 (6251). https://science.sciencemag.org/content/349/6251/aac4716.
Peels, Rik, and Lex Bouter. 2018. “Humanities Need a Replication Drive Too.” Nature 558 (372). https://www.nature.com/articles/d41586-018-05454-w.
Peels, Rik, and Lex Bouter. 2018a. “Replication Is Both Possible and Desirable in the Humanities, Just as It Is in the Sciences.” LSE Impact Blog, 2018. http://blogs.lse.ac.uk/impactofsocialsciences/2018/10/01/replication-is-both-possible-and-desirable-in-the-humanities-just-as-it-is-in-the-sciences/.
Peels, Rik, and Lex Bouter. 2018b. “The Possibility and Desirability of Replication in the Humanities.” Palgrave Communications 4 (95). https://science.sciencemag.org/content/349/6251/aac4716.
Pender, Bart, J. Britt Holbrook, and Sarah De Rijke. 2019. “Rinse and Repeat: Understanding the Value of Replication across Different Ways of Knowing.” Publications 2019 7 (3): 1–15. https://ideas.repec.org/a/gam/jpubli/v7y2019i3p52-d249307.html.
Schöch, Christof. 2017. “Wiederholende Forschung in Den Digitalen Geisteswissenschaften.” In Konferenzabstracts DHd2017: Digitale Nachhaltigkeit, edited by DHd-Verband. https://doi.org/http://doi.org/10.5281/zenodo.277113.
Sinclair, Stéfan, and Geoffrey Rockwell. 2015. “Epistemologica.” 2015. https://github.com/sgsinclair/epistemologica.
Sutton, Adam, Thomas Lansdall, and Nello Cristianini. 2018. “Biased Embeddings from Wild Data: Measuring, Understanding and Removing.” In Advances in Intelligent Data Analysis XVII. https://doi.org/https://doi.org/10.1007/978-3-030-01768-2\_27.
Underwood, Ted. 2019. Distant Horizons: Digital Evidence and Literary Change. Chicago: The University of Chicago Press.
Notes
1. See: https://nips.cc/Conferences/2019/Committees.
2. For a selection of responses, see relevant contributions to Cultural Analytics: https://culturalanalytics.org/articles?tag=replication
3. For an earlier iteration of the typology, see Schöch (2017). For typologies in other fields, see Gómez, Juristo, and Vegas (2010) and Hüffmeier, Mazei, and Schulte (2015).
4. A practice-oriented guide is Deckker and Lackie (2016).
5. See: https://www.nwo.nl/en/research-and-results/programmes/replication+studies. Call for applications:
https://www.nwo.nl/en/funding/our-funding-instruments/sgw/replication-studies/replication-studies.html.
6. The discussion was continued in (Peels and Bouter 2018b), Peels and Bouter (2018a) and in Pender, Holbrook, and De Rijke (2019).
7. See: https://www.nwo.nl/en/news-and-events/news/2019/03/third-round-in-pilot-replication-studies-now-includes-the-humanities.html.
8. See: https://www.nwo.nl/en/research-and-results/programmes/magw/replication-studies/awards-2019.html.","schoech@uni-trier.de, karina.van.dalen@huygens.knaw.nl, maa343@cornell.edu, fotis.jannidis@uni-wuerzburg.de, mimno@cornell.edu",Panel
"Tayler, Felicity (1);
Simpkin, Sarah (1);
Mitchell, Marjorie (2);
Crompton, Constance (1);
Shearer, Karis (2);
Lincoln, Matthew (3);
Proulx, Mikhel (4);
Goodchild, Meghan (5)","1: University of Ottawa, Canada;
2: University of British Columbia Okanagan;
3: Carnegie Mellon University Libraries;
4: Concordia University;
5: Queen’s University and Scholars Portal, Ontario Council of University Libraries",Making Research Data Public: Workshopping Data Curation for Digital Humanities Projects,"data communities, research data management, data curation, digital scholarship, best practices","English
North America
Contemporary
data publishing projects, systems, and methods
digital ecologies and digital communities creation management and analysis
Library & information science",English,North America,Contemporary,"data publishing projects, systems, and methods
digital ecologies and digital communities creation management and analysis",Library & information science,"<p>A lack of formal training opportunities for data curation in multi-site DH teams means that the data produced in these teams is in danger of being lost! This four-hour workshop will cover all areas of data management including: IP permissions and informed consent, data collection, metadata standards, file sharing, preservation (data deposit), and data sharing through the open data spectrum of access. Participants will work on their own data curation challenges in break-out sessions and with reference to case study examples presented by a panel of DH scholars and digital asset management specialists: Constance Crompton (uOttawa), Karis Shearer (UBCO), Matthew Lincoln (Carnegie-Mellon U), Mikhel Proulx (Concordia U and Indigenous Digital Art Archive), Meghan Goodchild (Queen’s U and Scholars Portal). The lesson plan is designed and delivered by Felicity Tayler (uOttawa), Sarah Simpkin (uOttawa), and Marjorie Mitchell (UBCO).</p>
<p>This workshop is a good preparation for researchers who must create a data management plan to comply with funding agency requirements. The workshop arises at a moment when DH researchers have greater access to funding to support large-scale multi-partner projects with diverse digital assets. The manifold nature of DH, and its reflexive challenges to culturally imposed power imbalances in digital systems presents unique challenges for data curation. Responding to the conference thematic of cultural and disciplinary intersections, this workshop proposes that DH is one of the social and conceptual spaces where the informal networks of international “data communities” arise through acts of data curation and sharing (Cooper and Springer). Our approach to data curation recognizes that data communities are multilingual and multi-cultural just as they cross epistemological and disciplinary lines.</p>
<p><strong>Contact information of co-facilitators </strong></p>
<p>Felicity Tayler, Research Data Management Librarian, University of Ottawa <br />ftayler@uottawa.ca | https://uniweb.uottawa.ca/members/3883</p>
<p>Example of workshop previously led on this topic: <br />“Data Curation for Communities of Sound: A Speculative Workshop” led by Felicity Tayler and Marjorie Mitchell. SpokenWeb Symposium. Simon Fraser University, BC. 30 May 2019; OLA SuperConference 2020, Toronto, ON. 30 January 2020. <br />https://www.olasuperconference.ca/conf_workshop/data-curation-for-communities-of-sound-a-speculative-workshop/</p>
<p>Sarah Simpkin, Head, Research Support (Arts and Special Collections), University of Ottawa <br />sarah.simpkin@uottawa.ca | https://orcid.org/0000-0003-0774-4777</p>
<p>Marjorie Mitchell, Copyright, Scholarly Communications, and Research Data Management Librarian, Research Services, University of British Columbia Okanagan Marjorie.Mitchell@ubc.ca | https://spokenweb.ca/</p>
<p><strong>Contact information of confirmed presenters/affiliated authors </strong></p>
<p>Constance Crompton, Canada Research Chair in Digital Humanities in the Department of Communication, University of Ottawa ccrompto@uottawa.ca | http://constancecrompton.com</p>
<p>Karis Shearer, Associate Professor, English and Cultural Studies, University of British Columbia Okanagan karis.shearer@ubc.ca | http://amplab.ok.ubc.ca/</p>
<p>Matthew Lincoln, Research Software Engineer, Carnegie Mellon University Libraries mlincoln@andrew.cmu.edu | https://matthewlincoln.net/</p>
<p>Mikhel Proulx, Indigenous Digital Art Archive, Concordia University, <br />mikhel.proulx@concordia.ca | http://indigenousfutures.net/archive/ <br />Meghan Goodchild, Acting Head, Discovery and Technology Services, Research Data Management Systems Librarian, Queen’s University and Scholars Portal, Ontario Council of University Libraries (OCUL), meghan.goodchild@queensu.ca | https://www.actorproject.org/</p>
<p><strong>Description of target audience</strong></p>
<p>This workshop is of interest to DH researchers working in national and international collaborative teams, in particular, those who are funded through agencies that have Data Management Plan requirements. Students and Research Assistants will also benefit from this training, as the next generation of researchers who will adopt these data curation best practices. As a key focus of the conference is First Nations, Native American, and Indigenous Studies (FNAIS), we anticipate that many workshop attendees will bring knowledge systems into the room with them that challenge data curation best practices based in Western traditions of property ownership and colonial methods of data collection - we will address these gaps.</p>
<p><strong>Technical requirements </strong></p>
<p>Should this workshop be accepted, the University of Ottawa Library has committed to hosting us in its “Tinkering Lab,” designed for collaborative learning styles and data visualization. The room is equipped with an 8K LED video wall, and 2 LED 4k displays, as well as capacity for up to 50 participants seated in modular group learning dynamics. We will provide our own technical support. Refreshments will be served in the mezzanine area outside of the Lab.</p>
<p><strong>Syllabus for proposed workshop</strong></p>
<p>Making Research Data Public: <br />Workshopping Data Curation for Digital Humanities Projects <br />Prepared by: Felicity Tayler, Sarah Simpkin, Marjorie Mitchell</p>
<p><strong>Learning Outcome: </strong></p>
<p>By the end of this session, participants will be able to understand data curation terms and best practices, in a way that is meaningful to their own disciplinary and computational methodologies.</p>
<p><strong>Learning Objectives:</strong></p>
<p>Participants will be able to <br />1) Recognize and describe their research data <br />2) Identify data curation practices they already undertake <br />3) Describe data management best practices that they could adopt to better achieve their goals <br />4) Summarize what they have learned as it applies to their own projects</p>
<p><strong>Lesson Content / Process: </strong></p>
<p>40 mins</p>
<ol start=""1""><li>
<p>What is your data? Introduction to data curation key concepts (Presenters: Felicity Tayler, Sarah Simpkin, Marjorie Mitchell)</p>
</li>
</ol><ol start=""2""><li>
<p>Introduction to data management plans in international contexts (DMPOnline (UK), DMPTool (USA); DMP Assistant (Canada)</p>
</li>
</ol><p>Activity: Ask group about what data they are producing/working with.</p>
<p>---</p>
<p>60 mins with Q&A</p>
<ol start=""3""><li>
<p>“Learning from examples” DH data curation successes (and failures) presentation of 1-4 case studies</p>
</li>
</ol><ul><li>
<p>Constance Crompton: LINCS: Linked Infrastructure for Networked Cultural Scholarship</p>
</li>
<li>
<p>Karis Shearer: Press Play: Making Spoken Web Research Data Public</p>
</li>
<li>
<p>Matthew Lincoln: Humanities Data: The Middle Path for Real-Life Researchers</p>
</li>
<li>
<p>Mikhel Proulx: Indigenous Data Sovereignty and Ethical Allyship in the Archive</p>
</li>
<li>
<p>Meghan Goodchild: Collaborating on ACTOR (Analysis, Creation, and Teaching of Orchestration)</p>
</li>
</ul><p>---</p>
<p>30 min</p>
<p>Refreshments & networking</p>
<p>---</p>
<p>30 min</p>
<ol start=""4""><li>
<p>“What is Your Data Flow and Discovery Model?” <br />How to map out your research data at different phases of the project addressing: IP permissions and informed consent, data collection and storage, metadata description, file sharing, preservation (data deposit), data sharing through the spectrum of access, publication, credit and citation.</p>
</li>
</ol><p>Activity: Facilitator-led break-out sessions (F. Tayler, S. Simpkin, and M. Mitchell). Participants work individually or in a group.</p>
<p>---</p>
<p>10 min</p>
<p>Break</p>
<p>---</p>
<p>30 min</p>
<ol start=""5""><li>
<p>“What is Your Spectrum of Data Access?”</p>
</li>
</ol><p>How to map your data flow models onto 5 categories <br />of access from secure & protected to open license.</p>
<p>Data Papers & Data Journals</p>
<p>Activity: Facilitator-led break-out sessions (F. Tayler, S. Simpkin and M. Mitchell). Participants work individually or in a group.</p>
<p>---</p>
<p>10 min</p>
<p>Break</p>
<p>---</p>
<p>30 min</p>
<p>Sharing & feed-back session</p>
<p>Participants synthesize what they have learned by presenting their data flows in small groups for feed-back</p>
<p><strong>Some relevant readings and resources </strong></p>
<p>Anderson, Katrina and et al. ""Student Labour and Training in Digital Humanities."" DHQ: Digital Humanities Quarterly 10.1 (2016): n.p. Web. <http://www.digitalhumanities.org/dhq/vol/10/1/000233/000233.html>.</p>
<p>Institute for Critical Indigenous Studies. “Episode 2: Indigeneity in DH.” Recoding Relations. Web. <https://www.recodingrelations.org/></p>
<p>Canadian Writing Research Collaboratory. LINCS Cyberinfrastructure Project. 2019. Web. <br /><https://cwrc.ca/news/LINCS-CFI></p>
<p>Carnegie Mellon University. The Digital Humanities Literacy Guidebook. 2019. Web.  https://cmu-lib.github.io/dhlg/</p>
<p>Cooper, Danielle and Rebecca Springer. ""Data Communities: A New Model for Supporting STEM Data Sharing."" ITHKA S+R (2019): 1-26. Web. <https://doi.org/10.18665/sr.311396>.</p>
<p>Data Curation Network. Data Curation Network Primers. 2018. Web. <http://hdl.handle.net/11299/202810>.</p>
<p>Digital Curation Centre. About DMP Online. 2019. Web. <https://dmponline.dcc.ac.uk/about_us>.</p>
<p>Maryland Institute for Technology in the Humanities. Digital Humanities Data Curation Guide. [2014]. Web. <https://guide.dhcuration.org/</p>
<p>McGovern, Nancy Y. ""Radical Collaboration and Research Data Management: An Introduction."" Research Library Issues 296 (2018): 6-22. Web. <https://doi.org/10.29242/rli.296.2>.</p>
<p>Posner, Miriam. ""What's Next: The Radical, Unrealized Potential of Digital Humanities."" Gold, Matthew K. and Lauren F. Klein. Debates in the Digital Humanities 2016. Minneapolis: University of Minnesota Press, 2016. 32-41. Print.</p>
<p>Portage Network. DMP Assistant. Web. <https://assistant.portagenetwork.ca/></p>
<p>Tayler, Felicity; Marjorie Mitchell, and Rebecca Springer. “Emergent Data Community Spotlight II: An interview with Felicity Tayler and Marjorie Mitchell on the Spoken Web Project.” Ithaka S+R. Web. 10 September 2019. < https://sr.ithaka.org/blog/emergent-data-community-spotlight-ii/></p>
<p>University of California Curation Centre. DMP Tool : Data Management General Guidance. Web. 2019. <https://dmptool.org/general_guidance>.</p>
<p>Wernimont, Jacqueline and Elizabeth Losh, eds. Bodies of Information: Intersectional Feminism and the Digital Humanities. Minneapolis: University of Minnesota Press, 2018.</p>
","A lack of formal training opportunities for data curation in multi-site DH teams means that the data produced in these teams is in danger of being lost! This four-hour workshop will cover all areas of data management including: IP permissions and informed consent, data collection, metadata standards, file sharing, preservation (data deposit), and data sharing through the open data spectrum of access. Participants will work on their own data curation challenges in break-out sessions and with reference to case study examples presented by a panel of DH scholars and digital asset management specialists: Constance Crompton (uOttawa), Karis Shearer (UBCO), Matthew Lincoln (Carnegie-Mellon U), Mikhel Proulx (Concordia U and Indigenous Digital Art Archive), Meghan Goodchild (Queen’s U and Scholars Portal). The lesson plan is designed and delivered by Felicity Tayler (uOttawa), Sarah Simpkin (uOttawa), and Marjorie Mitchell (UBCO).
This workshop is a good preparation for researchers who must create a data management plan to comply with funding agency requirements. The workshop arises at a moment when DH researchers have greater access to funding to support large-scale multi-partner projects with diverse digital assets. The manifold nature of DH, and its reflexive challenges to culturally imposed power imbalances in digital systems presents unique challenges for data curation. Responding to the conference thematic of cultural and disciplinary intersections, this workshop proposes that DH is one of the social and conceptual spaces where the informal networks of international “data communities” arise through acts of data curation and sharing (Cooper and Springer). Our approach to data curation recognizes that data communities are multilingual and multi-cultural just as they cross epistemological and disciplinary lines.
Contact information of co-facilitators 
Felicity Tayler, Research Data Management Librarian, University of Ottawa 
ftayler@uottawa.ca https://uniweb.uottawa.ca/members/3883
Example of workshop previously led on this topic: 
“Data Curation for Communities of Sound: A Speculative Workshop” led by Felicity Tayler and Marjorie Mitchell. SpokenWeb Symposium. Simon Fraser University, BC. 30 May 2019; OLA SuperConference 2020, Toronto, ON. 30 January 2020. 
https://www.olasuperconference.ca/conf_workshop/data-curation-for-communities-of-sound-a-speculative-workshop/
Sarah Simpkin, Head, Research Support (Arts and Special Collections), University of Ottawa 
sarah.simpkin@uottawa.ca https://orcid.org/0000-0003-0774-4777
Marjorie Mitchell, Copyright, Scholarly Communications, and Research Data Management Librarian, Research Services, University of British Columbia Okanagan Marjorie.Mitchell@ubc.ca https://spokenweb.ca/
Contact information of confirmed presenters/affiliated authors 
Constance Crompton, Canada Research Chair in Digital Humanities in the Department of Communication, University of Ottawa ccrompto@uottawa.ca http://constancecrompton.com
Karis Shearer, Associate Professor, English and Cultural Studies, University of British Columbia Okanagan karis.shearer@ubc.ca http://amplab.ok.ubc.ca/
Matthew Lincoln, Research Software Engineer, Carnegie Mellon University Libraries mlincoln@andrew.cmu.edu https://matthewlincoln.net/
Mikhel Proulx, Indigenous Digital Art Archive, Concordia University, 
mikhel.proulx@concordia.ca http://indigenousfutures.net/archive/ 
Meghan Goodchild, Acting Head, Discovery and Technology Services, Research Data Management Systems Librarian, Queen’s University and Scholars Portal, Ontario Council of University Libraries (OCUL), meghan.goodchild@queensu.ca https://www.actorproject.org/
Description of target audience
This workshop is of interest to DH researchers working in national and international collaborative teams, in particular, those who are funded through agencies that have Data Management Plan requirements. Students and Research Assistants will also benefit from this training, as the next generation of researchers who will adopt these data curation best practices. As a key focus of the conference is First Nations, Native American, and Indigenous Studies (FNAIS), we anticipate that many workshop attendees will bring knowledge systems into the room with them that challenge data curation best practices based in Western traditions of property ownership and colonial methods of data collection - we will address these gaps.
Technical requirements 
Should this workshop be accepted, the University of Ottawa Library has committed to hosting us in its “Tinkering Lab,” designed for collaborative learning styles and data visualization. The room is equipped with an 8K LED video wall, and 2 LED 4k displays, as well as capacity for up to 50 participants seated in modular group learning dynamics. We will provide our own technical support. Refreshments will be served in the mezzanine area outside of the Lab.
Syllabus for proposed workshop
Making Research Data Public: 
Workshopping Data Curation for Digital Humanities Projects 
Prepared by: Felicity Tayler, Sarah Simpkin, Marjorie Mitchell
Learning Outcome: 
By the end of this session, participants will be able to understand data curation terms and best practices, in a way that is meaningful to their own disciplinary and computational methodologies.
Learning Objectives:
Participants will be able to 
1) Recognize and describe their research data 
2) Identify data curation practices they already undertake 
3) Describe data management best practices that they could adopt to better achieve their goals 
4) Summarize what they have learned as it applies to their own projects
Lesson Content / Process: 
40 mins
What is your data? Introduction to data curation key concepts (Presenters: Felicity Tayler, Sarah Simpkin, Marjorie Mitchell)
Introduction to data management plans in international contexts (DMPOnline (UK), DMPTool (USA); DMP Assistant (Canada)
Activity: Ask group about what data they are producing/working with.
---
60 mins with Q&A
“Learning from examples” DH data curation successes (and failures) presentation of 1-4 case studies
Constance Crompton: LINCS: Linked Infrastructure for Networked Cultural Scholarship
Karis Shearer: Press Play: Making Spoken Web Research Data Public
Matthew Lincoln: Humanities Data: The Middle Path for Real-Life Researchers
Mikhel Proulx: Indigenous Data Sovereignty and Ethical Allyship in the Archive
Meghan Goodchild: Collaborating on ACTOR (Analysis, Creation, and Teaching of Orchestration)
---
30 min
Refreshments & networking
---
30 min
“What is Your Data Flow and Discovery Model?” 
How to map out your research data at different phases of the project addressing: IP permissions and informed consent, data collection and storage, metadata description, file sharing, preservation (data deposit), data sharing through the spectrum of access, publication, credit and citation.
Activity: Facilitator-led break-out sessions (F. Tayler, S. Simpkin, and M. Mitchell). Participants work individually or in a group.
---
10 min
Break
---
30 min
“What is Your Spectrum of Data Access?”
How to map your data flow models onto 5 categories 
of access from secure & protected to open license.
Data Papers & Data Journals
Activity: Facilitator-led break-out sessions (F. Tayler, S. Simpkin and M. Mitchell). Participants work individually or in a group.
---
10 min
Break
---
30 min
Sharing & feed-back session
Participants synthesize what they have learned by presenting their data flows in small groups for feed-back
Some relevant readings and resources 
Anderson, Katrina and et al. ""Student Labour and Training in Digital Humanities."" DHQ: Digital Humanities Quarterly 10.1 (2016): n.p. Web. <http://www.digitalhumanities.org/dhq/vol/10/1/000233/000233.html>.
Institute for Critical Indigenous Studies. “Episode 2: Indigeneity in DH.” Recoding Relations. Web. <https://www.recodingrelations.org/>
Canadian Writing Research Collaboratory. LINCS Cyberinfrastructure Project. 2019. Web. 
<https://cwrc.ca/news/LINCS-CFI>
Carnegie Mellon University. The Digital Humanities Literacy Guidebook. 2019. Web.  https://cmu-lib.github.io/dhlg/
Cooper, Danielle and Rebecca Springer. ""Data Communities: A New Model for Supporting STEM Data Sharing."" ITHKA S+R (2019): 1-26. Web. <https://doi.org/10.18665/sr.311396>.
Data Curation Network. Data Curation Network Primers. 2018. Web. <http://hdl.handle.net/11299/202810>.
Digital Curation Centre. About DMP Online. 2019. Web. <https://dmponline.dcc.ac.uk/about_us>.
Maryland Institute for Technology in the Humanities. Digital Humanities Data Curation Guide. [2014]. Web. <https://guide.dhcuration.org/
McGovern, Nancy Y. ""Radical Collaboration and Research Data Management: An Introduction."" Research Library Issues 296 (2018): 6-22. Web. <https://doi.org/10.29242/rli.296.2>.
Posner, Miriam. ""What's Next: The Radical, Unrealized Potential of Digital Humanities."" Gold, Matthew K. and Lauren F. Klein. Debates in the Digital Humanities 2016. Minneapolis: University of Minnesota Press, 2016. 32-41. Print.
Portage Network. DMP Assistant. Web. <https://assistant.portagenetwork.ca/>
Tayler, Felicity; Marjorie Mitchell, and Rebecca Springer. “Emergent Data Community Spotlight II: An interview with Felicity Tayler and Marjorie Mitchell on the Spoken Web Project.” Ithaka S+R. Web. 10 September 2019. < https://sr.ithaka.org/blog/emergent-data-community-spotlight-ii/>
University of California Curation Centre. DMP Tool : Data Management General Guidance. Web. 2019. <https://dmptool.org/general_guidance>.
Wernimont, Jacqueline and Elizabeth Losh, eds. Bodies of Information: Intersectional Feminism and the Digital Humanities. Minneapolis: University of Minnesota Press, 2018.","ftayler@uottawa.ca, sarah.simpkin@uottawa.ca, Marjorie.Mitchell@ubc.ca, constance.crompton@uottawa.ca, karis.shearer@ubc.ca, mlincoln@andrew.cmu.edu, mikhel.proulx@concordia.ca, meghan.goodchild@queensu.ca",Workshop/Tutorial 4
"Bentley, Patricia",York University,Intersections of the Cultural Kind: Public Digital Humanities and the Museum,"museum, data analysis, methodology","Asia
English
North America
19th Century
20th Century
Contemporary
cultural analytics
data modeling
Galleries and museum studies",English,"Asia
North America","19th Century
20th Century
Contemporary","cultural analytics
data modeling",Galleries and museum studies,"<p>This presentation examines a recent research intervention that used data mining and data mapping and visualization methods to assess how visitors in a museum of Islamic art were making sense of their encounters with patterned works of art in the galleries.</p>
",This presentation examines a recent research intervention that used data mining and data mapping and visualization methods to assess how visitors in a museum of Islamic art were making sense of their encounters with patterned works of art in the galleries.,bentley.pbentley@gmail.com,Short Presentation
"Mapes, Kristen;
Moll, Ellen;
Petersen, Andy Boyles","Michigan State University, United States of America",Bringing Newcomers into the Fold: Faculty Development through Values-Driven DH Pedagogy,"faculty development, curriculum development, training","English
North America
Contemporary
curricular and pedagogical development and analysis
open access methods
Education/ pedagogy",English,North America,Contemporary,"curricular and pedagogical development and analysis
open access methods",Education/ pedagogy,"<p>A pervasive goal of DH is to bring new and curious faculty into the fold in a sustained way. At Michigan State University, we have developed a highly successful DH Pedagogy Learning Community to help new-to-DH instructors effectively integrate DH into their courses. Key to faculty engagement was our focus on how DH values such as collaboration, community, inclusion, public humanities, and experimentation aligned with their own priorities in teaching. We are now launching an OER that includes the curriculum for this values-driven learning community that can be used by either individual instructors or anyone wishing to lead their own discipline-agnostic learning community for new-to-DH faculty.</p>
","A pervasive goal of DH is to bring new and curious faculty into the fold in a sustained way. At Michigan State University, we have developed a highly successful DH Pedagogy Learning Community to help new-to-DH instructors effectively integrate DH into their courses. Key to faculty engagement was our focus on how DH values such as collaboration, community, inclusion, public humanities, and experimentation aligned with their own priorities in teaching. We are now launching an OER that includes the curriculum for this values-driven learning community that can be used by either individual instructors or anyone wishing to lead their own discipline-agnostic learning community for new-to-DH faculty.","kmapes@msu.edu, mollelle@msu.edu, andyjp@msu.edu",Lightning
"Kim, Hoyeol (1);
Ives, Maura (2)","1: Texas A&M University, United States of America;
2: Texas A&M University, United States of America",Colorization of Illustrations in Charles Dickens’ Novels Using Deep Learning,"Colorization, Deep Learning, Illustration, Victorian Literature, Charles Dickens","Global
Europe
English
North America
19th Century
artificial intelligence and machine learning
digital art production and analysis
Education/ pedagogy
Literary studies",English,"Global
Europe
North America",19th Century,"artificial intelligence and machine learning
digital art production and analysis","Education/ pedagogy
Literary studies","<p>Charles Dickens dedicated himself to the development of Victorian visual culture by actively employing illustrations in his fiction. For Dickens, illustrations were a way to attract readers and boost sales, as demonstrated by the commercial success of <em>The Pickwick Papers</em> (1837), as well as a crucial element of artistic expression. All but two of Dickens’s novels were illustrated, and his involvement in every stage of the illustration process was well documented. Dickens collaborated closely with his illustrators by providing detailed instructions: he provided specific colors for his illustrations, although the illustrations would be printed in black and white, and he intentionally positioned each illustration in a specific location in his serials in order to communicate details and emotions effectively with his readers. Scholars of Dickens’s works generally understand the illustrations to be integral to the text.</p>
<p>Given the expense of printing illustrations in color, most of the illustrations in Dickens’s work were printed in black and white. However, the hand-colored illustrations in <em>A Christmas Carol</em> (1843) demonstrate both Dickens’s interest in providing color when he could, and the significance of color in interpreting design elements as well as shaping interpretation of Dickens’s text. Because colorizing Dickens’s illustrations has the potential for enhancing the reader’s understanding of the text, and for opening up new interpretive possibilities, it has pedagogical implications that we wish to explore.</p>
<p>Our project is the first deep learning colorization venture in Victorian era media. We will colorize all of the illustrations in several of Dickens’s major novels, using the pix2pix model based on cGANs with Kim’s Victorian400 dataset as a research method. The Victorian400 dataset, published as an open data source, is a collection of colorful illustrations painted with nineteenth-century palettes. By using Victorian paintings and hand colored illustrations as a training set, our research methods make it possible for Dickens’s illustrations to be viewed with a Victorian color palette, approximating the color choices that a contemporary audience would have expected. We will present two groups of students with both the colorized and black and white illustrations and ask them to evaluate each in terms of design and in terms of their effect on their engagement with the text. Colorization with deep learning carries possibilities of misrepresentation or misinterpretation, which can be used to clarify students’ preconceptions about the nature or role of illustration and to spur creative responses to the text; it can also provide interest, anticipation, and imagination for readers. As a pedagogical tool, we hope that colorization using deep learning will improve students’ ability to think critically about the intersection of text and image while promoting deeper understanding and interest in Dickens’s works, and lay the groundwork for experimentation with deep learning colorization of other illustrated works by Victorian authors.</p>
","Charles Dickens dedicated himself to the development of Victorian visual culture by actively employing illustrations in his fiction. For Dickens, illustrations were a way to attract readers and boost sales, as demonstrated by the commercial success of The Pickwick Papers (1837), as well as a crucial element of artistic expression. All but two of Dickens’s novels were illustrated, and his involvement in every stage of the illustration process was well documented. Dickens collaborated closely with his illustrators by providing detailed instructions: he provided specific colors for his illustrations, although the illustrations would be printed in black and white, and he intentionally positioned each illustration in a specific location in his serials in order to communicate details and emotions effectively with his readers. Scholars of Dickens’s works generally understand the illustrations to be integral to the text.
Given the expense of printing illustrations in color, most of the illustrations in Dickens’s work were printed in black and white. However, the hand-colored illustrations in A Christmas Carol (1843) demonstrate both Dickens’s interest in providing color when he could, and the significance of color in interpreting design elements as well as shaping interpretation of Dickens’s text. Because colorizing Dickens’s illustrations has the potential for enhancing the reader’s understanding of the text, and for opening up new interpretive possibilities, it has pedagogical implications that we wish to explore.
Our project is the first deep learning colorization venture in Victorian era media. We will colorize all of the illustrations in several of Dickens’s major novels, using the pix2pix model based on cGANs with Kim’s Victorian400 dataset as a research method. The Victorian400 dataset, published as an open data source, is a collection of colorful illustrations painted with nineteenth-century palettes. By using Victorian paintings and hand colored illustrations as a training set, our research methods make it possible for Dickens’s illustrations to be viewed with a Victorian color palette, approximating the color choices that a contemporary audience would have expected. We will present two groups of students with both the colorized and black and white illustrations and ask them to evaluate each in terms of design and in terms of their effect on their engagement with the text. Colorization with deep learning carries possibilities of misrepresentation or misinterpretation, which can be used to clarify students’ preconceptions about the nature or role of illustration and to spur creative responses to the text; it can also provide interest, anticipation, and imagination for readers. As a pedagogical tool, we hope that colorization using deep learning will improve students’ ability to think critically about the intersection of text and image while promoting deeper understanding and interest in Dickens’s works, and lay the groundwork for experimentation with deep learning colorization of other illustrated works by Victorian authors.","elibooklover@gmail.com, m-ives@tamu.edu",Poster
"Mallen, Enrique (1);
Meneses, Luis (2)","1: Sam Houston State University, United States of America;
2: University of Victoria, Canada",Using computer vision to identify graphic elements in Picasso’s poetry,"computer vision, graphic elements, poetry","Europe
English
North America
20th Century
image processing and analysis
manuscripts description, representation, and analysis
Art history
Humanities computing",English,"Europe
North America",20th Century,"image processing and analysis
manuscripts description, representation, and analysis","Art history
Humanities computing","<p>Pablo Picasso started writing in 1935. The onset of this new endeavor is said to have coincided with a devastating marital crisis—a financially risky divorce to be exact—decreasing substantially his pictorial output. Writing now became his al­ternative outlet. Not surprisingly, Picasso’s poetry is quite visual. While other poets in the past have added graphic elements to enhance their poems, their use in Picasso is of special importance as they are an essential component of his writing. Furthermore, the method he used in his poetic compositions is essentially combinatorial in nature, with words co-occurring often in unexpected ways, this relates to his collage technique during Cubism. Examples of these features are shown in figures 1 and 2.</p>
<p>Figure 1: P. Picasso, OPP35-138, 1935</p>
<p>Figure 2: P. Picasso, OPP36-138, 1936.</p>
<p>Among the graphic elements Picasso includes in his poems are dashes, brackets, arrows, lines, blots, etc.  The combinatorial nature of his writing highlights the interconnections between words and graphic elements (Mallen 2003). This combination of unique graphic and verbal components in Picasso’s poetry leads us to view the poems as a set distinct layers of text and images that form the layout of a document—making it quite hard to define different boundaries while providing an important contribution to the interpretation of the layers of text. Additionally, Picasso's poems are often revisited multiple times, and he kept a record of all the changes from one state to another at various moments of their creation.  Each layer with new additions and deletions is carefully dated by the author, indicating the importance that each of them had for him. The “final” published copy is artificial in the sense that the printed version has been subjected to a frozen linearized transcription (often carried out by Picasso’s secretary, Jaime Sabartés) and fails to portray the information in the additional layers that can be found in the original manuscripts which fortunately have been preserved.</p>
<p>We previously concluded that there is a close correlation between graphic elements and the verbal context in which they occur (Meneses and Mallen 2017) and offered a solution to encode graphic features and stratified text in machine-readable form (Mallen and Meneses 2019). However, identifying and encoding the graphical elements in the corpus of poems can be a laborious and intensive process. In this paper we will present a new approach that uses OpenCV, a library of programming functions aimed at real-time computer vision (OpenCV 2019), to identify the graphic elements in Picasso’s poetry. More specifically, we will present our findings on training the models, and discerning between false positives and negatives. This analysis will get us closer to understanding how the presence of graphic elements in a specific line of a poem affects the interpretation of the word string in that line and of the poem as a whole. Our final goal is to provide a systematic encoding of both the text and the graphic elements so that their mutually dependent interpretation may be properly evaluated.</p>
<p><strong>References</strong></p>
<p>Mallen, Enrique. 2003. <em>The Visual Grammar of Pablo Picasso</em>. New York: Peter Lang.</p>
<p>Mallen, Enrique, and Luis Meneses. 2019. “Visual Text: Encoding Challenges in Picasso’s Poetry.” <em>Journal of Fine Arts</em> 2 (2): 31–37.</p>
<p>Meneses, Luis, and Enrique Mallen. 2017. “Visual Text: Encoding Challenges in Picasso’s Poetry.” presented at the TEI 2017, Victoria, BC, Canada, November 11.</p>
<p>OpenCV. 2019. “OpenCV.” Accessed October 11. https://opencv.org/.</p>
","Pablo Picasso started writing in 1935. The onset of this new endeavor is said to have coincided with a devastating marital crisis—a financially risky divorce to be exact—decreasing substantially his pictorial output. Writing now became his al­ternative outlet. Not surprisingly, Picasso’s poetry is quite visual. While other poets in the past have added graphic elements to enhance their poems, their use in Picasso is of special importance as they are an essential component of his writing. Furthermore, the method he used in his poetic compositions is essentially combinatorial in nature, with words co-occurring often in unexpected ways, this relates to his collage technique during Cubism. Examples of these features are shown in figures 1 and 2.
Figure 1: P. Picasso, OPP35-138, 1935
Figure 2: P. Picasso, OPP36-138, 1936.
Among the graphic elements Picasso includes in his poems are dashes, brackets, arrows, lines, blots, etc.  The combinatorial nature of his writing highlights the interconnections between words and graphic elements (Mallen 2003). This combination of unique graphic and verbal components in Picasso’s poetry leads us to view the poems as a set distinct layers of text and images that form the layout of a document—making it quite hard to define different boundaries while providing an important contribution to the interpretation of the layers of text. Additionally, Picasso's poems are often revisited multiple times, and he kept a record of all the changes from one state to another at various moments of their creation.  Each layer with new additions and deletions is carefully dated by the author, indicating the importance that each of them had for him. The “final” published copy is artificial in the sense that the printed version has been subjected to a frozen linearized transcription (often carried out by Picasso’s secretary, Jaime Sabartés) and fails to portray the information in the additional layers that can be found in the original manuscripts which fortunately have been preserved.
We previously concluded that there is a close correlation between graphic elements and the verbal context in which they occur (Meneses and Mallen 2017) and offered a solution to encode graphic features and stratified text in machine-readable form (Mallen and Meneses 2019). However, identifying and encoding the graphical elements in the corpus of poems can be a laborious and intensive process. In this paper we will present a new approach that uses OpenCV, a library of programming functions aimed at real-time computer vision (OpenCV 2019), to identify the graphic elements in Picasso’s poetry. More specifically, we will present our findings on training the models, and discerning between false positives and negatives. This analysis will get us closer to understanding how the presence of graphic elements in a specific line of a poem affects the interpretation of the word string in that line and of the poem as a whole. Our final goal is to provide a systematic encoding of both the text and the graphic elements so that their mutually dependent interpretation may be properly evaluated.
References
Mallen, Enrique. 2003. The Visual Grammar of Pablo Picasso. New York: Peter Lang.
Mallen, Enrique, and Luis Meneses. 2019. “Visual Text: Encoding Challenges in Picasso’s Poetry.” Journal of Fine Arts 2 (2): 31–37.
Meneses, Luis, and Enrique Mallen. 2017. “Visual Text: Encoding Challenges in Picasso’s Poetry.” presented at the TEI 2017, Victoria, BC, Canada, November 11.
OpenCV. 2019. “OpenCV.” Accessed October 11. https://opencv.org/.","enriquemallenphd@gmail.com, ldmm@uvic.ca",Short Presentation
"Toscano, Maurizio (2);
Rabadán, Aroa (3);
Ros, Salvador (1);
González-Blanco, Elena (1)","1: Universidad Nacional de Educación a Distancia (UNED), Spain;
2: Universidad de Granada;
3: Universidad Complutense de Madrid",Evolución y escenario actual de las Humanidades Digitales en España,"Spain, Digital Humanities, digital infrastructures","Europe
Spanish
Contemporary
database creation, management, and analysis
History of science
Humanities computing",Spanish,Europe,Contemporary,"database creation, management, and analysis","History of science
Humanities computing","<p><strong>Introducción</strong></p>
<p>Las Humanidades Digitales (HD) se han convertido en un campo de interés en España, especialmente en la última década, a pesar de haber llegado más tarde que en la mayoría de los demás países europeos. De hecho, representan una tendencia destacada en la investigación, ya sea como campo de estudio que como tema de financiación preferente. Al mismo tiempo, por su novedad, están siendo objeto de escrutinio por parte de la comunidad investigadora y de las instituciones gubernamentales que financian la investigación. El objetivo del estudio ha sido identificar a los investigadores que trabajan en el campo de las HD en España y explorar su financiación, sus afiliaciones institucionales, las temáticas de investigación y los recursos digitales desarrollados.</p>
<p>En el pasado se han promovido iniciativas similares, que han producido mapeos centrados en el ámbito internacional o nacional, algunos de los cuales siguen disponibles en línea, con diferencias que van desde la cobertura geográfica hasta el tipo de datos mapeados (Ortega; Eunice-Gutiérrez, 2014; Romero-Frías; Del-Barrio-García, 2014). Otros estudios sobre este tema han optado por un enfoque diferente, utilizando la bibliografía u otras fuentes para identificar las etapas más relevantes en la evolución y consolidación de esta disciplina en España (Rojas-Castro, 2013; González-Blanco, 2013; Spence; González-Blanco, 2014; Baraibar-Echeverria, 2014). El presente trabajo no pretende ser una revisión histórica exhaustiva, sino ofrecer una visión complementaria y actualizada del panorama de las HD en España, tomando en consideración datos recientes y fuentes de información no explotadas anteriormente.</p>
<p><strong>Materiales y métodos</strong></p>
<p>A continuación se describen los resultados de la investigación sobre el estado actual de las infraestructuras de investigación digital en España, entendidas como la combinación e integración entre los recursos de información digital, las herramientas analíticas y de visualización y la comunidad activa de investigadores, colaborando a través de proyectos de investigación, financiados por el sector público o privado. Por esta razón, hemos subdividido el objeto de la investigación en cinco entidades principales, concretamente: investigadores, proyectos, recursos, bibliografía y cursos de posgrado.</p>
<p>Nuestra metodología de recopilación de datos ha tenido una doble vertiente. Por un lado, hemos seleccionado manualmente información disponible en línea a partir de congresos, seminarios, convocatorias temáticas, mapas participativos, etc. Por el otro, hemos extraído información a partir de bases de datos existentes: publicaciones científicas en el campo de las HD presentes en Dialnet y en ÍnDICEs-CSIC y proyectos de investigación financiados a través de la Agencia Estatal de Investigación, seleccionados a través de una serie de palabras claves (Figura 1).</p>
<p>El volumen total de registros recopilados ha sido 1.359, distribuidos de la siguiente manera: 577 investigadores; 368 proyectos; 88 recursos; 9 cursos de posgrado y 8 revistas científicas. El conjunto de datos analizados se encuentra disponible en Acceso Abierto (https://doi.org/10.5281/zenodo.3893546), junto con los Jupiter Notebooks y el código Python para reproducir los análisis.</p>
<p>Figura 1. Fuentes de datos utilizadas en la investigación.</p>
<p><strong>Resultados</strong></p>
<ul><li>Entre los investigadores identificados, 305 son varones (52,9%) y 272 mujeres (47,1%): una proporción que, comparada con la proporción de género entre los investigadores de España en todas las disciplinas (61,2% de varones y 38,8% de mujeres) o limitada a las Humanidades (59,8% de varones y 40,2% de mujeres), refleja una presencia femenina significativamente mayor de la esperada.</li>
<li>La clasificación de los investigadores por disciplinas, 19 en total, muestra una amplia variedad, con una clara prevalencia de filólogos (36%), seguidos por historiadores (16,5%) e informáticos (10,8%).</li>
<li>El análisis de las conexiones entre disciplinas y temas de investigación (Figura 2) revela cinco grandes grupos disciplinarios: Historia (32% de los nodos), Filología (24%), Comunicación (20%), Ciencias de la Computación (17,3%) y Documentación (6,7%). Las Ciencias de la Computación, a pesar de ser la cuarta comunidad en términos de nodos, resulta ser la disciplina relacionada con el mayor número de temas de investigación (13,3%).</li>
</ul><p>Figura 2. Análisis de redes de disciplinas y temas de investigación. El tamaño de los nodos corresponde al número total de conexiones, el color a la comunidad y el grosor de las aristas al grado de conectividad entre parejas de nodos.</p>
<ul><li>Según la afiliación institucional, la mitad de los investigadores (49,2%) pertenece a un total de nueve instituciones, mientras que la otra mitad se encuentra dispersa entre 84 diferentes centros (http://sl.ugr.es/DHmap).</li>
<li>La afiliación a departamentos (Figura 3) muestra un patrón mucho más variado en cada institución: por ejemplo, el peso relativo de la Filología es mayor en la Complutense de Madrid o en Santiago de Compostela con respecto a Granada, donde por otra parte se aprecia mucha variedad, con investigadores procedentes de 13 departamentos.</li>
</ul><p>Figura 3. Peso relativo de cada departamento en los nueve centros de investigación principales. Para favorecer la comparación, se ha utilizado un histograma apilado.</p>
<ul><li>El análisis de la inversión en 337 proyectos de investigación en los últimos 25 años (Figura 4) permite avanzar una periodización en tres fases en la consolidación de las HD en España: 1993-2003; 2004-2014; 2015-2019. La financiación media por proyecto ha sido de 64.313€, pero la mediana se encuentra en 42.350€; el 5% de los proyectos ha recibido hasta 5.000€ (micro-proyectos) y el 90% menos de 100.000€.</li>
</ul><p>Figura 4. Total de proyectos de investigación en HD y financiación recibida durante el período 1993-2019.</p>
<ul><li>Entre las fuentes de financiación, el rol del ministerio es predominante, con 77% de las propuestas financiadas y 72% de los recursos asignados, a pesar de la gran variedad de organismos implicados, hasta 26, y de un 10% de recursos procedentes del sector privado.</li>
<li>Disciplinas como la Filología, la Lingüística y la Biblioteconomía evidencian una tradición más larga que otras, como la Historia, la Arqueología y la Historia del Arte, en el desarrollo de recursos digitales para la investigación (Figura 5). Los artefactos desarrollados con mayor frecuencia (72,4%) son diferentes tipos de bases de datos (bibliotecas digitales, catálogos, repositorios, etc.), mientras que a partir de 2014 se observa un cambio hacia instrumentos más analíticos o participativos. La sostenibilidad de las plataformas en le tiempo se ha garantizado principalmente mediante la financiación en serie de proyectos del Programa Estatal de I+D+i.</li>
</ul><p>Figura 5. Recursos digitales clasificados según disciplina y tipología.</p>
<p><strong>Conclusiones</strong></p>
<p>A modo de conclusión, podemos destacar los siguientes aspectos: (1) la mayoría de las evidencias detectadas por otros estudios se han confirmado numéricamente; (2) los análisis cuantitativos de la financiación, una dimensión prácticamente inexplorada en las Humanidades, han demostrado ser extremadamente valiosos en la valoración de la evolución histórica de una disciplina científica; (3) se han establecido nuevas métricas y valores que constituyen una base de referencia para monitorear la evolución de las HD en España y favorecer las comparaciones, tanto a lo largo del tiempo como con otros contextos a nivel europeo e internacional.</p>
<p><strong>Referencias bibliográficas</strong></p>
<ol><li>Baraibar-Echeverria, Álvaro (2014). In: Baraibar-Echeverria, Álvaro (ed.). <em>Humanidades digitales: Una aproximación transdisciplinar</em>. A Coruña: Universidade da Coruña, SIELAE.</li>
<li>González-Blanco, Elena (2013). “Actualidad de las Humanidades Digitales y un ejemplo de ensamblaje poético en la red: ReMetCa”. In: <em>Cuadernos hispanoamericanos</em>, n. 761, pp. 53-67. https://doi.org/10.4000/jtei.1274</li>
<li>Ortega, Elika; Eunice-Gutiérrez, Silvia (2014). “MapaHD. Una exploración de las humanidades digitales en español y portugués”. In: Romero-Frías, Esteban (ed.); Sánchez-González. María (ed.). <em>Ciencias sociales y humanidades digitales: técnicas, herramientas y experiencias de e-research e Investigación En Colaboración</em>. La Laguna, Tenerife: Sociedad Latina de Comunicación Social, pp. 101-128.</li>
<li>Rojas-Castro, Antonio (2013). “El mapa y el territorio. Una aproximación histórico-bibliográfica a la emergencia de las humanidades digitales en España”. In: <em>Caracteres</em>, 2, pp. 10-52.</li>
<li>Romero-Frías, Esteban; Del-Barrio-García, Salvador (2014). “Una visión de las humanidades digitales a través de sus centros.” In: <em>Profesional de la información</em>, v. 23, n. 5, pp. 485–92. https://doi.org/10.3145/epi.2014.sep.05</li>
<li>Spence, Paul; González-Blanco, Elena (2014). “A historical perspective on the digital humanities in Spain”. In: <em>H-Soz-Kult</em>, November 7th. www.hsozkult.de/debate/id/diskussionen-2449</li>
</ol>","Introducción
Las Humanidades Digitales (HD) se han convertido en un campo de interés en España, especialmente en la última década, a pesar de haber llegado más tarde que en la mayoría de los demás países europeos. De hecho, representan una tendencia destacada en la investigación, ya sea como campo de estudio que como tema de financiación preferente. Al mismo tiempo, por su novedad, están siendo objeto de escrutinio por parte de la comunidad investigadora y de las instituciones gubernamentales que financian la investigación. El objetivo del estudio ha sido identificar a los investigadores que trabajan en el campo de las HD en España y explorar su financiación, sus afiliaciones institucionales, las temáticas de investigación y los recursos digitales desarrollados.
En el pasado se han promovido iniciativas similares, que han producido mapeos centrados en el ámbito internacional o nacional, algunos de los cuales siguen disponibles en línea, con diferencias que van desde la cobertura geográfica hasta el tipo de datos mapeados (Ortega; Eunice-Gutiérrez, 2014; Romero-Frías; Del-Barrio-García, 2014). Otros estudios sobre este tema han optado por un enfoque diferente, utilizando la bibliografía u otras fuentes para identificar las etapas más relevantes en la evolución y consolidación de esta disciplina en España (Rojas-Castro, 2013; González-Blanco, 2013; Spence; González-Blanco, 2014; Baraibar-Echeverria, 2014). El presente trabajo no pretende ser una revisión histórica exhaustiva, sino ofrecer una visión complementaria y actualizada del panorama de las HD en España, tomando en consideración datos recientes y fuentes de información no explotadas anteriormente.
Materiales y métodos
A continuación se describen los resultados de la investigación sobre el estado actual de las infraestructuras de investigación digital en España, entendidas como la combinación e integración entre los recursos de información digital, las herramientas analíticas y de visualización y la comunidad activa de investigadores, colaborando a través de proyectos de investigación, financiados por el sector público o privado. Por esta razón, hemos subdividido el objeto de la investigación en cinco entidades principales, concretamente: investigadores, proyectos, recursos, bibliografía y cursos de posgrado.
Nuestra metodología de recopilación de datos ha tenido una doble vertiente. Por un lado, hemos seleccionado manualmente información disponible en línea a partir de congresos, seminarios, convocatorias temáticas, mapas participativos, etc. Por el otro, hemos extraído información a partir de bases de datos existentes: publicaciones científicas en el campo de las HD presentes en Dialnet y en ÍnDICEs-CSIC y proyectos de investigación financiados a través de la Agencia Estatal de Investigación, seleccionados a través de una serie de palabras claves (Figura 1).
El volumen total de registros recopilados ha sido 1.359, distribuidos de la siguiente manera: 577 investigadores; 368 proyectos; 88 recursos; 9 cursos de posgrado y 8 revistas científicas. El conjunto de datos analizados se encuentra disponible en Acceso Abierto (https://doi.org/10.5281/zenodo.3893546), junto con los Jupiter Notebooks y el código Python para reproducir los análisis.
Figura 1. Fuentes de datos utilizadas en la investigación.
Resultados
Entre los investigadores identificados, 305 son varones (52,9%) y 272 mujeres (47,1%): una proporción que, comparada con la proporción de género entre los investigadores de España en todas las disciplinas (61,2% de varones y 38,8% de mujeres) o limitada a las Humanidades (59,8% de varones y 40,2% de mujeres), refleja una presencia femenina significativamente mayor de la esperada.
La clasificación de los investigadores por disciplinas, 19 en total, muestra una amplia variedad, con una clara prevalencia de filólogos (36%), seguidos por historiadores (16,5%) e informáticos (10,8%).
El análisis de las conexiones entre disciplinas y temas de investigación (Figura 2) revela cinco grandes grupos disciplinarios: Historia (32% de los nodos), Filología (24%), Comunicación (20%), Ciencias de la Computación (17,3%) y Documentación (6,7%). Las Ciencias de la Computación, a pesar de ser la cuarta comunidad en términos de nodos, resulta ser la disciplina relacionada con el mayor número de temas de investigación (13,3%).
Figura 2. Análisis de redes de disciplinas y temas de investigación. El tamaño de los nodos corresponde al número total de conexiones, el color a la comunidad y el grosor de las aristas al grado de conectividad entre parejas de nodos.
Según la afiliación institucional, la mitad de los investigadores (49,2%) pertenece a un total de nueve instituciones, mientras que la otra mitad se encuentra dispersa entre 84 diferentes centros (http://sl.ugr.es/DHmap).
La afiliación a departamentos (Figura 3) muestra un patrón mucho más variado en cada institución: por ejemplo, el peso relativo de la Filología es mayor en la Complutense de Madrid o en Santiago de Compostela con respecto a Granada, donde por otra parte se aprecia mucha variedad, con investigadores procedentes de 13 departamentos.
Figura 3. Peso relativo de cada departamento en los nueve centros de investigación principales. Para favorecer la comparación, se ha utilizado un histograma apilado.
El análisis de la inversión en 337 proyectos de investigación en los últimos 25 años (Figura 4) permite avanzar una periodización en tres fases en la consolidación de las HD en España: 1993-2003; 2004-2014; 2015-2019. La financiación media por proyecto ha sido de 64.313€, pero la mediana se encuentra en 42.350€; el 5% de los proyectos ha recibido hasta 5.000€ (micro-proyectos) y el 90% menos de 100.000€.
Figura 4. Total de proyectos de investigación en HD y financiación recibida durante el período 1993-2019.
Entre las fuentes de financiación, el rol del ministerio es predominante, con 77% de las propuestas financiadas y 72% de los recursos asignados, a pesar de la gran variedad de organismos implicados, hasta 26, y de un 10% de recursos procedentes del sector privado.
Disciplinas como la Filología, la Lingüística y la Biblioteconomía evidencian una tradición más larga que otras, como la Historia, la Arqueología y la Historia del Arte, en el desarrollo de recursos digitales para la investigación (Figura 5). Los artefactos desarrollados con mayor frecuencia (72,4%) son diferentes tipos de bases de datos (bibliotecas digitales, catálogos, repositorios, etc.), mientras que a partir de 2014 se observa un cambio hacia instrumentos más analíticos o participativos. La sostenibilidad de las plataformas en le tiempo se ha garantizado principalmente mediante la financiación en serie de proyectos del Programa Estatal de I+D+i.
Figura 5. Recursos digitales clasificados según disciplina y tipología.
Conclusiones
A modo de conclusión, podemos destacar los siguientes aspectos: (1) la mayoría de las evidencias detectadas por otros estudios se han confirmado numéricamente; (2) los análisis cuantitativos de la financiación, una dimensión prácticamente inexplorada en las Humanidades, han demostrado ser extremadamente valiosos en la valoración de la evolución histórica de una disciplina científica; (3) se han establecido nuevas métricas y valores que constituyen una base de referencia para monitorear la evolución de las HD en España y favorecer las comparaciones, tanto a lo largo del tiempo como con otros contextos a nivel europeo e internacional.
Referencias bibliográficas
Baraibar-Echeverria, Álvaro (2014). In: Baraibar-Echeverria, Álvaro (ed.). Humanidades digitales: Una aproximación transdisciplinar. A Coruña: Universidade da Coruña, SIELAE.
González-Blanco, Elena (2013). “Actualidad de las Humanidades Digitales y un ejemplo de ensamblaje poético en la red: ReMetCa”. In: Cuadernos hispanoamericanos, n. 761, pp. 53-67. https://doi.org/10.4000/jtei.1274
Ortega, Elika; Eunice-Gutiérrez, Silvia (2014). “MapaHD. Una exploración de las humanidades digitales en español y portugués”. In: Romero-Frías, Esteban (ed.); Sánchez-González. María (ed.). Ciencias sociales y humanidades digitales: técnicas, herramientas y experiencias de e-research e Investigación En Colaboración. La Laguna, Tenerife: Sociedad Latina de Comunicación Social, pp. 101-128.
Rojas-Castro, Antonio (2013). “El mapa y el territorio. Una aproximación histórico-bibliográfica a la emergencia de las humanidades digitales en España”. In: Caracteres, 2, pp. 10-52.
Romero-Frías, Esteban; Del-Barrio-García, Salvador (2014). “Una visión de las humanidades digitales a través de sus centros.” In: Profesional de la información, v. 23, n. 5, pp. 485–92. https://doi.org/10.3145/epi.2014.sep.05
Spence, Paul; González-Blanco, Elena (2014). “A historical perspective on the digital humanities in Spain”. In: H-Soz-Kult, November 7th. www.hsozkult.de/debate/id/diskussionen-2449","maurizio.toscano@gmail.com, aroaraba@ucm.es, sros@scc.uned.es, egonzalezblanco@flog.uned.es",Short Presentation
"Tucker, Aaron (1);
Ramnarine, Kieran (2)","1: York University and Ryerson University, Canada;
2: Ryerson University, Canada",Datasets of Criminal Faces Within and Under Facial Recognition Software (FRS) From a Digital Humanities Perspective ,"machine learning, facial recognition, computer vision, digital literacy, data ethics","English
North America
20th Century
Contemporary
artificial intelligence and machine learning
digital activism and advocacy
Communication studies
Media studies",English,North America,"20th Century
Contemporary","artificial intelligence and machine learning
digital activism and advocacy","Communication studies
Media studies","<p>This paper responds to the intersectional problematics of facial databases within contemporary facial recognition software and computer vision machine learning by highlight our ongoing project This Criminal Does Not Exist. Beginning with the MEDS database, our project applies a Convolutional Generative Adversarial Network to produce synthetic faces. Aesthetically, the portraits generated resemble eugencist Francis Galton’s “composite portraits” of different races that he deployed in the 19th century. The project is a data visualization project: using machine learning techniques, we have been able to surface what is the “most common” type of face within the dataset; that the portraits generated are primarily of African American males speaks to the types of faces over-represented in these virtual spaces. Further, from this data visualization, “This Criminal Does not Exist” is indicative of contemporary State applications of FRS, bringing to light the clear biases inherent in the dataset, biases further perpetuated through algorithms trained on these types of dataset.</p>
<p>This response is made from a digital humanities perspective that combines principles of ethical data annotation and classification with critical making. In particular, this paper addresses how digital humanities can contribute potential solutions to the ethics of studying and surfacing problematic databases.More specifically, drawing from the the impacts of 19th century pseudo-science like eugenics, phrenology, physiognomy, and signaletics, our project “This Criminal Does Not Exist” signals another potential set of tactics and research creation paths that simultaneously educates the public about the nature of problematic facial datasets, alongside producing arguments about the ethical implications about such databases and their in-built classification practices. Further, this paper explores how digital humanities scholars can provide a public critical engagement with such databases that is grounded in humanized narrative, that does not further replicate and/or ingrain the intersectional and carceral biases of the databases.</p>
<p>Our research begins by recounting how the contemporary study and fears surrounding FRS has been largely focused on large scale corporate- and state-led surveillance apparatuses and their impacts on users’ data privacy. This work, exemplified by scholars like Ann Cavoukian and her framework of Privacy by Design, is undeniably useful; similarly, research by surveillance studies theorists like David Lyon and Gary Marx has contributed greatly to advocating for responsible building and application of technologies like FRS. The initial scholarship into the problematic construction of FRS has been driven, in large part, by a wealth of research and reporting about the known inherent biases of the technology, which, as the Georgetown Law Center on Privacy & Technology’s report “The Perpetual Line-up” insists, “face recognition may be least accurate for those it is most likely to affect: African Americans.” The technology’s consistent optimization, in construction and application, for white male faces is especially troubling as the technology moves from being surveilling, national security, and law enforcement tactics, into the ubiquitous, and far more normalized, activities of intervening in job interviews, the monitoring of low incoming housing, and the granting of bank loans. These last three FRS tasks are examples of what Safiya Noble, in her text Algorithms of Oppression, would give as examples of “technological redlining,” which she explains is the use of algorithms and big data to “reinforce oppressive social relationships and enact new modes of racial profiling.”  </p>
<p>Given this, how might digital humanities scholars make the contents of these databases public and available for wider scrutiny and potential regulation while not replicating the dangerous practices that initially led to the construction and implementation of such data? One effective example is artist Trevor Paglen’s collaboration with scholar Kate Crawford titled ImageNet Roulette. The project trains an app on the massive ImageNet database’s of images labeled in the “person” category. The result is a surfacing of how “ImageNet contains a number of problematic, offensive, and bizarre categories. Hence, the results ImageNet Roulette returns often draw upon those categories. That is by design: we want to shed light on what happens when technical systems are trained using problematic training data.” Their accompanying essay, “Excavating AI: The Politics of Images in Machine Learning Training Sets,” expands further in labelling their own work as an “archeology of datasets”: “we have been digging through the material layers, cataloguing the principles and values by which something was constructed, and analyzing what normative patterns of life were assumed, supported, and reproduced. By excavating the construction of these training sets and their underlying structures, many unquestioned assumptions are revealed.” Digital humanities scholars are extremely well suited to take up similar archeological projects, in FRS or other AI- and machine learning-aided environments, as the discipline’s focus on ethics, digital tools and humanities-based close-reading techniques grant scholars the abilities to take up the urgent problems of FRS’s everyday applications.</p>
<p>Works Cited</p>
<p>Buolamwini, Joy. “Incoding - In the Beginning” Medium. https://medium.com/mit-media-lab/incoding-in-the-beginning-4e2a5c51a45d. May 16 2016. para. 6. Accessed 11 June 2020.</p>
<p>Buolamwini, Joy and Timnit Gebru. “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification” Conference on Fairness, Accountability, and Transparency. Proceedings of Machine Learning Research 81:1–15, 2018..</p>
<p>Cavoukian, Ann. “The Seven Foundational Principles” https://www.ryerson.ca/pbdce/certification/seven-foundational-principles-of-privacy-by-design/. 2017. Accessed 11 June 2020.</p>
<p>Crawford, Kate and Trevor Paglen. “Excavating AI: The Politics of Images in Machine Learning Training Sets.” https://www.excavating.ai/. September 19, 2019. para. 37. Accessed 11 June 2020.</p>
<p>Galton. Francis. “Composite Portraits.” Journal of the Anthropological Institute. 1870. 132-144; Hereditary Genius: An Inquiry into its Laws and Consequences. Macmillan and Co, 1869.</p>
<p>Garvie, Clare, Alvaro M. Bedoya and Jonathan Frankle. “The Perpetual Line-Up” Georgetown Law Centre on Privacy and Technology. https//:www.theperpetualineup.org. 2016, para. 23. Accessed 11 June 2020.</p>
<p>Lyon, David. Surveillance as Social Sorting: Privacy, Risk, and Digital Discrimination. Routledge, 2003; Identifying Citizens: ID Cards as Surveillance. Polity, 2009.</p>
<p>Marx, Gary. Windows into the Soul: Surveillance and Society in an Age of High Technology. University of Chicago Press, 2016.</p>
<p>Noble, Safiya U. Algorithms of Oppression: How Search Engines Reinforce Racism. New York University Press, 2018.</p>
","This paper responds to the intersectional problematics of facial databases within contemporary facial recognition software and computer vision machine learning by highlight our ongoing project This Criminal Does Not Exist. Beginning with the MEDS database, our project applies a Convolutional Generative Adversarial Network to produce synthetic faces. Aesthetically, the portraits generated resemble eugencist Francis Galton’s “composite portraits” of different races that he deployed in the 19th century. The project is a data visualization project: using machine learning techniques, we have been able to surface what is the “most common” type of face within the dataset; that the portraits generated are primarily of African American males speaks to the types of faces over-represented in these virtual spaces. Further, from this data visualization, “This Criminal Does not Exist” is indicative of contemporary State applications of FRS, bringing to light the clear biases inherent in the dataset, biases further perpetuated through algorithms trained on these types of dataset.
This response is made from a digital humanities perspective that combines principles of ethical data annotation and classification with critical making. In particular, this paper addresses how digital humanities can contribute potential solutions to the ethics of studying and surfacing problematic databases.More specifically, drawing from the the impacts of 19th century pseudo-science like eugenics, phrenology, physiognomy, and signaletics, our project “This Criminal Does Not Exist” signals another potential set of tactics and research creation paths that simultaneously educates the public about the nature of problematic facial datasets, alongside producing arguments about the ethical implications about such databases and their in-built classification practices. Further, this paper explores how digital humanities scholars can provide a public critical engagement with such databases that is grounded in humanized narrative, that does not further replicate and/or ingrain the intersectional and carceral biases of the databases.
Our research begins by recounting how the contemporary study and fears surrounding FRS has been largely focused on large scale corporate- and state-led surveillance apparatuses and their impacts on users’ data privacy. This work, exemplified by scholars like Ann Cavoukian and her framework of Privacy by Design, is undeniably useful; similarly, research by surveillance studies theorists like David Lyon and Gary Marx has contributed greatly to advocating for responsible building and application of technologies like FRS. The initial scholarship into the problematic construction of FRS has been driven, in large part, by a wealth of research and reporting about the known inherent biases of the technology, which, as the Georgetown Law Center on Privacy & Technology’s report “The Perpetual Line-up” insists, “face recognition may be least accurate for those it is most likely to affect: African Americans.” The technology’s consistent optimization, in construction and application, for white male faces is especially troubling as the technology moves from being surveilling, national security, and law enforcement tactics, into the ubiquitous, and far more normalized, activities of intervening in job interviews, the monitoring of low incoming housing, and the granting of bank loans. These last three FRS tasks are examples of what Safiya Noble, in her text Algorithms of Oppression, would give as examples of “technological redlining,” which she explains is the use of algorithms and big data to “reinforce oppressive social relationships and enact new modes of racial profiling.”  
Given this, how might digital humanities scholars make the contents of these databases public and available for wider scrutiny and potential regulation while not replicating the dangerous practices that initially led to the construction and implementation of such data? One effective example is artist Trevor Paglen’s collaboration with scholar Kate Crawford titled ImageNet Roulette. The project trains an app on the massive ImageNet database’s of images labeled in the “person” category. The result is a surfacing of how “ImageNet contains a number of problematic, offensive, and bizarre categories. Hence, the results ImageNet Roulette returns often draw upon those categories. That is by design: we want to shed light on what happens when technical systems are trained using problematic training data.” Their accompanying essay, “Excavating AI: The Politics of Images in Machine Learning Training Sets,” expands further in labelling their own work as an “archeology of datasets”: “we have been digging through the material layers, cataloguing the principles and values by which something was constructed, and analyzing what normative patterns of life were assumed, supported, and reproduced. By excavating the construction of these training sets and their underlying structures, many unquestioned assumptions are revealed.” Digital humanities scholars are extremely well suited to take up similar archeological projects, in FRS or other AI- and machine learning-aided environments, as the discipline’s focus on ethics, digital tools and humanities-based close-reading techniques grant scholars the abilities to take up the urgent problems of FRS’s everyday applications.
Works Cited
Buolamwini, Joy. “Incoding - In the Beginning” Medium. https://medium.com/mit-media-lab/incoding-in-the-beginning-4e2a5c51a45d. May 16 2016. para. 6. Accessed 11 June 2020.
Buolamwini, Joy and Timnit Gebru. “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification” Conference on Fairness, Accountability, and Transparency. Proceedings of Machine Learning Research 81:1–15, 2018..
Cavoukian, Ann. “The Seven Foundational Principles” https://www.ryerson.ca/pbdce/certification/seven-foundational-principles-of-privacy-by-design/. 2017. Accessed 11 June 2020.
Crawford, Kate and Trevor Paglen. “Excavating AI: The Politics of Images in Machine Learning Training Sets.” https://www.excavating.ai/. September 19, 2019. para. 37. Accessed 11 June 2020.
Galton. Francis. “Composite Portraits.” Journal of the Anthropological Institute. 1870. 132-144; Hereditary Genius: An Inquiry into its Laws and Consequences. Macmillan and Co, 1869.
Garvie, Clare, Alvaro M. Bedoya and Jonathan Frankle. “The Perpetual Line-Up” Georgetown Law Centre on Privacy and Technology. https//:www.theperpetualineup.org. 2016, para. 23. Accessed 11 June 2020.
Lyon, David. Surveillance as Social Sorting: Privacy, Risk, and Digital Discrimination. Routledge, 2003; Identifying Citizens: ID Cards as Surveillance. Polity, 2009.
Marx, Gary. Windows into the Soul: Surveillance and Society in an Age of High Technology. University of Chicago Press, 2016.
Noble, Safiya U. Algorithms of Oppression: How Search Engines Reinforce Racism. New York University Press, 2018.","artucker@yorku.ca, kieran.ramnarine@ryerson.ca",Short Presentation
"Polyck-O'Neill, Julia G.","Brock University, Canada",Potential Archives: How Digital Humanities and Feminist Ethical Praxis Will Transform the Interdisciplinary Artist Archive,"artists' archives, digital archives, multimedia, interdisciplinary art, feminist ethics","Comparative (2 or more geographical areas)
English
North America
20th Century
Contemporary
digital archiving
public humanities collaborations and methods
Art history
Feminist studies",English,"Comparative (2 or more geographical areas)
North America","20th Century
Contemporary","digital archiving
public humanities collaborations and methods","Art history
Feminist studies","<p>Potential Archives: How Digital Humanities and Feminist Ethical Praxis Will Transform the Interdisciplinary Artist Archive</p>
<p>As digital media conservators Deena Engel and Glenn Wharton identify in the premise for the <em>Artist Archive Initiative</em> at New York University, conventional approaches to the artist archive neglect to study how the complexity of an artist’s interdisciplinary creative practice can confound conventional archival systems and practices. My project demonstrates how artists’ archives benefit from non-traditional archival methods that combine emerging digital archival strategies that accommodate and represent community networks and collaborations with the intervention of the artists themselves in the co-creation of accessible multimedia archives. Digital methods will enable artists to augment and customize their archival holdings with attributes such as narratives/narration and networked information.</p>
<p>         The need to reconsider material and organizational aspects of artist fonds also has immediate and practical consequences for institutions. Often collections are broken up, and/or the acquisition process can be delayed by technical and policy-driven challenges. National funding opportunities, such as the Canada Council for the Art’s Digital Strategy Fund (DSF), add further incentive to the demand for fundamental procedural change.</p>
<p>         Archival scholars have identified two interrelated contentions underlying current approaches to artists’ archives within the present academic and archival milieu: systemic issues fundamental to archival conventions and practices, and shortcomings of formal organizational strategies within such practices. Feminist archival studies scholars such as Michelle Caswell, Marika Cifor, and Stacy Wood have identified that traditional archival practice is often rooted in colonial and patriarchal cultural and structural conditions. Engel and Wharton, scholars of artists’ archives, have addressed how the limitations of conventional archival systems often fail to accommodate the kinds of information, accuracy, and logistical affordances scholars and art professionals require for their research. Specialists in feminist archival studies respond to such organizational shortcomings, observing how the practice of the co-creation of archives with the artist(s) represented within the collections can contribute meaningfully to the value of the collection for scholars and communities. Caswell and Cifor’s proposal for a “feminist ethical framework” for archival studies situates the archive socially and culturally, with consideration of relational and affective contexts (24), and Cifor and Wood argue that “critical feminist theory can contribute to existing archival discourse and practice, critiquing concepts that have remained unquestioned, such as community and organization” (3). The addition of autobiographical, narrative, and networked data and digital media forms enable increased access, and have the potential to transform the relationships between artist, archival institution, and user.</p>
<p>         This paper explores two main, preliminary ideas: why a transformation of the organization of artist archives is timely and important; and how digital methods and platforms have the potential to benefit artists, arts scholars, and arts archivists. <em>Potential Archives</em> is both a study and a framework, providing both a map of how these non-traditional methods have worked in the past, and a model for how to develop future artist’s archives. My study and resulting framework will reconceptualize the interdisciplinary artist archive according to emerging feminist and digital epistemologies and methods to help artists plan for and prepare their future institutional archives and address emerging needs and concerns, while also assisting arts institutions in addressing such innovations.</p>
<p>Caswell, Michelle. “Teaching to Dismantle White Supremacy in the Archives.” <em>The Library Quarterly </em>vol. 87, no. 3, July 2017, pp. 222-235.</p>
<p>Caswell, Michelle and Marika Cifor. “From Human Rights to Feminist Ethics: Radical           Empathy in the Archives.” <em>Archivaria</em> no. 81, Spring 2016, pp. 23-43.</p>
<p>Cifor, Marika and Stacy Wood. “Critical Feminism in the Archives,” in “Critical Archival Studies,” eds. Michelle Caswell, Ricardo Punzalan, and T-Kay Sangwand. Special Issue, <em>Journal of Critical Library and Information Studies </em>no. 2, 2017, pp. 1-22.</p>
<p>Engel, Deena and Glenn Wharton. “Managing Contemporary Art Documentation in          Museums and Special Collections.” <em>Art Documentation: Journal of the Art       Libraries Society of North America</em> vol. 36, no. 2, 2017, pp. 293-311.</p>
","Potential Archives: How Digital Humanities and Feminist Ethical Praxis Will Transform the Interdisciplinary Artist Archive
As digital media conservators Deena Engel and Glenn Wharton identify in the premise for the Artist Archive Initiative at New York University, conventional approaches to the artist archive neglect to study how the complexity of an artist’s interdisciplinary creative practice can confound conventional archival systems and practices. My project demonstrates how artists’ archives benefit from non-traditional archival methods that combine emerging digital archival strategies that accommodate and represent community networks and collaborations with the intervention of the artists themselves in the co-creation of accessible multimedia archives. Digital methods will enable artists to augment and customize their archival holdings with attributes such as narratives/narration and networked information.
         The need to reconsider material and organizational aspects of artist fonds also has immediate and practical consequences for institutions. Often collections are broken up, and/or the acquisition process can be delayed by technical and policy-driven challenges. National funding opportunities, such as the Canada Council for the Art’s Digital Strategy Fund (DSF), add further incentive to the demand for fundamental procedural change.
         Archival scholars have identified two interrelated contentions underlying current approaches to artists’ archives within the present academic and archival milieu: systemic issues fundamental to archival conventions and practices, and shortcomings of formal organizational strategies within such practices. Feminist archival studies scholars such as Michelle Caswell, Marika Cifor, and Stacy Wood have identified that traditional archival practice is often rooted in colonial and patriarchal cultural and structural conditions. Engel and Wharton, scholars of artists’ archives, have addressed how the limitations of conventional archival systems often fail to accommodate the kinds of information, accuracy, and logistical affordances scholars and art professionals require for their research. Specialists in feminist archival studies respond to such organizational shortcomings, observing how the practice of the co-creation of archives with the artist(s) represented within the collections can contribute meaningfully to the value of the collection for scholars and communities. Caswell and Cifor’s proposal for a “feminist ethical framework” for archival studies situates the archive socially and culturally, with consideration of relational and affective contexts (24), and Cifor and Wood argue that “critical feminist theory can contribute to existing archival discourse and practice, critiquing concepts that have remained unquestioned, such as community and organization” (3). The addition of autobiographical, narrative, and networked data and digital media forms enable increased access, and have the potential to transform the relationships between artist, archival institution, and user.
         This paper explores two main, preliminary ideas: why a transformation of the organization of artist archives is timely and important; and how digital methods and platforms have the potential to benefit artists, arts scholars, and arts archivists. Potential Archives is both a study and a framework, providing both a map of how these non-traditional methods have worked in the past, and a model for how to develop future artist’s archives. My study and resulting framework will reconceptualize the interdisciplinary artist archive according to emerging feminist and digital epistemologies and methods to help artists plan for and prepare their future institutional archives and address emerging needs and concerns, while also assisting arts institutions in addressing such innovations.
Caswell, Michelle. “Teaching to Dismantle White Supremacy in the Archives.” The Library Quarterly vol. 87, no. 3, July 2017, pp. 222-235.
Caswell, Michelle and Marika Cifor. “From Human Rights to Feminist Ethics: Radical           Empathy in the Archives.” Archivaria no. 81, Spring 2016, pp. 23-43.
Cifor, Marika and Stacy Wood. “Critical Feminism in the Archives,” in “Critical Archival Studies,” eds. Michelle Caswell, Ricardo Punzalan, and T-Kay Sangwand. Special Issue, Journal of Critical Library and Information Studies no. 2, 2017, pp. 1-22.
Engel, Deena and Glenn Wharton. “Managing Contemporary Art Documentation in          Museums and Special Collections.” Art Documentation: Journal of the Art       Libraries Society of North America vol. 36, no. 2, 2017, pp. 293-311.",jp03uw@brocku.ca,Short Presentation
"Santa Maria, Teresa;
Dabrowska, Monika","Universidad Internacional de La Rioja, Spain",Análisis del coro como personaje en la dramaturgia grecolatina y española incluidas en DraCor,"DraCor, Coro, Teatro, Grafo, Literatura comparada","Europe
Spanish
BCE-4th Century
19th Century
20th Century
network analysis and graphs theory and application
semantic analysis
Literary studies
Performance Studies: Dance, Theatre",Spanish,Europe,"BCE-4th Century
19th Century
20th Century","network analysis and graphs theory and application
semantic analysis","Literary studies
Performance Studies: Dance, Theatre","<p>Drama Corpora Project (DraCor) ha ido conformando un repositorio donde se encuentran, en estos momentos, once corpus teatrales diferentes, que pertenecen a periodos, géneros dramáticos y lugares geográficos diversos. Entre estos corpus encontramos veinticinco obras de la Edad de Plata española (finales del XIX-mediados del siglo XX) que conforman el “Spanish Drama Corpus” y que han sido importadas de la Biblioteca Electrónica Textual del Teatro Español de 1868-1936 (BETTE).</p>
<p>Además de los textos dramáticos en XML-TEI que se consultan en DraCor, podemos crear las redes sociales con grafos que conforman los personajes de cada una de dichas piezas dramáticas. Uno de los personajes comunes en muchas de estas piezas y que resulta fundamental en el desarrollo de la trama y para dar corporeidad a los pensamientos de otros personajes lo constituye el Coro.</p>
","Drama Corpora Project (DraCor) ha ido conformando un repositorio donde se encuentran, en estos momentos, once corpus teatrales diferentes, que pertenecen a periodos, géneros dramáticos y lugares geográficos diversos. Entre estos corpus encontramos veinticinco obras de la Edad de Plata española (finales del XIX-mediados del siglo XX) que conforman el “Spanish Drama Corpus” y que han sido importadas de la Biblioteca Electrónica Textual del Teatro Español de 1868-1936 (BETTE).
Además de los textos dramáticos en XML-TEI que se consultan en DraCor, podemos crear las redes sociales con grafos que conforman los personajes de cada una de dichas piezas dramáticas. Uno de los personajes comunes en muchas de estas piezas y que resulta fundamental en el desarrollo de la trama y para dar corporeidad a los pensamientos de otros personajes lo constituye el Coro.","teresa.santamaria@unir.net, monika.dabrowska@unir.net",Lightning
"Jofre, Ana (1);
Cole, Josh (2);
Reale, Michael (1);
Berardi, Vincent (3)","1: SUNY Polytechnic, United States of America;
2: Queen's University, Canada;
3: Chapman University, United States of America","What’s in a Face? Gender representation of faces in Time, 1940s-1990s","Time Magazine, Gender representation, Image analysis","English
North America
20th Century
cultural analytics
image processing and analysis
Computer science
History",English,North America,20th Century,"cultural analytics
image processing and analysis","Computer science
History","<p><strong></strong><strong>Introduction</strong></p>

<p>Beginning with its inception in 1923, <em>Time </em>magazine, perhaps more than any comparable publication, has both reflected and influenced American popular attitudes to domestic and global politics. These include the changing ideas about women since the mid-twentieth century, which is the subject of this paper.</p>
<p>We used supervised machine learning to extract 327,322 visual images of faces from an archive of <em>Time</em> magazine, which contains 3389 issues ranging from 1923 to 2014, and computationally classified the faces as male or female. We then closely read selected <em>Time</em> articles to make sense of this quantitative data against the background of postwar feminism writ-large, and the history of the magazine itself. Our focus is on the period between the 1940s and the 1990s, which witnessed significant changes in attitudes toward women, and where our data of the proportion of female faces exhibits significant fluctuation.</p>
<p>We found four clear phases in the visual representation of women in <em>Time</em> from the 1940s to the 1990s: a peak in the mid-to-late 1940s, a dip from the mid-1950s to early 1960s, another peak in the 1970s, and another dip in the 1980s. The number of female faces depicted in <em>Time</em> then rises steadily since the early-1990s. We interpret these variations through an interdisciplinary framework. Through our combined quantitative and qualitative approach, we found that the percentage of female faces found in <em>Time</em> between 1940 and 1990 correlates with attitudes towards women in both the larger historical context as well as within the textual content of the magazine.</p>
<p><strong>Methods and Results</strong></p>
<p>We first collected data through human labor using Amazon Mechanical Turk (AMT) to identify and tag faces from the archive. This data was used to train a RetinaNet detector[1] to automatically identify and extract faces from the remainder of the archive. Using an accuracy threshold of 90 percent yielded 327,322 faces. A pre-trained face descriptor convolutional neural network VGGFace[2] was then fine-tuned and used to classify each face as either male or female.</p>
<p>Similar patterns emerge from both the AMT data and the automated data, featuring an increase in the proportion of female faces from the 1920s to 1945, a post-Second World War dip, a rebound beginning in the mid-1960s, followed by a decrease in the 1980’s, and a final rebound beginning in the early-1990s. Since similar trend lines were found using the AMT and the automatic extraction data, our analysis focuses on the latter, more comprehensive data set.</p>
<p>The proportion of women in each issue is shown in Figure 1. While there is a significant amount of variance per issue (compared to when the data is aggregated per year), a clear trendline emerges when the data is Lowess smoothed. Charting the proportion of women in each issue was useful for identifying outliers for our close reading analysis.</p>
<p align=""center""></p>
<p align=""center""><em>Figure 1: The percentage of women’s faces in each issue. The solid line is a Lowess smoothed version of the data.</em></p>

<p>To interpret the image data, we analyzed <em>Time</em> within the broader historical context of the 20<sup>th</sup> century. Our analysis consisted of a close reading of selected issues and articles, chosen based on the following criteria: 1) outlier issues from Fig. 2 defined as those between 1940 and 1990 with > 40% women, 2) issues and articles that were referenced in our secondary sources, and 3) results from EBSCO’s <em>Academic Search Complete</em> database, which we used to retrieve all articles in which the word ‘woman’ or ‘women’ or ‘housewife’ was mentioned within our dates of interest.</p>
<p>Our quantitative data coincides closely with the findings of our qualitative analysis. The number of images of women in <em>Time</em> magazine increases during the second world war as women’s role in the workforce expanded beyond traditional ‘feminine’ occupations to fill the gap and to satisfy increased production in the defence industry[3]. In the post-war years, the number images of women decreases as women across North America were instructed by “social engineers, such as psychologists, that they needed to be good wives and mothers in order to fit normally into post-war life”[4]. Our close reading analysis revealed that during these years, notable women who made the news bore the label of ‘housewife’, regardless of whether or not they were actually a housewife. The number of images of women increases once again as the women’s liberation movement began to take shape in the mid-sixties, peaking at the height of the movement in the 1970s. During this time, the women’s movement pushed the institutional structure of <em>Time</em> magazine to change from a gender-based caste system to a more equitable work place[5]. In the 1980s, there is a decrease in images, and we believe that this drop in the representation of women is consistent with the analysis in Susan Faludi’s well-known book <em>Backlash</em>[6]<em>, </em>in which she describes the 1980s as a decade that rejected feminism. The representation of women then starts increasing once again in the 1990s and onward.</p>
<p><strong>Conclusion</strong></p>
<p>We found that a distant reading of the images of faces in <em>Time</em> magazine is consistent with a historical analysis of American socio-political trends and with a close reading of the magazine’s content. Specifically, we found that the percentage of female faces peaks during eras when women have been more active in public life, and wanes in eras of backlash against women’s rights. This finding is particularly relevant in our contemporary post-literate world in which people absorb culture through images, and spend more time scanning images than reading print content.</p>

<p>[1] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar, “Focal Loss for Dense Object Detection,” <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> (2018): 1–1, https://doi.org/10.1109/TPAMI.2018.2858826;</p>
<p>Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár, “Focal Loss for Dense Object Detection,” <em>ArXiv:1708.02002 [Cs]</em> (August 2017), http://arxiv.org/abs/1708.02002.</p>
<p>[2] P Omkar M. Parkhi, Andrea Vedaldi, and Andrew Zisserman, “Deep Face Recognition,” in <em>Proceedings of the British Machine Vision Conference 2015</em>, 41.1–41.12 (Swansea: British Machine Vision Association, 2015), https://doi.org/10.5244/C.29.41; Refik Can Malli, “VGGFace Implementation with Keras Framework. Contribute to Rcmalli/Keras-Vggface Development by Creating an Account on GitHub,” <em>GitHub</em>, [2016] 2019, accessed September 28, 2019, https://github.com/rcmalli/keras-vggface.</p>
<p>[3] Ellen Carol DuBois and Lynn Dumenil, <em>Through Women’s Eyes: An American History with Documents</em>, 2nd ed. (Boston: Bedford/St. Martin’s, 2009), 548–53.</p>
<p>[4] Mona Gleason, <em>Psychology, Schooling, and the Family in Postwar Canada</em> (Toronto: University of Toronto Press, 1999), 52.</p>
<p>[5] Curtis Prendergast and Geoffrey Colvin, <em>The World of Time Inc: The Intimate History of a Changing Enterprise 1960 – 1980</em> (New York: Atheneum, 1986)</p>
<p>[6] Susan Faludi, <em>Backlash: The Undeclared War Against American Women</em> (Anniversary edition) (New York: Broadway Books, 2006).</p>
","Introduction
Beginning with its inception in 1923, Time magazine, perhaps more than any comparable publication, has both reflected and influenced American popular attitudes to domestic and global politics. These include the changing ideas about women since the mid-twentieth century, which is the subject of this paper.
We used supervised machine learning to extract 327,322 visual images of faces from an archive of Time magazine, which contains 3389 issues ranging from 1923 to 2014, and computationally classified the faces as male or female. We then closely read selected Time articles to make sense of this quantitative data against the background of postwar feminism writ-large, and the history of the magazine itself. Our focus is on the period between the 1940s and the 1990s, which witnessed significant changes in attitudes toward women, and where our data of the proportion of female faces exhibits significant fluctuation.
We found four clear phases in the visual representation of women in Time from the 1940s to the 1990s: a peak in the mid-to-late 1940s, a dip from the mid-1950s to early 1960s, another peak in the 1970s, and another dip in the 1980s. The number of female faces depicted in Time then rises steadily since the early-1990s. We interpret these variations through an interdisciplinary framework. Through our combined quantitative and qualitative approach, we found that the percentage of female faces found in Time between 1940 and 1990 correlates with attitudes towards women in both the larger historical context as well as within the textual content of the magazine.
Methods and Results
We first collected data through human labor using Amazon Mechanical Turk (AMT) to identify and tag faces from the archive. This data was used to train a RetinaNet detector[1] to automatically identify and extract faces from the remainder of the archive. Using an accuracy threshold of 90 percent yielded 327,322 faces. A pre-trained face descriptor convolutional neural network VGGFace[2] was then fine-tuned and used to classify each face as either male or female.
Similar patterns emerge from both the AMT data and the automated data, featuring an increase in the proportion of female faces from the 1920s to 1945, a post-Second World War dip, a rebound beginning in the mid-1960s, followed by a decrease in the 1980’s, and a final rebound beginning in the early-1990s. Since similar trend lines were found using the AMT and the automatic extraction data, our analysis focuses on the latter, more comprehensive data set.
The proportion of women in each issue is shown in Figure 1. While there is a significant amount of variance per issue (compared to when the data is aggregated per year), a clear trendline emerges when the data is Lowess smoothed. Charting the proportion of women in each issue was useful for identifying outliers for our close reading analysis.
Figure 1: The percentage of women’s faces in each issue. The solid line is a Lowess smoothed version of the data.
To interpret the image data, we analyzed Time within the broader historical context of the 20th century. Our analysis consisted of a close reading of selected issues and articles, chosen based on the following criteria: 1) outlier issues from Fig. 2 defined as those between 1940 and 1990 with > 40% women, 2) issues and articles that were referenced in our secondary sources, and 3) results from EBSCO’s Academic Search Complete database, which we used to retrieve all articles in which the word ‘woman’ or ‘women’ or ‘housewife’ was mentioned within our dates of interest.
Our quantitative data coincides closely with the findings of our qualitative analysis. The number of images of women in Time magazine increases during the second world war as women’s role in the workforce expanded beyond traditional ‘feminine’ occupations to fill the gap and to satisfy increased production in the defence industry[3]. In the post-war years, the number images of women decreases as women across North America were instructed by “social engineers, such as psychologists, that they needed to be good wives and mothers in order to fit normally into post-war life”[4]. Our close reading analysis revealed that during these years, notable women who made the news bore the label of ‘housewife’, regardless of whether or not they were actually a housewife. The number of images of women increases once again as the women’s liberation movement began to take shape in the mid-sixties, peaking at the height of the movement in the 1970s. During this time, the women’s movement pushed the institutional structure of Time magazine to change from a gender-based caste system to a more equitable work place[5]. In the 1980s, there is a decrease in images, and we believe that this drop in the representation of women is consistent with the analysis in Susan Faludi’s well-known book Backlash[6], in which she describes the 1980s as a decade that rejected feminism. The representation of women then starts increasing once again in the 1990s and onward.
Conclusion
We found that a distant reading of the images of faces in Time magazine is consistent with a historical analysis of American socio-political trends and with a close reading of the magazine’s content. Specifically, we found that the percentage of female faces peaks during eras when women have been more active in public life, and wanes in eras of backlash against women’s rights. This finding is particularly relevant in our contemporary post-literate world in which people absorb culture through images, and spend more time scanning images than reading print content.
[1] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar, “Focal Loss for Dense Object Detection,” IEEE Transactions on Pattern Analysis and Machine Intelligence (2018): 1–1, https://doi.org/10.1109/TPAMI.2018.2858826;
Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár, “Focal Loss for Dense Object Detection,” ArXiv:1708.02002 [Cs] (August 2017), http://arxiv.org/abs/1708.02002.
[2] P Omkar M. Parkhi, Andrea Vedaldi, and Andrew Zisserman, “Deep Face Recognition,” in Proceedings of the British Machine Vision Conference 2015, 41.1–41.12 (Swansea: British Machine Vision Association, 2015), https://doi.org/10.5244/C.29.41; Refik Can Malli, “VGGFace Implementation with Keras Framework. Contribute to Rcmalli/Keras-Vggface Development by Creating an Account on GitHub,” GitHub, [2016] 2019, accessed September 28, 2019, https://github.com/rcmalli/keras-vggface.
[3] Ellen Carol DuBois and Lynn Dumenil, Through Women’s Eyes: An American History with Documents, 2nd ed. (Boston: Bedford/St. Martin’s, 2009), 548–53.
[4] Mona Gleason, Psychology, Schooling, and the Family in Postwar Canada (Toronto: University of Toronto Press, 1999), 52.
[5] Curtis Prendergast and Geoffrey Colvin, The World of Time Inc: The Intimate History of a Changing Enterprise 1960 – 1980 (New York: Atheneum, 1986)
[6] Susan Faludi, Backlash: The Undeclared War Against American Women (Anniversary edition) (New York: Broadway Books, 2006).","jofrea@sunypoly.edu, acole3@gmail.com, realemj@sunypoly.edu, berardi@chapman.edu",Long Presentation
"Lu, Yaya Chenyue (1);
Swift, Ben (1);
Hawes, Greta (2)","1: Research School of Computer Science, The Australian National University;
2: Research School of Humanities & the Arts, The Australian National University",Analysis and Visualisation of Complex Familial Relationships in Greek Mythology,"Greek mythology, genealogy, graphic design, relationship analysis, graphing algorithms","Global
Europe
English
BCE-4th Century
Contemporary
digital biography, personography, and prosopography
Interface design, development, and analysis
Humanities computing
Literary studies",English,"Global
Europe","BCE-4th Century
Contemporary","digital biography, personography, and prosopography
Interface design, development, and analysis","Humanities computing
Literary studies","<p>Greek Mythology contains numerous idiosyncratic familial relationships that pose unusual challenges for systematic analysis and visualisation. This project examines these idiosyncracies by using a subset of the Greek Mythology data collected according to the MANTO ontology developed by Dr Greta Hawes. This includes the design and development of a user-friendly web-based tool and graph layout that visualises and explores these complex relationships in the context of the digital humanities. In the process, it hopes to generate new avenues of research for data visualisation of complex linked open data structures, and raise societal awareness for unconventional ""family"" and ""relationship"" types.</p>
","Greek Mythology contains numerous idiosyncratic familial relationships that pose unusual challenges for systematic analysis and visualisation. This project examines these idiosyncracies by using a subset of the Greek Mythology data collected according to the MANTO ontology developed by Dr Greta Hawes. This includes the design and development of a user-friendly web-based tool and graph layout that visualises and explores these complex relationships in the context of the digital humanities. In the process, it hopes to generate new avenues of research for data visualisation of complex linked open data structures, and raise societal awareness for unconventional ""family"" and ""relationship"" types.","yaya.lu@anu.edu.au, ben.swift@anu.edu.au, greta.hawes@anu.edu.au",Poster
"Lengo, Maxim","Bar-Ilan University, Israel",From Millions of Words to a Single Phrase: Examination of the TXM Software in Hebrew,"Textometrical Research, TXM software, Online Social Networks","Global
Europe
English
North America
Contemporary
database creation, management, and analysis
social media analysis and methods
Communication studies
Linguistics",English,"Global
Europe
North America",Contemporary,"database creation, management, and analysis
social media analysis and methods","Communication studies
Linguistics","<p>Following the technological development during the second half of the 20<sup>th</sup> Century, linguistic researchers have begun for the first time to analyze corpora of big data. One research methodology, which was developed for lexicometry and text statistical analysis, is Textometry. I.e., the attempt to combine various statistical analysis techniques, such as factorial correspondence analysis (Benzécri, 1977) and hierarchical ascendant classification (Ward Jr, 1963), with full-text search techniques such as kwic concordances (Luhn, 1960), in order to trace the precise original editorial context of any textual event participating to the analysis.</p>
<p>The TXM software (Heiden, 2010), which was developed in France as a modular platform of a new generation of textometrical research and which I adapted to Hebrew, gives the ability to analyze a large corpus of texts as in my research, by using tools and methods based on linguistics and discourse analysis, that is, decomposing the text into factors and elements, carrying out statistical analysis, identifying the hidden social patterns, and then restoring the corpus to its original mode.</p>
<p>As a Ph.D. student in the discipline of Social Sciences, my research focuses on the image repair theory (Benoit, 2015) as an ensemble of strategies such as evading responsibility and reducing offensiveness, used by individuals, organizations and groups in order to repair their image during times of crisis. It examines the ways in which rhetorical measures are used in online verbal exchanges among users of the online social networks who attempt to repair their personal image. To achieve my goal, I use the TXM software to analyze a corpus of more than eight million words in 365 Facebook posts, which were published by the Israeli Prime Minister Benjamin Netanyahu during his current affairs, and more than 285,000 comments, made by the users. Netanyahu's Affairs are four police investigations in which he is involved as a suspect or has given a testimony.</p>
<p>During the poster session, I will present a review of some tools I used with the TXM software during the digitized analysis process in my Ph.D. research and the way they allowed me to recognize the following key phrase used by Netanyahu: ""They have the media, we have you"". Among these tools are Progression, which is the frequency of occurrence of one term throughout the text corpus; Co-occurrence, which is the frequency of occurrence of two terms in a text corpus alongside each other in a certain order; and Specificity, which is the score a term is given based on its occurrence in the corresponding part of the corpus relative to the one in the entire corpus, and indicates whether it is overused, underused or useless.</p>
<p align=""center""></p>
<p align=""center""><strong>Bibliography</strong></p>
<p align=""center""></p>
<p>Benoit, W. L. (2015). <em>Accounts, excuses, and apologies: Image repair theory and </em><em>research</em>. Albany, New York: State University of New York Press.</p>
<p>Benzécri, J. P. (1977). Sur l'analyse des tableaux binaires associés à une correspondance multiple. <em>Cahiers de l'Analyse des Donn</em><em>é</em><em>es, 2(1)</em>, 55-71.</p>
<p>Heiden, S. (2010). The TXM Platform: Building Open-Source Textual Analysis Software Compatible with the TEI Encoding Scheme. In R. Otoguro, Ishikawa, K., Umemoto, H., Yoshimoto, K., Harada, Y. (Eds.), <em>24th Pacific Asia Conference on Language, Information and Computation - PACLIC24</em> (Pp. 389-398). Waseda University, Sendai, Japan: Institute for Digital Enhancement of Cognitive Development.</p>
<p>Luhn, H. P. (1960). Key word‐in‐context index for technical literature (kwic index). <em>American Documentation, 11(4)</em>, 288-295.</p>
<p>Ward Jr, J. H. (1963). Hierarchical grouping to optimize an objective function. <em>Journal </em><em>of the American statistical association, 58(301)</em>, 236-244.</p>
","Following the technological development during the second half of the 20th Century, linguistic researchers have begun for the first time to analyze corpora of big data. One research methodology, which was developed for lexicometry and text statistical analysis, is Textometry. I.e., the attempt to combine various statistical analysis techniques, such as factorial correspondence analysis (Benzécri, 1977) and hierarchical ascendant classification (Ward Jr, 1963), with full-text search techniques such as kwic concordances (Luhn, 1960), in order to trace the precise original editorial context of any textual event participating to the analysis.
The TXM software (Heiden, 2010), which was developed in France as a modular platform of a new generation of textometrical research and which I adapted to Hebrew, gives the ability to analyze a large corpus of texts as in my research, by using tools and methods based on linguistics and discourse analysis, that is, decomposing the text into factors and elements, carrying out statistical analysis, identifying the hidden social patterns, and then restoring the corpus to its original mode.
As a Ph.D. student in the discipline of Social Sciences, my research focuses on the image repair theory (Benoit, 2015) as an ensemble of strategies such as evading responsibility and reducing offensiveness, used by individuals, organizations and groups in order to repair their image during times of crisis. It examines the ways in which rhetorical measures are used in online verbal exchanges among users of the online social networks who attempt to repair their personal image. To achieve my goal, I use the TXM software to analyze a corpus of more than eight million words in 365 Facebook posts, which were published by the Israeli Prime Minister Benjamin Netanyahu during his current affairs, and more than 285,000 comments, made by the users. Netanyahu's Affairs are four police investigations in which he is involved as a suspect or has given a testimony.
During the poster session, I will present a review of some tools I used with the TXM software during the digitized analysis process in my Ph.D. research and the way they allowed me to recognize the following key phrase used by Netanyahu: ""They have the media, we have you"". Among these tools are Progression, which is the frequency of occurrence of one term throughout the text corpus; Co-occurrence, which is the frequency of occurrence of two terms in a text corpus alongside each other in a certain order; and Specificity, which is the score a term is given based on its occurrence in the corresponding part of the corpus relative to the one in the entire corpus, and indicates whether it is overused, underused or useless.
Bibliography
Benoit, W. L. (2015). Accounts, excuses, and apologies: Image repair theory and research. Albany, New York: State University of New York Press.
Benzécri, J. P. (1977). Sur l'analyse des tableaux binaires associés à une correspondance multiple. Cahiers de l'Analyse des Données, 2(1), 55-71.
Heiden, S. (2010). The TXM Platform: Building Open-Source Textual Analysis Software Compatible with the TEI Encoding Scheme. In R. Otoguro, Ishikawa, K., Umemoto, H., Yoshimoto, K., Harada, Y. (Eds.), 24th Pacific Asia Conference on Language, Information and Computation - PACLIC24 (Pp. 389-398). Waseda University, Sendai, Japan: Institute for Digital Enhancement of Cognitive Development.
Luhn, H. P. (1960). Key word‐in‐context index for technical literature (kwic index). American Documentation, 11(4), 288-295.
Ward Jr, J. H. (1963). Hierarchical grouping to optimize an objective function. Journal of the American statistical association, 58(301), 236-244.",maximlengo@gmail.com,Poster
"Papantonakis, Panagiotis (1);
Fitsilis, Fotios (2);
Leventis, Sotiris (3);
Mikros, George (4)","1: Hellenic OCR Team;
2: Hellenic Parliament;
3: Hypernetica;
4: Hamad Bin Khalifa University, Qatar",Xtralingua: An open-source tool for extracting quantitative text profiles,"quantitative text analysis, quantitative text profiles, Hellenic OCR Team, Google Summer of Code, Open Science","Global
English
Contemporary
natural language processing
text mining and analysis
Humanities computing
Linguistics",English,Global,Contemporary,"natural language processing
text mining and analysis","Humanities computing
Linguistics","<p>The aim of this poster is to present a novel tool for extracting quantitative text profiles from corpora using a friendly Graphical User Interface. Xtralingua is a software that incorporates over 60 specialized quantitative text analysis measurements including text readability and lexical diversity indices as well as specialized measurements in the text inspired by theoretical work done in the area of Quantitative Linguistics. The tool is open-source and can be further enriched with custom quantitative text indices that the users can add using a scripting language. Xtralingua offers researchers with no specialized technical skills the ability to quickly extract rich quantitative text profiles for further processing. Moreover, it is easy to operate and can support both research and teaching needs in a variety of DH topics.</p>
","The aim of this poster is to present a novel tool for extracting quantitative text profiles from corpora using a friendly Graphical User Interface. Xtralingua is a software that incorporates over 60 specialized quantitative text analysis measurements including text readability and lexical diversity indices as well as specialized measurements in the text inspired by theoretical work done in the area of Quantitative Linguistics. The tool is open-source and can be further enriched with custom quantitative text indices that the users can add using a scripting language. Xtralingua offers researchers with no specialized technical skills the ability to quickly extract rich quantitative text profiles for further processing. Moreover, it is easy to operate and can support both research and teaching needs in a variety of DH topics.","panpapantonakis@gmail.com, fotis@fitsilis.gr, sotiris.leventis@hypernetica.com, gmikros@gmail.com",Poster
"Riguet, Marine;
Alrahabi, Motasem","Labex OBVIL, France",Analyse automatique pour une étude du genre : quels jugements des écrivaines au XIXe siècle ?,"Literary criticism, sentiment analysis, semantic annotation, gender studies, named entity recognition","Europe
French
19th Century
semantic analysis
text mining and analysis
Gender and sexuality studies
Literary studies",French,Europe,19th Century,"semantic analysis
text mining and analysis","Gender and sexuality studies
Literary studies","<p>Cet article présente une méthode d'annotation sémantique développée afin d’étudier le traitement particulier des écrivaines dans la critique littéraire française de la seconde moitié du XIX<sup>e</sup> siècle. Nous espérons ainsi circonscrire un discours sur la littérature féminine et questionner une pensée littéraire façonnée au prisme du genre. Mais nous entendons également proposer une méthode d’analyse sémantique exportable à d’autres discours, et adaptable aux besoins spécifiques d’autres recherches littéraires.</p>
","Cet article présente une méthode d'annotation sémantique développée afin d’étudier le traitement particulier des écrivaines dans la critique littéraire française de la seconde moitié du XIXe siècle. Nous espérons ainsi circonscrire un discours sur la littérature féminine et questionner une pensée littéraire façonnée au prisme du genre. Mais nous entendons également proposer une méthode d’analyse sémantique exportable à d’autres discours, et adaptable aux besoins spécifiques d’autres recherches littéraires.","marineriguet@gmail.com, motasem.alrahabi@gmail.com",Short Presentation
"Alassi, Sepideh (1);
Rosenthaler, Lukas (1);
Iliffe, Rob (2)","1: Digital Humanities Lab, University of Basel;
2: Faculty of History, University of Oxford",An Interactive 3D Visualization of RDF-based Digital Editions,"RDF-graph, 3D, visualization, VR, force-directed graph","Global
English
15th-17th Century
18th Century
Contemporary
network analysis and graphs theory and application
semantic analysis
History of science",English,Global,"15th-17th Century
18th Century
Contemporary","network analysis and graphs theory and application
semantic analysis",History of science,"<p>Humanities research produces a vast amount of data that needs to be visualized to understand and interpret the underlying facts. Modeling data with RDF based OWL ontologies defines a directed graph where nodes are the resources, and the properties the edges. Many digital humanities projects visualize the RDF graphs by flattening them into two dimensions. Although this representation helps researchers with recognizing the direct and indirect connections between the resources, it suffers from loss of information due to the overlap of nodes and edges. One can overcome this problem by visualizing the data as a 3D force graph. An interactive 3D visualization also introduces tangibility to the displayed data so that researchers can rotate the model to study the distribution of the data from every angle. The visualization tool is web-based and connected to a platform that serves the digital editions. This connection enables the users to access the underlying resources by directly clicking on the nodes.</p>
<p>Figure 1: 3D force-directed graph of Newton's scientific correspondence</p>
<p>Representation of the data as a 3D force-directed graph is commonly used for scientific data (Paananen, Wong 2009), but it can be easily adapted to the humanities data as well. To illustrate this, we have chosen to visualize the early modern scientific correspondences. Our database consists of the correspondences of natural philosophers such as Leibniz, Newton, Leonhard Euler, and members of the Bernoulli dynasty. There is already a network that connects the digital editions of these correspondences and makes them openly accessible to the public through one platform (Alassi et al., 2019). This platform is a virtual research environment based on the Knora API (https://www.knora.org/), which manages and stores the data as RDF.<sub></sub>Users of this platform can access and query the data online using Angular based front-end components of the API, Knora-ui (https://github.com/dasch-swiss/knora-ui). The 3D visualization tool will be integrated into this user interface and will be openly accessible online. Through Knora, one can derive the graph of the data in the JSON format directly from the triplestore, which is then employed to create a real-time 3D simulation (Figure 1). A configuration step defines graphical features, such as shape, colors, and labels of the nodes and edges (Figure 2). The repulsive forces prescribed on the nodes and edges prevent the overlays of the graph components, and spring-like characteristics of the edges restrain the movement of the nodes. This leads to the formation of clusters of objects that are well connected.</p>
<p>Figure 2: Sample 3D visualization of RDF triples.</p>
<p>Since time plays a crucial role in the study of the historical facts, an additional dimension has to be introduced into the model to represent the time (Schweizer et al., 2015, p.321). In the 3D model, this can be achieved by the dynamic appearance of the nodes relative to the creation date of the letters. All features developed for this tool will be generic and can be used to visualize any RDF based humanities data. We also intend to generate a virtual reality version of the 3D simulations to enhance the interaction of the users with the data.</p>
<p><strong>Bibliography</strong></p>
<p>Paananen, Jussi and Wong, Garry (2009), FORG3D: Force-directed 3D graph editor for visualization of integrated genome scale data, BMC Systems Biology, 2019. DOI:10.1186/1752-0509-3-26</p>
<p>Alassi, Sepideh and Schweizer, Tobias, et al. (2019) Newton virtually meets Euler and Bernoulli, DH2019, https://dev.clariah.nl/files/dh2019/boa/0421.html</p>
<p>Schweizer, Tobias and Rosenthaler, Lukas and Subotic, Ivan (2015) Visualizierung von Annotationen und Verknüpfungen in SALSAH, Geschichte und Informatik, vol.18/19.</p>
","Humanities research produces a vast amount of data that needs to be visualized to understand and interpret the underlying facts. Modeling data with RDF based OWL ontologies defines a directed graph where nodes are the resources, and the properties the edges. Many digital humanities projects visualize the RDF graphs by flattening them into two dimensions. Although this representation helps researchers with recognizing the direct and indirect connections between the resources, it suffers from loss of information due to the overlap of nodes and edges. One can overcome this problem by visualizing the data as a 3D force graph. An interactive 3D visualization also introduces tangibility to the displayed data so that researchers can rotate the model to study the distribution of the data from every angle. The visualization tool is web-based and connected to a platform that serves the digital editions. This connection enables the users to access the underlying resources by directly clicking on the nodes.
Figure 1: 3D force-directed graph of Newton's scientific correspondence
Representation of the data as a 3D force-directed graph is commonly used for scientific data (Paananen, Wong 2009), but it can be easily adapted to the humanities data as well. To illustrate this, we have chosen to visualize the early modern scientific correspondences. Our database consists of the correspondences of natural philosophers such as Leibniz, Newton, Leonhard Euler, and members of the Bernoulli dynasty. There is already a network that connects the digital editions of these correspondences and makes them openly accessible to the public through one platform (Alassi et al., 2019). This platform is a virtual research environment based on the Knora API (https://www.knora.org/), which manages and stores the data as RDF.Users of this platform can access and query the data online using Angular based front-end components of the API, Knora-ui (https://github.com/dasch-swiss/knora-ui). The 3D visualization tool will be integrated into this user interface and will be openly accessible online. Through Knora, one can derive the graph of the data in the JSON format directly from the triplestore, which is then employed to create a real-time 3D simulation (Figure 1). A configuration step defines graphical features, such as shape, colors, and labels of the nodes and edges (Figure 2). The repulsive forces prescribed on the nodes and edges prevent the overlays of the graph components, and spring-like characteristics of the edges restrain the movement of the nodes. This leads to the formation of clusters of objects that are well connected.
Figure 2: Sample 3D visualization of RDF triples.
Since time plays a crucial role in the study of the historical facts, an additional dimension has to be introduced into the model to represent the time (Schweizer et al., 2015, p.321). In the 3D model, this can be achieved by the dynamic appearance of the nodes relative to the creation date of the letters. All features developed for this tool will be generic and can be used to visualize any RDF based humanities data. We also intend to generate a virtual reality version of the 3D simulations to enhance the interaction of the users with the data.
Bibliography
Paananen, Jussi and Wong, Garry (2009), FORG3D: Force-directed 3D graph editor for visualization of integrated genome scale data, BMC Systems Biology, 2019. DOI:10.1186/1752-0509-3-26
Alassi, Sepideh and Schweizer, Tobias, et al. (2019) Newton virtually meets Euler and Bernoulli, DH2019, https://dev.clariah.nl/files/dh2019/boa/0421.html
Schweizer, Tobias and Rosenthaler, Lukas and Subotic, Ivan (2015) Visualizierung von Annotationen und Verknüpfungen in SALSAH, Geschichte und Informatik, vol.18/19.","sepideh.alassi@unibas.ch, lukas.rosenthaler@unibas.ch, robert.iliffe@history.ox.ac.uk",Short Presentation
"Heßbrüggen-Walter, Stefan",National Research University Higher School of Economics,What Is Formalisation? And Why Do We Need to Talk About It In DH?,"formalisation, markup, dh software, philosophy","Global
English
Contemporary
meta-criticism (reflections on digital humanities and humanities computing)
Philosophy",English,Global,Contemporary,meta-criticism (reflections on digital humanities and humanities computing),Philosophy,"<p>The digital humanities as a discipline are centered on ""formalisation"". We acknowledge the fact that the machines we use employ formal languages, but we still lack a clear idea of what follows from this insight for the self-conceptualisation of the discipline. My presentation clarifies the concept of formalisation and spells out some of its practical consequences.</p>
","The digital humanities as a discipline are centered on ""formalisation"". We acknowledge the fact that the machines we use employ formal languages, but we still lack a clear idea of what follows from this insight for the self-conceptualisation of the discipline. My presentation clarifies the concept of formalisation and spells out some of its practical consequences.",shessbru@hse.ru,Lightning
"Lui, Pengfei (1);
Loudcher, Sabine (1);
Darmont, Jérôme (1);
Perrin, Emmanuelle (2);
Girard, Jean-Pierre (2);
Rousset, Marie-Odile (2)","1: Université de Lyon, Lyon 2, ERIC EA 3083, France;
2: Maison de l’Orient et de la Méditerranée, France",Metadata model for an archeological data lake,"archéologie, thésaurus, lac de données, archives ouvertes, métadonnées","Europe
French
BCE-4th Century
5th-14th Century
20th Century
database creation, management, and analysis
systems and information architecture and usability
Archaeology
Computer science",French,Europe,"BCE-4th Century
5th-14th Century
20th Century","database creation, management, and analysis
systems and information architecture and usability","Archaeology
Computer science","<p>The HyperThesau project was initiated by a multidisciplinary team consisting of two research laboratories of archaeology and computer science, a digital library, two archeological museums and a private company. This project has two main objectives: 1) the design and implementation of an integrated platform to host, search, share and analyze archaeological data; 2) the design of a domain-specific thesaurus taking the whole archaeological data lifecycle into account.</p>
<p>Archeological data may bear many different types (documents, photos, drawings, sensor data, ...). The description of an archaeological object also differs with respect to users, usages and time. Such variety of archeological data induces many scientific challenges related to storing heterogeneous data in a centralized repository, guaranteeing data quality, cleaning and transforming the data to make them interoperable, finding and accessing data efficiently and cross-analyzing the data. To overcome all these challenges, we exploit the concept of data lake</p>
","The HyperThesau project was initiated by a multidisciplinary team consisting of two research laboratories of archaeology and computer science, a digital library, two archeological museums and a private company. This project has two main objectives: 1) the design and implementation of an integrated platform to host, search, share and analyze archaeological data; 2) the design of a domain-specific thesaurus taking the whole archaeological data lifecycle into account.
Archeological data may bear many different types (documents, photos, drawings, sensor data, ...). The description of an archaeological object also differs with respect to users, usages and time. Such variety of archeological data induces many scientific challenges related to storing heterogeneous data in a centralized repository, guaranteeing data quality, cleaning and transforming the data to make them interoperable, finding and accessing data efficiently and cross-analyzing the data. To overcome all these challenges, we exploit the concept of data lake","liu.pengfei@hotmail.fr, sabine.loudcher@univ-lyon2.fr, jerome.darmont@univ-lyon2.fr, emmanuelle.perrin@mom.fr, truelles-pixels@laposte.net, marie-odile.rousset@mom.fr",Short Presentation
"Ivanov, Lubomir","Iona College, United States of America",Haiku Author Recognition,"author attribution and identification, haiku, machine learning","English
North America
15th-17th Century
18th Century
19th Century
artificial intelligence and machine learning
attribution studies and stylometric analysis
Computer science
Literary studies",English,North America,"15th-17th Century
18th Century
19th Century","artificial intelligence and machine learning
attribution studies and stylometric analysis","Computer science
Literary studies","<p align=""center""><strong>Haiku Author Recognition</strong></p>
<p>1. <strong>Introduction</strong></p>
<p>Haiku is a Japanese poetic form renowned for its brevity and expressiveness. Haiku derives from <em>renga/renku</em> – collaborative collections of verses with a 3-line opening <em>hokku</em> verse in the form 5-7-5 <em>on</em> (equiv. <em>syllable</em>). Matsuo Basho made famous the stand-alone <em>hokku</em> form, preserving the 5-7-5 <em>on</em> structure. The name <em>haiku</em> was associated with this form of <em>hokku</em> during 19<sup>th</sup> century.</p>
<p>Four haiku authors rise in prominence above all: Matsuo Basho (17<sup>th</sup> century) is considered the “father” of haiku. Yosa Buson (18<sup>th</sup> century) focused on haiku as an art rather than a reflection of reality. Buson combined hokku with painting, inventing <em>haiga</em> (verse-painting). Kobayashi Issa (18-19<sup>th</sup> century) reinvented haiku through his depth of feeling and humanism. In the second half of the 19<sup>th</sup> century, Masaoka Shiki critically re-evaluated the art of haiku (coining the term), braking away from the traditional 5-7-5 form, and popularizing the poetic style beyond Japan.</p>
<p>We present a study, which employs authorship attribution techniques to determine the distinctiveness of poetic styles in haiku, focusing on the poetry of Basho, Buson, Issa, and Shiki. There has been little work in the field of haiku attribution. A theoretical study of phonological complexity in haiku was presented in [1]. An approach to automatic evaluation of the quality of haiku was presented in [2]. An interesting work [3] deals with identifying unintended haiku in text. We approach haiku attribution as a classification problem: Given a set of attributed haikus, we train classifiers to recognize the writing style of each poet, and apply an ensemble of trained models to unattributed texts.</p>
<p><strong>2. Our Haiku Corpus</strong></p>
<p>The first step in creating our model was obtaining a haiku corpus. There are three approaches:</p>
<ul><li>Use actual haikus written in hiragana (a form of Japanese alphabet)</li>
<li>Use Roman alphabet transcriptions (rōmaji) of haikus.</li>
<li>Use English translations of haikus.</li>
</ul><p>While using hiragana haikus is arguably the best option, our software lacks the capability to process hiragana text. English translations of haikus are readily available, but while research suggests that the authorial signal is stronger than the translators’ [4], we do not know if that applies to haiku. We opted to construct a corpus of rōmaji transcribed haikus. This was difficult since most resources are either hiragana originals or translations. We obtained 723 haikus by Basho from [5], 842 haikus by Buson from [6], and 603 haikus by Issa from [7]. Finding transcribed Shiki haikus proved extremely challenging. Even though Shiki wrote over 24000 haikus, only a handful have been transcribed into rōmaji. Failing to secure transcriptions, we downloaded the full set of 24000 hiragana haikus from [8]. We then used an online hiragana-to-rōmaji transcription tool [9] to transcribe 967 randomly selected haikus by Shiki. Since many of the extracted haikus were organized alphabetically or by topic, we wrote Python code to randomly shuffle the order of the haikus for each author. A different program broke up the haikus into files of size 50 haikus each.</p>
<p><strong>3. Attribution Methodology</strong></p>
<p>Our attribution software is a based on JGAAP [10] and implements an ensemble of classifier/stylistic-feature pairs [11,12]. For this study, we limited the set of stylistic features to character-2/3/4/5-grams (CnG), word-2/3-grams (WnG), vowel-initiated words (VIW), and first-word-in-sentence (FWIS). The classifiers used were support vector machines with sequential minimal optimization (SMO) and multilayer perceptrons (MLP).</p>
<p><strong>4. Results</strong></p>
<p>We conducted several experiments, where we randomly chose one 50-haiku file for each author and removed it from the training set. We trained the classifiers on the remaining set of haikus using leave-one-out (L1O) validation. The results of the training for three sets of experiments are presented in Table 1:</p>
<p align=""center"">Table 1: Training Accuracy for Basho, Buson, Issa, and Shiki</p>
<p align=""center""></p>
<p>Next, we tested the authorship of the 50-haiku files that were left out of the training. The results of those experiments are presented in Table 2:</p>
<p align=""center"">Table 2: Attribution Results for Basho, Buson, Issa, and Shiki</p>
<p align=""center""></p>
<p>It is quite clear that even with a reduced set of stylistic features, the attribution is very strong and the author identification definitive. We conducted an additional set of experiments, where we used each of the trained models to test the authorship of five haikus by the 18<sup>th</sup> century haiku poet Takarai Kikaku. The models were not trained on Kikaku, so, as expected, the results were split among two or more authors (Table 3):</p>
<p align=""center"">Table 3: Attribution Results for Kikaku</p>
<p align=""center""></p>
<p>Interestingly, Kikaku was a prominent student and disciple of Basho, yet none of the models makes that association. This is most likely due to the small number of Kikaku haikus tested.</p>
<p><strong>5. Conclusion and Future Work</strong></p>
<p>We presented results from haiku author identification experiments, which suggest that haiku authorship can be determined even with a limited set of stylistic features from rōmaji-transcribed haikus. Our next efforts will be to experiment with a larger set of haiku authors, with English translations, and, possibly, with hiragana haikus. Among the questions we wish to answer are:</p>
<ul><li>What is the minimal set of haikus sufficient to identify an author?</li>
<li>Is the authorial signal stronger than the translator’s for haiku translations?</li>
<li>Can prosodic features be used for haiku author identification?</li>
<li>Does the historical period affect the accuracy of attribution?</li>
</ul><p><strong>References:</strong></p>
<p>[1]    Hayata, K. (2018). “Phonological Complexity in the Japanese Short Poetry: Coexistence Between Nearest-Neighbor Correlations and Far-Reaching Anticorrelations”. <em>Front. Phys</em>. 6:31. doi: 10.3389/fphy.2018.00031</p>
<p>[2]    Kikuchi, S. et al. (2016). “Quality estimation for Japanese Haiku poems using Neural Network”. <em>IEEE Symposium Series on Computational Intelligence (SSCI)</em>, Athens, Greece, DOI: 10.1109/SSCI.2016.7850030</p>
<p>[3]    Online resource: https://labs.ft.com/2016/07/finding-hidden-haiku/</p>
<p>[4]    Hoover, D. (2019). “The Invisible Translator Revisited”. <em>Digital Humanities Conference (DH2019)</em>. Utrecht, The Netherlands</p>
<p>[5]    Barnhill, D.L. (2004). “Basho’s Haiku: Selected Poems of Matsuo Basho”. <em>State University of New York Press</em>, ISBN-13: 978-0791461662</p>
<p>[6]    Persinger, A. (2013). ""Foxfire: The Selected Poems of Yosa Buson, a Translation"". <em>Theses and Dissertations</em>. Paper 748.</p>
<p>[7]    Terebes, G. (2000). “A cup of tea with Isa<em>”.</em> <em>Orpheusz Publishing House</em>, Budapest, 2000, c. bilingual, corrected and expanded version of volume 2.</p>
<p>[8]    Online resource: https://terebess.hu/english/haiku/shiki.html</p>
<p>[9]    Online resource: http://www.romajidesu.com/translator</p>
<p>[10] Juola, P. 2009. JGAAP: A system for comparative evaluation of authorship attribution. <em>Journal of the Chicago Colloquium on Digital Humanities and Computer Science</em>, 1(1): 1-5.</p>
<p>[11] Petrovic, S., Berton, G., Campbell, S., Ivanov, L. 2015. Attribution of 18th Century Political Writings Using Machine Learning. <em>Journal of Technologies in Society</em>, volume 11, issue 3, pp. 1-13</p>
<p>[12] Petrovic, S., Berton, G., Schiaffino, R., Ivanov, L. 2016. Examining the Thomas Paine Corpus: Automated Computer Author Attribution Methodology Applied to Thomas Paine’s Writings. <em>Chapter, New Directions in Thomas Paine Studies, Edition: 1</em>, Publisher: Palgrave Macmillan US, Editors: S. Cleary I. Stabell, DOI: 10.1057/9781137589996</p>
","Haiku Author Recognition
1. Introduction
Haiku is a Japanese poetic form renowned for its brevity and expressiveness. Haiku derives from renga/renku – collaborative collections of verses with a 3-line opening hokku verse in the form 5-7-5 on (equiv. syllable). Matsuo Basho made famous the stand-alone hokku form, preserving the 5-7-5 on structure. The name haiku was associated with this form of hokku during 19th century.
Four haiku authors rise in prominence above all: Matsuo Basho (17th century) is considered the “father” of haiku. Yosa Buson (18th century) focused on haiku as an art rather than a reflection of reality. Buson combined hokku with painting, inventing haiga (verse-painting). Kobayashi Issa (18-19th century) reinvented haiku through his depth of feeling and humanism. In the second half of the 19th century, Masaoka Shiki critically re-evaluated the art of haiku (coining the term), braking away from the traditional 5-7-5 form, and popularizing the poetic style beyond Japan.
We present a study, which employs authorship attribution techniques to determine the distinctiveness of poetic styles in haiku, focusing on the poetry of Basho, Buson, Issa, and Shiki. There has been little work in the field of haiku attribution. A theoretical study of phonological complexity in haiku was presented in [1]. An approach to automatic evaluation of the quality of haiku was presented in [2]. An interesting work [3] deals with identifying unintended haiku in text. We approach haiku attribution as a classification problem: Given a set of attributed haikus, we train classifiers to recognize the writing style of each poet, and apply an ensemble of trained models to unattributed texts.
2. Our Haiku Corpus
The first step in creating our model was obtaining a haiku corpus. There are three approaches:
Use actual haikus written in hiragana (a form of Japanese alphabet)
Use Roman alphabet transcriptions (rōmaji) of haikus.
Use English translations of haikus.
While using hiragana haikus is arguably the best option, our software lacks the capability to process hiragana text. English translations of haikus are readily available, but while research suggests that the authorial signal is stronger than the translators’ [4], we do not know if that applies to haiku. We opted to construct a corpus of rōmaji transcribed haikus. This was difficult since most resources are either hiragana originals or translations. We obtained 723 haikus by Basho from [5], 842 haikus by Buson from [6], and 603 haikus by Issa from [7]. Finding transcribed Shiki haikus proved extremely challenging. Even though Shiki wrote over 24000 haikus, only a handful have been transcribed into rōmaji. Failing to secure transcriptions, we downloaded the full set of 24000 hiragana haikus from [8]. We then used an online hiragana-to-rōmaji transcription tool [9] to transcribe 967 randomly selected haikus by Shiki. Since many of the extracted haikus were organized alphabetically or by topic, we wrote Python code to randomly shuffle the order of the haikus for each author. A different program broke up the haikus into files of size 50 haikus each.
3. Attribution Methodology
Our attribution software is a based on JGAAP [10] and implements an ensemble of classifier/stylistic-feature pairs [11,12]. For this study, we limited the set of stylistic features to character-2/3/4/5-grams (CnG), word-2/3-grams (WnG), vowel-initiated words (VIW), and first-word-in-sentence (FWIS). The classifiers used were support vector machines with sequential minimal optimization (SMO) and multilayer perceptrons (MLP).
4. Results
We conducted several experiments, where we randomly chose one 50-haiku file for each author and removed it from the training set. We trained the classifiers on the remaining set of haikus using leave-one-out (L1O) validation. The results of the training for three sets of experiments are presented in Table 1:
Table 1: Training Accuracy for Basho, Buson, Issa, and Shiki
Next, we tested the authorship of the 50-haiku files that were left out of the training. The results of those experiments are presented in Table 2:
Table 2: Attribution Results for Basho, Buson, Issa, and Shiki
It is quite clear that even with a reduced set of stylistic features, the attribution is very strong and the author identification definitive. We conducted an additional set of experiments, where we used each of the trained models to test the authorship of five haikus by the 18th century haiku poet Takarai Kikaku. The models were not trained on Kikaku, so, as expected, the results were split among two or more authors (Table 3):
Table 3: Attribution Results for Kikaku
Interestingly, Kikaku was a prominent student and disciple of Basho, yet none of the models makes that association. This is most likely due to the small number of Kikaku haikus tested.
5. Conclusion and Future Work
We presented results from haiku author identification experiments, which suggest that haiku authorship can be determined even with a limited set of stylistic features from rōmaji-transcribed haikus. Our next efforts will be to experiment with a larger set of haiku authors, with English translations, and, possibly, with hiragana haikus. Among the questions we wish to answer are:
What is the minimal set of haikus sufficient to identify an author?
Is the authorial signal stronger than the translator’s for haiku translations?
Can prosodic features be used for haiku author identification?
Does the historical period affect the accuracy of attribution?
References:
[1]    Hayata, K. (2018). “Phonological Complexity in the Japanese Short Poetry: Coexistence Between Nearest-Neighbor Correlations and Far-Reaching Anticorrelations”. Front. Phys. 6:31. doi: 10.3389/fphy.2018.00031
[2]    Kikuchi, S. et al. (2016). “Quality estimation for Japanese Haiku poems using Neural Network”. IEEE Symposium Series on Computational Intelligence (SSCI), Athens, Greece, DOI: 10.1109/SSCI.2016.7850030
[3]    Online resource: https://labs.ft.com/2016/07/finding-hidden-haiku/
[4]    Hoover, D. (2019). “The Invisible Translator Revisited”. Digital Humanities Conference (DH2019). Utrecht, The Netherlands
[5]    Barnhill, D.L. (2004). “Basho’s Haiku: Selected Poems of Matsuo Basho”. State University of New York Press, ISBN-13: 978-0791461662
[6]    Persinger, A. (2013). ""Foxfire: The Selected Poems of Yosa Buson, a Translation"". Theses and Dissertations. Paper 748.
[7]    Terebes, G. (2000). “A cup of tea with Isa”. Orpheusz Publishing House, Budapest, 2000, c. bilingual, corrected and expanded version of volume 2.
[8]    Online resource: https://terebess.hu/english/haiku/shiki.html
[9]    Online resource: http://www.romajidesu.com/translator
[10] Juola, P. 2009. JGAAP: A system for comparative evaluation of authorship attribution. Journal of the Chicago Colloquium on Digital Humanities and Computer Science, 1(1): 1-5.
[11] Petrovic, S., Berton, G., Campbell, S., Ivanov, L. 2015. Attribution of 18th Century Political Writings Using Machine Learning. Journal of Technologies in Society, volume 11, issue 3, pp. 1-13
[12] Petrovic, S., Berton, G., Schiaffino, R., Ivanov, L. 2016. Examining the Thomas Paine Corpus: Automated Computer Author Attribution Methodology Applied to Thomas Paine’s Writings. Chapter, New Directions in Thomas Paine Studies, Edition: 1, Publisher: Palgrave Macmillan US, Editors: S. Cleary I. Stabell, DOI: 10.1057/9781137589996",livanov@iona.edu,Short Presentation
"Gillis, Roger Christopher",Dalhousie University,"""Open GLAM"": Opening up digital cultural heritage collections for the digital humanities ","Copyright, Cultural Heritage, Digital Collections, Open Access, Open GLAM","Global
English
Contemporary
copyright, licensing, and permissions standards, systems, and processes
digital access, privacy, and ethics analysis
Galleries and museum studies
Library & information science",English,Global,Contemporary,"copyright, licensing, and permissions standards, systems, and processes
digital access, privacy, and ethics analysis","Galleries and museum studies
Library & information science","<p>Over the past twenty plus years, cultural heritage organizations have put vast amounts of digitized available online.The movement towards Open Access for digitized cultural heritage has come to be known as Open GLAM (Galleries, Libraries, Archives, and Museums). This presentation will explore key issues and recent efforts around Open GLAM and the broader issues of Open Access to cultural heritage, and in particular will highlight many current activities around Open GLAM, as well as issues and challenges that exist in this area.</p>
","Over the past twenty plus years, cultural heritage organizations have put vast amounts of digitized available online.The movement towards Open Access for digitized cultural heritage has come to be known as Open GLAM (Galleries, Libraries, Archives, and Museums). This presentation will explore key issues and recent efforts around Open GLAM and the broader issues of Open Access to cultural heritage, and in particular will highlight many current activities around Open GLAM, as well as issues and challenges that exist in this area.",roger.gillis@dal.ca,Short Presentation
"Aboelnagah, Hadeer","Prince Sultan University, Saudi Arabia",Building Online Communities as a Platform for Collaborative Learning and Cross-Cultural Self- Expression; Saudi Female Students’ Blog Hajj Behind the Scenes as an Example,"Blogging, Collaborative Creativity, Sharing Knowledge, Open Access","Asia
Global
English
Contemporary
curricular and pedagogical development and analysis
public humanities collaborations and methods
Cultural studies
Education/ pedagogy",English,"Asia
Global",Contemporary,"curricular and pedagogical development and analysis
public humanities collaborations and methods","Cultural studies
Education/ pedagogy","<p>Digital Humanities as an emerging field provides endless opportunities to paradigm shifts in the educational experience in higher education, it opens wide doors for interdisciplinary collaborative students’ projects that can be the seed of larger-scale national and international. Blogging is a pedagogical activity that is used to enhance collaborative learning and social responsibility (Yu-Chun Kuo 2017). Hajj Behind the Scenes is a blog by Saudi Female Students. The study explores blogging as an activity and its possible utilization to build cross-disciplinary online communities and to enhance societal involvement in the field of Digital Humanities. The said blog is used as an example to study its effect on the students and their abilities to self-expression, autonomous learning, and collaborative creativity.</p>
","Digital Humanities as an emerging field provides endless opportunities to paradigm shifts in the educational experience in higher education, it opens wide doors for interdisciplinary collaborative students’ projects that can be the seed of larger-scale national and international. Blogging is a pedagogical activity that is used to enhance collaborative learning and social responsibility (Yu-Chun Kuo 2017). Hajj Behind the Scenes is a blog by Saudi Female Students. The study explores blogging as an activity and its possible utilization to build cross-disciplinary online communities and to enhance societal involvement in the field of Digital Humanities. The said blog is used as an example to study its effect on the students and their abilities to self-expression, autonomous learning, and collaborative creativity.",habouelnagah@psu.edu.sa,Lightning
"Herrmann, J. Berenike;
Messerli, Thomas C.","University of Basel, Switzerland",Metaphors we read by: Finding metaphorical conceptualizations of reading in web 2.0 book reviews,"metaphor, social reading, cultural analytics, metaphor identification","Comparative (2 or more geographical areas)
Europe
English
Contemporary
cultural analytics
semantic analysis
Cultural studies
Literary studies",English,"Comparative (2 or more geographical areas)
Europe",Contemporary,"cultural analytics
semantic analysis","Cultural studies
Literary studies","<p dir=""ltr"">While there is abundant interdisciplinary research on metaphor (Eggs, 2000; Semino & Demjén, 2017; Veale, Shutova,& Klebanov, 2016), so far only few corresponding studies have been conducted within DH. In our contribution, we investigate metaphors in reading experiences of literary texts by lay reviewers. Taking a deliberately simple methodological approach that operates on seed words for conceptual sources and for targets, we present an exploratory quantitative metaphor analysis of a corpus of German language lay book reviews.</p>
<p dir=""ltr"">We explore a corpus of approx. 439 mill. words (1.3 mill. book reviews) for metaphors used to conceptualize the target domain READING EXPERIENCE.</p>
<p dir=""ltr"">Starting from findings on English (Stockwell, 2009; Nuttall & Harrison, 2018) and   German reviews (Köhler, 1999), we analyse metaphor patterns in social reading networks, with a particular focus on the mapping READING EXPERIENCE IS MOTION. The main aim at this stage is to draw up a first typology of mappings.</p>
","While there is abundant interdisciplinary research on metaphor (Eggs, 2000; Semino & Demjén, 2017; Veale, Shutova,& Klebanov, 2016), so far only few corresponding studies have been conducted within DH. In our contribution, we investigate metaphors in reading experiences of literary texts by lay reviewers. Taking a deliberately simple methodological approach that operates on seed words for conceptual sources and for targets, we present an exploratory quantitative metaphor analysis of a corpus of German language lay book reviews.
We explore a corpus of approx. 439 mill. words (1.3 mill. book reviews) for metaphors used to conceptualize the target domain READING EXPERIENCE.
Starting from findings on English (Stockwell, 2009; Nuttall & Harrison, 2018) and   German reviews (Köhler, 1999), we analyse metaphor patterns in social reading networks, with a particular focus on the mapping READING EXPERIENCE IS MOTION. The main aim at this stage is to draw up a first typology of mappings.","berenike.herrmann@unibas.ch, thomas.messerli@unibas.ch",Long Presentation
"Herrmann, J. Berenike (1);
Odebrecht, Carolin (2);
Santos, Diana (3);
Francois, Pieter (4)","1: University of Basel, Switzerland;
2: Humboldt-University Berlin, Germany;
3: University of Oslo, Norway;
4: University of Oxford, UK",Towards Modeling the European Novel. Introducing ELTeC for Multilingual and Pluricultural Distant Reading,"corpus, TEI, literary modeling, comparative literary studies, 19th Century","Comparative (2 or more geographical areas)
Europe
English
19th Century
data modeling
database creation, management, and analysis
Humanities computing
Literary studies",English,"Comparative (2 or more geographical areas)
Europe",19th Century,"data modeling
database creation, management, and analysis","Humanities computing
Literary studies","<p dir=""ltr"">This contribution reports on the collaborative effort of building an open access multilingual corpus of European novels of the 19th century (the European Literary Text Collection - ELTeC) within the COST Action “Distant Reading”. </p>
<p dir=""ltr"">We adopt a metadata-based approach that allows for representing the diversity of novels published 1840-1920 across the multilingual and pluri-cultural topologies of Europe. Our sampling and balancing criteria use metadata including publication date, text length, reprint counts and authors’ gender, and we deliberately focus on inclusion of non-canonical novels.</p>
<p>We have built a workflow for systematically sampling and encoding novels as well as a consistent annotation model of data and metadata (cf. Burnard, Schöch, Odebrecht, 2019). Currently, ELTeC constitutes a dynamic intersection of fictional discourse in fourteen languages, including Czech, English, French, German, Greek, Hungarian, Italian, Norwegian, Polish, Portuguese, Romanian, Serbian, Slovenian, and Spanish (ca. 600 text candidates amounting to ca. 52 mio. words).</p>
","This contribution reports on the collaborative effort of building an open access multilingual corpus of European novels of the 19th century (the European Literary Text Collection - ELTeC) within the COST Action “Distant Reading”. 
We adopt a metadata-based approach that allows for representing the diversity of novels published 1840-1920 across the multilingual and pluri-cultural topologies of Europe. Our sampling and balancing criteria use metadata including publication date, text length, reprint counts and authors’ gender, and we deliberately focus on inclusion of non-canonical novels.
We have built a workflow for systematically sampling and encoding novels as well as a consistent annotation model of data and metadata (cf. Burnard, Schöch, Odebrecht, 2019). Currently, ELTeC constitutes a dynamic intersection of fictional discourse in fourteen languages, including Czech, English, French, German, Greek, Hungarian, Italian, Norwegian, Polish, Portuguese, Romanian, Serbian, Slovenian, and Spanish (ca. 600 text candidates amounting to ca. 52 mio. words).","berenike.herrmann@unibas.ch, carolin.odebrecht@hu-berlin.de, d.s.m.santos@ilos.uio.no, pwfrancois79@gmail.com",Short Presentation
"Arbuckle, Alyssa;
Siemens, Ray","University of Victoria, Canada","Open, Digital Scholarship: Issues, Initiatives, and Research Commons in the Humanities and Social Sciences ","open scholarship, open access, research commons, digital scholarship","English
North America
Contemporary
open access methods
public humanities collaborations and methods
Communication studies",English,North America,Contemporary,"open access methods
public humanities collaborations and methods",Communication studies,"<p>In the spirit of the public humanities, initiatives are emerging that foster the open sharing, re-purposing, and development of scholarly projects, publications, educational resources, data, and tools. One example is the Canadian Humanities and Social Sciences (HSS) Commons, an in-development prototype for a national-scale, online research commons that features:</p>
<ul><li><strong>subject repository</strong> for open access publications with digital object identifiers (DOIs) upon upload and FAIR (Findable, Accessible, Interactive, Reusable) guidelines for data management</li>
<li><strong>project development environment</strong> integrated with Google Drive, Github, or Dropbox</li>
<li><strong>individual user profiles</strong> with federated login/identity authorization, including ORCID</li>
<li><strong>blogging</strong> capabilities</li>
<li><strong>subject interest groups</strong> and <strong>member interactions</strong> (e.g., profile building, messaging)</li>
</ul><p>The workshop includes:</p>
<ul><li>talks by key players in the Canadian digital research infrastructure world;</li>
<li>a theoretical overview of the commons in research development and community building;</li>
<li>instruction in how to set up a profile, upload publications, form a special interest group, and begin a digital scholarship project in the prototype Canadian HSS Commons.</li>
</ul><p>This initiative builds on consultations among Implementing New Knowledge Environments (INKE) Partnership members—in particular the Canadian Social Knowledge Institute, Compute Canada, and the Federation for the HSS—and has roots in similar initiatives such as Humanities Commons, a partner in this work.</p>
<p>This platform is intended to serve the unique needs of the Canadian HSS community. Based on early-stage consultations we anticipate that it will draw a significant number of DH practitioners. This prototype is built on HUBZero—an open source content management system available for deployment within other international contexts as well.</p>
","In the spirit of the public humanities, initiatives are emerging that foster the open sharing, re-purposing, and development of scholarly projects, publications, educational resources, data, and tools. One example is the Canadian Humanities and Social Sciences (HSS) Commons, an in-development prototype for a national-scale, online research commons that features:
subject repository for open access publications with digital object identifiers (DOIs) upon upload and FAIR (Findable, Accessible, Interactive, Reusable) guidelines for data management
project development environment integrated with Google Drive, Github, or Dropbox
individual user profiles with federated login/identity authorization, including ORCID
blogging capabilities
subject interest groups and member interactions (e.g., profile building, messaging)
The workshop includes:
talks by key players in the Canadian digital research infrastructure world;
a theoretical overview of the commons in research development and community building;
instruction in how to set up a profile, upload publications, form a special interest group, and begin a digital scholarship project in the prototype Canadian HSS Commons.
This initiative builds on consultations among Implementing New Knowledge Environments (INKE) Partnership members—in particular the Canadian Social Knowledge Institute, Compute Canada, and the Federation for the HSS—and has roots in similar initiatives such as Humanities Commons, a partner in this work.
This platform is intended to serve the unique needs of the Canadian HSS community. Based on early-stage consultations we anticipate that it will draw a significant number of DH practitioners. This prototype is built on HUBZero—an open source content management system available for deployment within other international contexts as well.","alyssaa@uvic.ca, siemens@uvic.ca",Workshop/Tutorial 4
"Hyman, Christy Lynn (1);
Crandell, Alli (2);
Bergeron, Sue (3);
Rouse, Jesse (4);
Lynch, Shane (5);
Moore-Pewu, Jamila (6);
Carter, Bryan (7);
Green, Hilary (8)","1: University of Nebraska Lincoln, United States of America;
2: Coastal Carolina University, United States of America;
3: Coastal Carolina University, United States of America;
4: University of North Carolina-Pembroke, United States of America;
5: University of Kansas, United States of America;
6: California State University-Fullerton, United States of America;
7: University of Arizona, United States of America;
8: University of Alabama, United States of America",Constructing Spatial Narratives: Considerations and Practices Across Communities,"digital mapping, public history, critical GIS, social justice, recovery","Comparative (2 or more geographical areas)
English
5th-14th Century
19th Century
Contemporary
public humanities collaborations and methods
spatial & spatio-temporal analysis, modeling and visualization
Geography and geo-humanities
History",English,Comparative (2 or more geographical areas),"5th-14th Century
19th Century
Contemporary","public humanities collaborations and methods
spatial & spatio-temporal analysis, modeling and visualization","Geography and geo-humanities
History","<p dir=""ltr"">This panel considers how critical, innovative approaches with GIS draw our attention to new pathways of digital mapping. How can the practice of digital mapping within a critical lens produce new cartographies for spaces of possibility? How can spatial narratives restore the tarnished lineages of cultural geographies obscured from history?</p>
<p dir=""ltr"">We want to engage with these questions in our panel. Our hope is to highlight the ethical responsibilities of critically engaged mapping projects. The assorted projects that we propose to present on in this panel are connected through a theoretical grounding that harnesses the power of imagining cultural recovery of landscape as a means of redress from historical obscurity. Ultimately our panel seeks to challenge the matrix of colonial epistemic power underlying traditional foundations of how we construct spatial constructions of communities.</p>
","This panel considers how critical, innovative approaches with GIS draw our attention to new pathways of digital mapping. How can the practice of digital mapping within a critical lens produce new cartographies for spaces of possibility? How can spatial narratives restore the tarnished lineages of cultural geographies obscured from history?
We want to engage with these questions in our panel. Our hope is to highlight the ethical responsibilities of critically engaged mapping projects. The assorted projects that we propose to present on in this panel are connected through a theoretical grounding that harnesses the power of imagining cultural recovery of landscape as a means of redress from historical obscurity. Ultimately our panel seeks to challenge the matrix of colonial epistemic power underlying traditional foundations of how we construct spatial constructions of communities.","christy@huskers.unl.edu, allicrandell@gmail.com, sbergero@coastal.edu, jesse.rouse@uncp.edu, shane_lynch@ku.edu, jmoorepewu@fullerton.edu, bryancarter@email.arizona.edu, hngreen1@ua.edu",Panel
"Barnett, Tully (1);
Cytron, Megan (2);
Gairola, Rahul (3);
Sumner, Tyne Daile (4)","1: Flinders University, Australia;
2: Universidad Complutense de Madrid;
3: Murdoch University/ Asia Research Centre;
4: University of Melbourne",Global Renderings in the Queer Digital Humanities,"queer, intersectional, textual, margins, surveillance","Europe
English
North America
Australia/Oceania
20th Century
Contemporary
digitization (2D & 3D)
text mining and analysis
Gender and sexuality studies
Literary studies",English,"Europe
North America
Australia/Oceania","20th Century
Contemporary","digitization (2D & 3D)
text mining and analysis","Gender and sexuality studies
Literary studies","<p>How might DH textual scholars respond to call to queer DH practices, methodologies and projects? This panel brings together four papers investigating the queer margins of digital textual or methodological culture. Barnett considers the queer politics of metadata in digitisation projects identifying the way item-level markings become work-level interpretive interventions. Cytron deploys close and distant reading to position Eduardo Mendicutti’s 1982 radical act of queer world-building in a global context through analytical stylometry and TEI. Sumner considers the queer implications of distant reading techniques on the FBI files on Langston Hughes and James Baldwin, which conflate homosexuality, race, and poetics by identifying all three as “perversity.” Gairola deploys queer of color critique as a lens to read the intersectional nexus of queer and postcolonial theory through analyzing machine learning’s supposed ability to detect “gayface” with facial recognition software. Together, these papers identify and press upon diverse margins in global queer digital inscriptions.  </p>
","How might DH textual scholars respond to call to queer DH practices, methodologies and projects? This panel brings together four papers investigating the queer margins of digital textual or methodological culture. Barnett considers the queer politics of metadata in digitisation projects identifying the way item-level markings become work-level interpretive interventions. Cytron deploys close and distant reading to position Eduardo Mendicutti’s 1982 radical act of queer world-building in a global context through analytical stylometry and TEI. Sumner considers the queer implications of distant reading techniques on the FBI files on Langston Hughes and James Baldwin, which conflate homosexuality, race, and poetics by identifying all three as “perversity.” Gairola deploys queer of color critique as a lens to read the intersectional nexus of queer and postcolonial theory through analyzing machine learning’s supposed ability to detect “gayface” with facial recognition software. Together, these papers identify and press upon diverse margins in global queer digital inscriptions.","tully.barnett@flinders.edu.au, mcytron@ucm.es, Rahul.Gairola@murdoch.edu.au, tdsumner@unimelb.edu.au",Panel
"Galleron, Ioana (1);
Patras, Roxana (2);
Gradinaru, Camelia (2)","1: Université Sorbonne-Nouvelle, Paris, France;
2: Universitatea ""Al. I. Cuza"", Iasi, Romania",Annotating spatial entities in Romanian Novels,"spatial entities, Hajdouk novels, manual annotation","Europe
English
19th Century
spatial & spatio-temporal analysis, modeling and visualization
text encoding and markup language creation, deployment, and analysis
Literary studies
Central/Eastern European Studies",English,Europe,19th Century,"spatial & spatio-temporal analysis, modeling and visualization
text encoding and markup language creation, deployment, and analysis","Literary studies
Central/Eastern European Studies","<p>This paper is based on HAIRO, a Franco-Romanian project for creating a library of Romanian Hajdouk novels in an XML/TEI format (see https://proiectulbrancusihairo.wordpress.com/home-1/). Hajdouks were outlaws living in the woods, that fascinated the public in the second half of the 19<sup>th</sup> century and at the beginning of the 20<sup>th</sup> century, both for their cruelty and their sense of justice. Between 1840 and 1920, they appear in almost 12% of the Romanian novels, with at least 40 titles specifically dedicated to this picturesque character.</p>
<p>Our main concern is “the place-making mediated by the text”, and more precisely the creation of a Hajdouk space; in a rural Romania, structured by clear and stable relationships between spaces, their nomadic way of life constitutes a disrupting force, and we are looking at if and how this reflects in the novels. Much along the lines of (Hay and Butterworth 2019), our work focuses less on the “indexical relationship to the physical world”, and more on the ways in which the texts create their own spatiality.</p>
<p>In the first part, we discuss the adaptation of Pustejovsky’s ISO metamodel (2014, 2019) to operate what we call a “basic annotation” of our set of novels. Faced with the specificities of our texts, we have defined not two, but seven types of spatial entities: toponyms, places, paths, zones, vehicles, topical spaces and potential spaces. The two last categories are the most salient difference between our annotation schema and the previous existing ones, and we advocate their interest in literary contexts, where “the other world” or “in his bosom” are frequently mentioned, to quote but two examples from a very rich list.</p>
<p>We further characterize the spaces as “absolute” or “relative”. For this “basic annotation”, we have renounced to define other types of relations, such as orientation, movement or metrics.</p>
<p>The annotation exercise took place in two phases. In a first, exploratory round, we have worked on XML files, and implemented our schema as a feature structure in TEI. In a second round, we have configured a BRAT server and started by measuring the inter-annotator agreement on a set of 10 samples of about 1000 words (see results in Galleron et al., forthcoming). In a third phase, currently under development, we proceed to the actual annotation of texts, using a place names dictionary to pre-annotate. Another path currently explored is that of the syntactic tagging of phrase constituents: since a large part of our space entities appear to assume a function of circumstantial complement of place, they could be spotted with a specialized dependencies tagger. However, the first experiences in this respect are quite disappointing, and all the more so they have been conducted on samples in French – results will probably be worse on Romanian samples, since Romanian is a language less equipped with NLP tools. Please note that usual NER systems (Stanford, Spacy library, etc.) do not work, or give very poor results, on Romanian texts. For all these reasons, manual annotation still appears as the best way to go, in spite of being extremely time consuming.</p>
<p>To date, the repartition of the annotations per type, as indicated in figure 1, confirms that looking at toponyms only, with a NER/ NEL approach, fails to capture a large part of the placemaking process in a novel. Also, two major categories of novels seem to appear with regards to the writing of the space, one constituted by the texts in which places and zones are in even proportions, the other gathering novels in which places are dominant, to the detriment of zones.</p>
<p align=""center"">Figure 1. Annotations per type in a selection of novels</p>
<p align=""center""></p>
<p>In addition, categories “paths” and “vehicles” seem to be discriminant between two other types of fiction. Indeed, while the number of annotations remains quite low in both cases, they allow to identify certain novels as outliers, with lots of spatial changes, as opposed to the major part of novels that appear finally more “static”, and privileging scenes and summaries of the action. This is somewhat surprising, since we expected all our Hajdouk novels to pertain to the second category. We are currently trying to understand if the difference is motivated by the specific style of certain authors, the taste of an era, or it genuinely points towards a generic specificity within our corpus;</p>
<p><strong>Bibliography</strong></p>
<p>Bodenhamer, David J.; Corrigan, John; Harris, Trevor M., <em>Deep Maps and Spatial Narratives</em>, Bloomington & Indianapolis, Indiana University Press, 2015.</p>
<p>Bodenhamer, David J.; Corrigan, John; Harris, Trevor M., <em>The Spatial Humanities: GIS and the Future of Humanities Scholarship,</em> Bloomington: Indiana University Press, 2010, p. 109-123, muse.jhu.edu/book/26710.</p>
<p><em>Dicționarul cronologic al romanului românesc</em>, București, Editura Academiei Române, 2003.</p>
<p>Galleron, Ioana, Patras, Roxana, Gradinaru, Camelia, Mélanie-Bécquet, Frédérique « À la recherche des haïdouks. L’annotation des entités spatiales dans un corpus de romans roumains du XIX<sup>e</sup> siècle », submitted to <em>Revue des humanités numériques Humanistica</em>, 2020.</p>
<p>Hay, Duncan, Butterworth, Alex (2019) “Spatial allusion, temporal recurrence and cognitive uncertainty: visualising chronotopic structure in a literary text”, <em>DH2019 Book of abstracts</em>.</p>
<p>Pustejovsky, James, Lee, Kiyong, Bunt, Harry (2019) “The Semantics of ISO-Space”, ISO committee draft for the revision of ISO (2014), https://let.uvt.nl/general/people/bunt/docs/PustejovskyLeeBunt_ISO-Space_ISA-15.pdf</p>
","This paper is based on HAIRO, a Franco-Romanian project for creating a library of Romanian Hajdouk novels in an XML/TEI format (see https://proiectulbrancusihairo.wordpress.com/home-1/). Hajdouks were outlaws living in the woods, that fascinated the public in the second half of the 19th century and at the beginning of the 20th century, both for their cruelty and their sense of justice. Between 1840 and 1920, they appear in almost 12% of the Romanian novels, with at least 40 titles specifically dedicated to this picturesque character.
Our main concern is “the place-making mediated by the text”, and more precisely the creation of a Hajdouk space; in a rural Romania, structured by clear and stable relationships between spaces, their nomadic way of life constitutes a disrupting force, and we are looking at if and how this reflects in the novels. Much along the lines of (Hay and Butterworth 2019), our work focuses less on the “indexical relationship to the physical world”, and more on the ways in which the texts create their own spatiality.
In the first part, we discuss the adaptation of Pustejovsky’s ISO metamodel (2014, 2019) to operate what we call a “basic annotation” of our set of novels. Faced with the specificities of our texts, we have defined not two, but seven types of spatial entities: toponyms, places, paths, zones, vehicles, topical spaces and potential spaces. The two last categories are the most salient difference between our annotation schema and the previous existing ones, and we advocate their interest in literary contexts, where “the other world” or “in his bosom” are frequently mentioned, to quote but two examples from a very rich list.
We further characterize the spaces as “absolute” or “relative”. For this “basic annotation”, we have renounced to define other types of relations, such as orientation, movement or metrics.
The annotation exercise took place in two phases. In a first, exploratory round, we have worked on XML files, and implemented our schema as a feature structure in TEI. In a second round, we have configured a BRAT server and started by measuring the inter-annotator agreement on a set of 10 samples of about 1000 words (see results in Galleron et al., forthcoming). In a third phase, currently under development, we proceed to the actual annotation of texts, using a place names dictionary to pre-annotate. Another path currently explored is that of the syntactic tagging of phrase constituents: since a large part of our space entities appear to assume a function of circumstantial complement of place, they could be spotted with a specialized dependencies tagger. However, the first experiences in this respect are quite disappointing, and all the more so they have been conducted on samples in French – results will probably be worse on Romanian samples, since Romanian is a language less equipped with NLP tools. Please note that usual NER systems (Stanford, Spacy library, etc.) do not work, or give very poor results, on Romanian texts. For all these reasons, manual annotation still appears as the best way to go, in spite of being extremely time consuming.
To date, the repartition of the annotations per type, as indicated in figure 1, confirms that looking at toponyms only, with a NER/ NEL approach, fails to capture a large part of the placemaking process in a novel. Also, two major categories of novels seem to appear with regards to the writing of the space, one constituted by the texts in which places and zones are in even proportions, the other gathering novels in which places are dominant, to the detriment of zones.
Figure 1. Annotations per type in a selection of novels
In addition, categories “paths” and “vehicles” seem to be discriminant between two other types of fiction. Indeed, while the number of annotations remains quite low in both cases, they allow to identify certain novels as outliers, with lots of spatial changes, as opposed to the major part of novels that appear finally more “static”, and privileging scenes and summaries of the action. This is somewhat surprising, since we expected all our Hajdouk novels to pertain to the second category. We are currently trying to understand if the difference is motivated by the specific style of certain authors, the taste of an era, or it genuinely points towards a generic specificity within our corpus;
Bibliography
Bodenhamer, David J.; Corrigan, John; Harris, Trevor M., Deep Maps and Spatial Narratives, Bloomington & Indianapolis, Indiana University Press, 2015.
Bodenhamer, David J.; Corrigan, John; Harris, Trevor M., The Spatial Humanities: GIS and the Future of Humanities Scholarship, Bloomington: Indiana University Press, 2010, p. 109-123, muse.jhu.edu/book/26710.
Dicționarul cronologic al romanului românesc, București, Editura Academiei Române, 2003.
Galleron, Ioana, Patras, Roxana, Gradinaru, Camelia, Mélanie-Bécquet, Frédérique « À la recherche des haïdouks. L’annotation des entités spatiales dans un corpus de romans roumains du XIXe siècle », submitted to Revue des humanités numériques Humanistica, 2020.
Hay, Duncan, Butterworth, Alex (2019) “Spatial allusion, temporal recurrence and cognitive uncertainty: visualising chronotopic structure in a literary text”, DH2019 Book of abstracts.
Pustejovsky, James, Lee, Kiyong, Bunt, Harry (2019) “The Semantics of ISO-Space”, ISO committee draft for the revision of ISO (2014), https://let.uvt.nl/general/people/bunt/docs/PustejovskyLeeBunt_ISO-Space_ISA-15.pdf","ioana.galleron@sorbonne-nouvelle.fr, roxana.patras@uaic.ro, cameliagradinaru2013@gmail.com",Lightning
"Wermer-Colan, Henry Alexander (1);
Mulligan, Rikk (2)","1: Temple University, United States of America;
2: Carnegie Mellon University, United States of America",Prototyping the SF Nexus: Collaborative Models for Digitizing and Curating Speculative Fiction Collections as Data,"Science Fiction, Digital Collections, Cultural Analytics","Europe
English
North America
19th Century
20th Century
Contemporary
cultural analytics
digital libraries creation, management, and analysis
Book and print history
Literary studies",English,"Europe
North America","19th Century
20th Century
Contemporary","cultural analytics
digital libraries creation, management, and analysis","Book and print history
Literary studies","<p>This paper overviews the SF Nexus prototype, the developmental stage of a research project to digitize and curate available works of Anglo-American speculative fiction. This resource enables access to texts in the public domain and under copyright, from magazines to mass-market novels. We overview challenges confronting scholars of SF book history, including restricted access to and the uncatalogued materials in special collections, copyright barriers to sharing digital texts, and the lack of comprehensive indices to SF texts and their publication records. The presentation will present models for multi-institutional, collaborative mass digitization efforts to ingest into HathiTrust, share across research centers and libraries, and curate on the web with user-friendly, embeddable tools for the cultural analytics of books as image and text data. We conclude by proposing our coalition as a collaborative model for similar digitization and curation projects requiring standardized policies, legal agreements, and data curation workflows.</p>
","This paper overviews the SF Nexus prototype, the developmental stage of a research project to digitize and curate available works of Anglo-American speculative fiction. This resource enables access to texts in the public domain and under copyright, from magazines to mass-market novels. We overview challenges confronting scholars of SF book history, including restricted access to and the uncatalogued materials in special collections, copyright barriers to sharing digital texts, and the lack of comprehensive indices to SF texts and their publication records. The presentation will present models for multi-institutional, collaborative mass digitization efforts to ingest into HathiTrust, share across research centers and libraries, and curate on the web with user-friendly, embeddable tools for the cultural analytics of books as image and text data. We conclude by proposing our coalition as a collaborative model for similar digitization and curation projects requiring standardized policies, legal agreements, and data curation workflows.","alex.wermer-colan@temple.edu, rikk@cmu.edu",Long Presentation
"Blackwell, Christopher William;
Blackwell, William;
Norman, Max","Furman University, United States of America",Les Misérables & the CITE Architecture: A Publication and Toolkit,"Hugo, Citation, N-Gram, Scala, Paris","Europe
English
French
19th Century
scholarly editing and editions development, analysis, and methods
text mining and analysis
Humanities computing
Literary studies","English
French",Europe,19th Century,"scholarly editing and editions development, analysis, and methods
text mining and analysis","Humanities computing
Literary studies","<p>Our poster will present and link to a publication and toolkit for working with Victor Hugo’s <em>Les <em>Misérables</em></em>, in French and English, using the CITE Architecture. The published data will include CTS-Compliant texts in French and English, and programmatically derived versions of those texts: TEI-XML, HTML, stop-words removed (useful for Topic Modelling), lemmatized (stemmed) editions, vocabulary lists, contextualized concordance, and a web-based translation-alignment tool.</p>
<p>The deliverable is not only a very rich deluxe, bilingual edition of the novel, but the documented scripts used to take a CITE/CTS text and transform it for different presentations and analyses.</p>
","Our poster will present and link to a publication and toolkit for working with Victor Hugo’s Les Misérables, in French and English, using the CITE Architecture. The published data will include CTS-Compliant texts in French and English, and programmatically derived versions of those texts: TEI-XML, HTML, stop-words removed (useful for Topic Modelling), lemmatized (stemmed) editions, vocabulary lists, contextualized concordance, and a web-based translation-alignment tool.
The deliverable is not only a very rich deluxe, bilingual edition of the novel, but the documented scripts used to take a CITE/CTS text and transform it for different presentations and analyses.","cwblackwell@gmail.com, wblackwell98@gmail.com, max.norman@furman.edu",Poster
"Esteva, Maria (1);
Clement, Tanya (2);
Xu, Weijia (1);
Aaron, Choate (3);
Robbins Hopkins, Hannah (2)","1: Texas Advanced Computing Center;
2: Department of English;
3: UT Libraries",AI4AV (Artificial Intelligence for Audiovisual): Design and Evaluation of a Shared System for LAMs,"audiovisual, artificial intelligence, machine learning, professional values, shared infrastructure","South America
English
North America
Contemporary
artificial intelligence and machine learning
metadata standards, systems, and methods
Computer science
Library & information science",English,"South America
North America",Contemporary,"artificial intelligence and machine learning
metadata standards, systems, and methods","Computer science
Library & information science","<p>Audiovisual (AV) materials are predominant historical and scientific records of our times, and their numbers are increasing exponentially in collecting institutions. Tasked with preserving and making AV materials available, libraries, archives, and museums (LAMs), need to find efficient and scalable curation solutions. Using machine learning (ML) to generate metadata is promising, but to adopt such methods information professionals must overcome a host of technological and cultural challenges. We introduce the AI4AV project in which we are conducting research around the design and evaluation of a system (currently a prototype) that uses ML to translate audio to text as well as natural language processing to classify and describe AV materials within open computing infrastructure that can be shared by multiple LAMs. This presentation describes the testbed collection, the ML and NLP methods and computing resources, and the protocol to incorporate LAMs values in the design and evaluation of the system.</p>
","Audiovisual (AV) materials are predominant historical and scientific records of our times, and their numbers are increasing exponentially in collecting institutions. Tasked with preserving and making AV materials available, libraries, archives, and museums (LAMs), need to find efficient and scalable curation solutions. Using machine learning (ML) to generate metadata is promising, but to adopt such methods information professionals must overcome a host of technological and cultural challenges. We introduce the AI4AV project in which we are conducting research around the design and evaluation of a system (currently a prototype) that uses ML to translate audio to text as well as natural language processing to classify and describe AV materials within open computing infrastructure that can be shared by multiple LAMs. This presentation describes the testbed collection, the ML and NLP methods and computing resources, and the protocol to incorporate LAMs values in the design and evaluation of the system.","maria@tacc.utexas.edu, tclement@utexas.edu, xwj@tacc.utexas.edu, achoate@austin.utexas.edu, hnrobb@gmail.com",Short Presentation
"Presner, Todd (1);
Bonazzi, Anna (1);
Fan, Lizhou (1);
Toth, Gabor (2);
Deblinger, Rachel (1);
Shepard, David (1)","1: UCLA, United States of America;
2: University of Southern California, United States of America",Digital Humanities Methods for Analyzing Holocaust and Genocide Testimonies,"Holocaust and genocide studies, testimony, text analysis, visualization","Comparative (2 or more geographical areas)
Global
Europe
English
20th Century
data modeling
text mining and analysis
History
Literary studies",English,"Comparative (2 or more geographical areas)
Global
Europe",20th Century,"data modeling
text mining and analysis","History
Literary studies","<p>The purpose of this panel is to present a set of digital methods for analyzing Holocaust and genocide testimonies at scale. The corpus of testimonies (approx. 55,000) and metadata come primarily from the USC Shoah Foundation and include survivors of the Armenian Genocide (1914-23), the Nanjing Massacre (1937/38), the Holocaust (1939-45), and the Rwandan Genocide (1994). Panel members also work with early Holocaust testimonies recorded in Displaced Persons Camps by David Boder (1946) as well as testimonies of from Yale’s Fortunoff Archive. One of the central research questions concerns the genre of “testimony” itself and how computational analysis can help us track changes in narrative structure, form, and content, particularly in dialogical interviews. We are also interested in how testimonies can be textually mined to fill in “gaps” and “missing voices” through linguistic analysis, including code switching, speech patterns, changes in voice and emotional expressivity, and so forth.</p>
","The purpose of this panel is to present a set of digital methods for analyzing Holocaust and genocide testimonies at scale. The corpus of testimonies (approx. 55,000) and metadata come primarily from the USC Shoah Foundation and include survivors of the Armenian Genocide (1914-23), the Nanjing Massacre (1937/38), the Holocaust (1939-45), and the Rwandan Genocide (1994). Panel members also work with early Holocaust testimonies recorded in Displaced Persons Camps by David Boder (1946) as well as testimonies of from Yale’s Fortunoff Archive. One of the central research questions concerns the genre of “testimony” itself and how computational analysis can help us track changes in narrative structure, form, and content, particularly in dialogical interviews. We are also interested in how testimonies can be textually mined to fill in “gaps” and “missing voices” through linguistic analysis, including code switching, speech patterns, changes in voice and emotional expressivity, and so forth.","presner@ucla.edu, annabonazzi@g.ucla.edu, lizhou@ucla.edu, gabor.toth@maximilianeum.de, rdeblinger@library.ucla.edu, dave@humnet.ucla.edu",Panel
"Klimashevskaia, Anastasia (1);
Geiger, Bernhard C. (2);
Hagmüller, Martin (1);
Helic, Denis (3);
Fischer, Frank (4)","1: Signal Processing and Speech Communication Laboratory, Graz University of Technology, Graz, Austria;
2: Know-Center GmbH, Graz, Austria;
3: Institute of Interactive Systems and Data Science, Graz University of Technology, Graz;
4: Higher School of Economics, Moscow","""To be or not to be central"" - On the Stability of Network Centrality Measures in Shakespeare's ""Hamlet""",literary network analysis,"Europe
English
15th-17th Century
Contemporary
artificial intelligence and machine learning
network analysis and graphs theory and application
Informatics
Literary studies",English,Europe,"15th-17th Century
Contemporary","artificial intelligence and machine learning
network analysis and graphs theory and application","Informatics
Literary studies","<p><strong>Introduction</strong></p>
<p><strong></strong><br />Centrality measures derived from character networks can be used to detect the main characters in a play. For example, previous research has shown that characters with high network centrality typically perform the majority of speech acts and appear in most of the scenes (Fischer, Trilcke, Kittel, Milling, & Skorinkin, 2018). However, one can extract character networks from plays in various ways: Close reading may omit minor characters like attendants or servants, e.g., (Moretti, 2011), while distant reading (e.g., parsing an XML file) may include aggregate characters like “All”, “Both Lords”, or similar. Furthermore, the networks may display either implicit or explicit connections, depending on whether we connect characters because they appear in the same scene or because they are directly addressing each other, respectively. Thus, as adding more characters or connections to the network affects centralities and other network measures, the interpretation of both qualitative and quantitative aspects of characternetworks depends on the extraction method. <br />In this work we are concerned with the specific question whether details of the textual source and the extraction method, such as adding minor or aggregate characters, make the main characters less “central”. A negative answer to this question would provide us with a further evidence for the validity of automated literary network analysis.</p>

<p><strong>Approach</strong></p>
<p><strong></strong><br />We analyse six versions of the character network of Shakespeare’s “Hamlet”. All networks were extracted via close or distant reading from different XML or text sources (see Figure 1) and analysed with NetworkX (Python). For each network, we compute four different centrality measures (closeness, betweenness, degree, and eigenvector centrality). Subsequently, for each centrality measure, we rank the 26 characters common in all networks and compare character ranks in different networks by computing their Spearman rank correlation.</p>
<p>Figure 1. Basic statistics of the character networks</p>

<p><strong>Observations, Conclusion, and Outlook</strong></p>
<p>The networks including implicit connections are denser than those with only explicit connections (cf. Figure 1). This yields different centrality ranks including the most important characters (cf. Figure 2). For example, Horatio has many more implicit connections and connections to minor characters, which makes him the character with the highest degree centrality in the Haworth network. In the Moretti network, which contains only explicit connections, Hamlet has the highest degree centrality.</p>
<p>Figure 2. Degree centralities for the Haworth (implicit connections, distant reading) and Moretti (explicit connections, close reading) networks</p>
<p>Despite such individual differences, the groups of main characters derived from different networks exhibit relatively stable rankings, cf. (Fischer, Trilcke, Kittel, Milling, & Skorinkin, 2018). In contrast, rankings for minor characters tend to differ significantly (see Figure 3). Therefore, for detecting the group of main characters, the details of the network extraction method do not have a significant effect, at least in the datasets we consider. In future work we aim to validate the generality of this claim by considering larger corpora of dramatic plays.</p>
<p>Figure 3. Heatmaps depicting the rank correlation between closeness centralities derived from<br />different networks for all, the 10 most important, and the 10 least important characters</p>
<p>Finally, we outline some further observations about different centrality measures: In our datasets, degree centrality is the most robust, exhibiting high rank correlation for all considered sets of characters. In contrast, eigenvector centrality has the widest range of rank correlations suggesting its high sensitivity with respect to the network structure.</p>

<p><strong>Acknowledgements</strong></p>
<p><strong></strong><br />The authors gratefully acknowledge permissions to use material from Martin Grandjean and Roger Haworth. The work was funded by the HRSM project “KONDE – Kompetenznetzwerk Digitale Edition”. The work of Bernhard C. Geiger was partially funded by the Austrian Academy of Sciences within the go!digital Next Generation project “DiSpecs” (GDNG_2018-046_DiSpecs). The Know-Center is funded within the Austrian COMET Program – Competence Centers for Excellent Technologies – under die auspices of the Austrian Federal Ministry of Transport, Innovation and Technology, the Austrian Federal Ministry of Digital and Economic Affairs, and by the State of Styria. COMET is managed by the Austrian Research Promotion Agency FFG.</p>

<p><strong>References</strong></p>
<ul><li>Fischer, F., Trilcke, P., & Orekhov, B. (2019). Drama Corpora Project. Retrieved from github.com/dracor-org/shakedracor</li>
<li>Fischer, F., Trilcke, P., Kittel, C., Milling, C., & Skorinkin, D. (2018). To Catch a Protagonist: Quantitative Dominance Relations in German Language Drama (1730-1930). Digital Humanities 2018 Puentes-Bridges. Book of Abstracts, 193-201.</li>
<li>Grandjean, M. (2015). Network visualization: mapping Shakespeare’s tragedies. Retrieved from martingrandjean.ch/network-visualization-shakespeare/</li>
<li>Haworth, R. W. (2019). Who appears where matrices and reader's versions for Shakespeare plays. Retrieved from http://rhaworth.net/shakes/shaxref.php?csv=on&targ=on&dnl=on&p=menu#dummy</li>
<li>Milling, C., & Fischer, F. (2019). Easy Linavis. Retrieved from https://ezlinavis.dracor.org/</li>
<li>Moretti, F. (2011, March). Network Theory, Plot Analysis. New Left Review(68), pp. 80-102.</li>
</ul>","Introduction
Centrality measures derived from character networks can be used to detect the main characters in a play. For example, previous research has shown that characters with high network centrality typically perform the majority of speech acts and appear in most of the scenes (Fischer, Trilcke, Kittel, Milling, & Skorinkin, 2018). However, one can extract character networks from plays in various ways: Close reading may omit minor characters like attendants or servants, e.g., (Moretti, 2011), while distant reading (e.g., parsing an XML file) may include aggregate characters like “All”, “Both Lords”, or similar. Furthermore, the networks may display either implicit or explicit connections, depending on whether we connect characters because they appear in the same scene or because they are directly addressing each other, respectively. Thus, as adding more characters or connections to the network affects centralities and other network measures, the interpretation of both qualitative and quantitative aspects of characternetworks depends on the extraction method. 
In this work we are concerned with the specific question whether details of the textual source and the extraction method, such as adding minor or aggregate characters, make the main characters less “central”. A negative answer to this question would provide us with a further evidence for the validity of automated literary network analysis.
Approach
We analyse six versions of the character network of Shakespeare’s “Hamlet”. All networks were extracted via close or distant reading from different XML or text sources (see Figure 1) and analysed with NetworkX (Python). For each network, we compute four different centrality measures (closeness, betweenness, degree, and eigenvector centrality). Subsequently, for each centrality measure, we rank the 26 characters common in all networks and compare character ranks in different networks by computing their Spearman rank correlation.
Figure 1. Basic statistics of the character networks
Observations, Conclusion, and Outlook
The networks including implicit connections are denser than those with only explicit connections (cf. Figure 1). This yields different centrality ranks including the most important characters (cf. Figure 2). For example, Horatio has many more implicit connections and connections to minor characters, which makes him the character with the highest degree centrality in the Haworth network. In the Moretti network, which contains only explicit connections, Hamlet has the highest degree centrality.
Figure 2. Degree centralities for the Haworth (implicit connections, distant reading) and Moretti (explicit connections, close reading) networks
Despite such individual differences, the groups of main characters derived from different networks exhibit relatively stable rankings, cf. (Fischer, Trilcke, Kittel, Milling, & Skorinkin, 2018). In contrast, rankings for minor characters tend to differ significantly (see Figure 3). Therefore, for detecting the group of main characters, the details of the network extraction method do not have a significant effect, at least in the datasets we consider. In future work we aim to validate the generality of this claim by considering larger corpora of dramatic plays.
Figure 3. Heatmaps depicting the rank correlation between closeness centralities derived from
different networks for all, the 10 most important, and the 10 least important characters
Finally, we outline some further observations about different centrality measures: In our datasets, degree centrality is the most robust, exhibiting high rank correlation for all considered sets of characters. In contrast, eigenvector centrality has the widest range of rank correlations suggesting its high sensitivity with respect to the network structure.
Acknowledgements
The authors gratefully acknowledge permissions to use material from Martin Grandjean and Roger Haworth. The work was funded by the HRSM project “KONDE – Kompetenznetzwerk Digitale Edition”. The work of Bernhard C. Geiger was partially funded by the Austrian Academy of Sciences within the go!digital Next Generation project “DiSpecs” (GDNG_2018-046_DiSpecs). The Know-Center is funded within the Austrian COMET Program – Competence Centers for Excellent Technologies – under die auspices of the Austrian Federal Ministry of Transport, Innovation and Technology, the Austrian Federal Ministry of Digital and Economic Affairs, and by the State of Styria. COMET is managed by the Austrian Research Promotion Agency FFG.
References
Fischer, F., Trilcke, P., & Orekhov, B. (2019). Drama Corpora Project. Retrieved from github.com/dracor-org/shakedracor
Fischer, F., Trilcke, P., Kittel, C., Milling, C., & Skorinkin, D. (2018). To Catch a Protagonist: Quantitative Dominance Relations in German Language Drama (1730-1930). Digital Humanities 2018 Puentes-Bridges. Book of Abstracts, 193-201.
Grandjean, M. (2015). Network visualization: mapping Shakespeare’s tragedies. Retrieved from martingrandjean.ch/network-visualization-shakespeare/
Haworth, R. W. (2019). Who appears where matrices and reader's versions for Shakespeare plays. Retrieved from http://rhaworth.net/shakes/shaxref.php?csv=on&targ=on&dnl=on&p=menu#dummy
Milling, C., & Fischer, F. (2019). Easy Linavis. Retrieved from https://ezlinavis.dracor.org/
Moretti, F. (2011, March). Network Theory, Plot Analysis. New Left Review(68), pp. 80-102.","klimashevskaya.anastasia@gmail.com, geiger@ieee.org, hagmueller@tugraz.at, dhelic@tugraz.at, ffischer@hse.ru",Poster
"Ames, Sarah",National Library of Scotland,Developing the Data Foundry: the National Library of Scotland’s data-delivery platform,"libraries, data, open data, rights","Global
Europe
English
Contemporary
data publishing projects, systems, and methods
digital libraries creation, management, and analysis
Book and print history
Library & information science",English,"Global
Europe",Contemporary,"data publishing projects, systems, and methods
digital libraries creation, management, and analysis","Book and print history
Library & information science","<p>The Collections as Data movement has gained significant traction in recent years, with large-scale projects leading the way in shaping and advocating for best practice (Padilla et al 2019). These studies, along with the OpenGLAM movement, have encouraged cultural heritage organisations to make collections available in machine readable formats and to support computational research with the collections, enabling libraries to cast new light on collections and present them in new ways for digital humanities audiences.</p>
<p>However, while there have been a number of recent, essential studies around Collections as Data, as well as research into making collections available openly and the reasoning behind this (Pekel 2014, Terras 2015), there has been little to date from an institutional point of view about what is involved in opening up the collections in this way.</p>
<p>How can libraries open up collections to wider audiences? How do we turn collections into data? What challenges does this present, relating to rights, access, and data management? What ethical considerations are needed and how can libraries be transparent about decision-making processes as they generate increasing amounts of data, becoming 'producers' of their own collections?</p>
<p>This paper lifts the lid on the process of making data available in a national library context and considers the changes to existing activities, processes and outlook in releasing collections as data.</p>
<p>The National Library of Scotland launched the Data Foundry (https://data.nls.uk/) in September 2019. As part of the Library’s Digital Scholarship Service, the Data Foundry provides access to data collections including digitised collections; metadata; map and spatial data; and organisational data; with further collections such as web archive and audiovisual data planned for future release.</p>
<p>The Data Foundry is based on three core principles: open, transparent and practical (National Library of Scotland 2019 [1]). The platform was designed to be a clear, easy-to-use website, with tiered data downloads; clear rights information; and at-a-glance details contextualising the datasets.</p>
<p>Collections on the Data Foundry are published openly, in reusable formats, and the Library does not assert further copyright over the datasets it produces (National Library of Scotland 2019 [2]). Furthermore, with transparency a key principle, the Data Foundry provides information about data provenance and the reasons behind why and how certain items have been digitised and ‘turned into’ data above others (Ames 2019).</p>
<p>Producing the Data Foundry has been a Library-wide effort. Working at the intersection of collections, technology and research, the National Library of Scotland’s Digital Scholarship Service draws upon existing expertise across the Library – including Rights, Developers, Curators, Metadata – as well as working closely with researchers to understand their needs.</p>
<p>This paper will highlight the practical side of opening up library collections for digital humanities use, exploring the everyday challenges and obstacles such as rights and technical issues and changes to workflows required to produce collections as data, as well as the broader implications of making collections available at scale for libraries and their users.</p>
<p>References</p>
<p>Ames, Sarah. (2019). ‘Digital scholarship and data provenance at The National Library of Scotland’. <em>Libraries as Research Partner in Digital Humanities</em>. http://doi.org/10.5281/zenodo.3269291</p>
<p>National Library of Scotland [1] (2019). https://data.nls.uk/</p>
<p>National Library of Scotland [2] (2019). National Library of Scotland Open Data Publication Plan. https://data.nls.uk/download/national-library-of-scotland-open-data-publication-plan.pdf</p>
<p>OpenGLAM. (2019). ‘Ways forward to Open Access for cultural heritage’. <em>OpenGLAM</em>. https://openglam.org/2019/04/30/openglam-principles-ways-forward-to-open-access-for-cultural-heritage/</p>
<p>Padilla, Thomas, Allen, Laurie, Frost, Hannah, Potvin, Sarah, Russey Roke, Elizabeth, & Varner, Stewart. (2019). <em>Final Report --- Always Already Computational: Collections as Data (Version 1)</em>. http://doi.org/10.5281/zenodo.3152935</p>
<p>Padilla, Thomas, Scates Ketler, Hannah, Allen, Laurie, Varner, Stewart. (2019). Collections as Data: Part to Whole. https://collectionsasdata.github.io/part2whole/</p>
<p>Pekel, Joris. (2014). <em>Democratising the Rijksmuseum</em>. Europeana Foundation. https://pro.europeana.eu/files/Europeana_Professional/Publications/Democratising%20the%20Rijksmuseum.pdf</p>
<p>Terras, Melissa. (2015). ‘Opening Access to Collections: the Making and Using of Open Digitised Cultural Content’. <em>Online Information Review</em>. http://discovery.ucl.ac.uk/1469561/1/MelissaTerras_OpeningAccess_OIR.pdf</p>
","The Collections as Data movement has gained significant traction in recent years, with large-scale projects leading the way in shaping and advocating for best practice (Padilla et al 2019). These studies, along with the OpenGLAM movement, have encouraged cultural heritage organisations to make collections available in machine readable formats and to support computational research with the collections, enabling libraries to cast new light on collections and present them in new ways for digital humanities audiences.
However, while there have been a number of recent, essential studies around Collections as Data, as well as research into making collections available openly and the reasoning behind this (Pekel 2014, Terras 2015), there has been little to date from an institutional point of view about what is involved in opening up the collections in this way.
How can libraries open up collections to wider audiences? How do we turn collections into data? What challenges does this present, relating to rights, access, and data management? What ethical considerations are needed and how can libraries be transparent about decision-making processes as they generate increasing amounts of data, becoming 'producers' of their own collections?
This paper lifts the lid on the process of making data available in a national library context and considers the changes to existing activities, processes and outlook in releasing collections as data.
The National Library of Scotland launched the Data Foundry (https://data.nls.uk/) in September 2019. As part of the Library’s Digital Scholarship Service, the Data Foundry provides access to data collections including digitised collections; metadata; map and spatial data; and organisational data; with further collections such as web archive and audiovisual data planned for future release.
The Data Foundry is based on three core principles: open, transparent and practical (National Library of Scotland 2019 [1]). The platform was designed to be a clear, easy-to-use website, with tiered data downloads; clear rights information; and at-a-glance details contextualising the datasets.
Collections on the Data Foundry are published openly, in reusable formats, and the Library does not assert further copyright over the datasets it produces (National Library of Scotland 2019 [2]). Furthermore, with transparency a key principle, the Data Foundry provides information about data provenance and the reasons behind why and how certain items have been digitised and ‘turned into’ data above others (Ames 2019).
Producing the Data Foundry has been a Library-wide effort. Working at the intersection of collections, technology and research, the National Library of Scotland’s Digital Scholarship Service draws upon existing expertise across the Library – including Rights, Developers, Curators, Metadata – as well as working closely with researchers to understand their needs.
This paper will highlight the practical side of opening up library collections for digital humanities use, exploring the everyday challenges and obstacles such as rights and technical issues and changes to workflows required to produce collections as data, as well as the broader implications of making collections available at scale for libraries and their users.
References
Ames, Sarah. (2019). ‘Digital scholarship and data provenance at The National Library of Scotland’. Libraries as Research Partner in Digital Humanities. http://doi.org/10.5281/zenodo.3269291
National Library of Scotland [1] (2019). https://data.nls.uk/
National Library of Scotland [2] (2019). National Library of Scotland Open Data Publication Plan. https://data.nls.uk/download/national-library-of-scotland-open-data-publication-plan.pdf
OpenGLAM. (2019). ‘Ways forward to Open Access for cultural heritage’. OpenGLAM. https://openglam.org/2019/04/30/openglam-principles-ways-forward-to-open-access-for-cultural-heritage/
Padilla, Thomas, Allen, Laurie, Frost, Hannah, Potvin, Sarah, Russey Roke, Elizabeth, & Varner, Stewart. (2019). Final Report --- Always Already Computational: Collections as Data (Version 1). http://doi.org/10.5281/zenodo.3152935
Padilla, Thomas, Scates Ketler, Hannah, Allen, Laurie, Varner, Stewart. (2019). Collections as Data: Part to Whole. https://collectionsasdata.github.io/part2whole/
Pekel, Joris. (2014). Democratising the Rijksmuseum. Europeana Foundation. https://pro.europeana.eu/files/Europeana_Professional/Publications/Democratising%20the%20Rijksmuseum.pdf
Terras, Melissa. (2015). ‘Opening Access to Collections: the Making and Using of Open Digitised Cultural Content’. Online Information Review. http://discovery.ucl.ac.uk/1469561/1/MelissaTerras_OpeningAccess_OIR.pdf",sarah.ames@nls.uk,Short Presentation
"Nielbo, Kristoffer (1,2);
Vahlstrup, Peter Bjerregaard (1,2);
Gao, Jianbo (3,4);
Bechmann, Anja (2)","1: Center for Humanities Computing Aarhus, Aarhus University, Denmark;
2: DATALAB, Aarhus University, Denmark;
3: {Center for Geodata and Analysis, Faculty of Geographical Science, Beijing Normal University, China;
4: Institute of Automation, Chinese Academy of Sciences, China",Sociocultural trend signatures in minimal persistence and past novelty,"trend estimation, dynamical systems, fractal analysis, information theory, cultural analytics","Asia
Europe
English
North America
Contemporary
cultural analytics
information retrieval and querying algorithms and methods
Humanities computing
Media studies",English,"Asia
Europe
North America",Contemporary,"cultural analytics
information retrieval and querying algorithms and methods","Humanities computing
Media studies","<p><strong>[Full abstract is attached as 1st file as latex file with metadata and figures]</strong></p>
<p><strong>[Short summary]</strong></p>
<p>Sociocultural trends from social media platforms such as Twitter or Instagram have become an important part of knowledge discovery. The `trend' construct is however ambiguous and its estimation from unstructured sociocultural data complicated by several methodological issues. This paper presents an approach to trend estimation that combines (`intersects') domain knowledge of social media with advances in information theory and dynamical systems. In particular, we show how *trend reservoirs* (i.e., signals that display trend potential) can be identified by their relationship between novel and resonant behavior, and their minimal persistence.This approach contrasts with trend estimation that relies on linear or polynomial techniques to study point-like novelty behavior in social media, and it completes approaches that rely on smooth functions of time.</p>
","[Full abstract is attached as 1st file as latex file with metadata and figures]
[Short summary]
Sociocultural trends from social media platforms such as Twitter or Instagram have become an important part of knowledge discovery. The `trend' construct is however ambiguous and its estimation from unstructured sociocultural data complicated by several methodological issues. This paper presents an approach to trend estimation that combines (`intersects') domain knowledge of social media with advances in information theory and dynamical systems. In particular, we show how *trend reservoirs* (i.e., signals that display trend potential) can be identified by their relationship between novel and resonant behavior, and their minimal persistence.This approach contrasts with trend estimation that relies on linear or polynomial techniques to study point-like novelty behavior in social media, and it completes approaches that rely on smooth functions of time.","kln@cas.au.dk, imvpbv@cc.au.dk, jbgao.pmb@aliyun.com, anjabechmann@cc.au.dk",Long Presentation
"Hubert, Hadassah St. (1);
Rojas Castro, Antonio (2);
Kraft, Tobias (2);
Kraller, Kathrin (2);
Afanador-Llach, María José (3);
Levi, Amalia S. (4)","1: CLIR Postdoctoral Fellow-Digital Library of the Caribbean at Florida International University, USA;
2: Berlin-Brandenburgische Akademie der Wissenschaften (BBWA), Germany;
3: Universidad de los Andes, Colombia;
4: The HeritEdge Connection, Barbados",Compartir lo que nos une. Digitizing and Curating Colonial Records from the Caribbean and Central and South America for Public Outreach,"colonialism, Caribbean, slavery, digitization, curation","South America
Comparative (2 or more geographical areas)
Europe
English
Spanish
18th Century
19th Century
digital libraries creation, management, and analysis
History
Library & information science","English
Spanish","South America
Comparative (2 or more geographical areas)
Europe","18th Century
19th Century","digital libraries creation, management, and analysis","History
Library & information science","<p><strong><em>Compartir lo que nos une</em>. Digitizing and Curating Colonial Records from the Caribbean and Central and South America for Public Outreach</strong></p>
<p><strong>Overview</strong></p>
<p>When digitizing and curating the colonial past, digital objects are not simply surrogates, but also ways of seeing and interpreting the world: slavery, colonization, and transculturation are embedded in any document, and if interrogated, their digital representations can become tools for understanding the impact of imperialism and colonialism in the present.</p>
<p>Consequently, digitization and curation of nineteenth-century Caribbean and Central American collections give rise to new questions and challenges in the field of DH: How can we preserve endangered documents while keeping in mind their present-day needs & concerns? How can we digitize and give access to documents dispersed across international institutions to create meaningful collections around slavery? How can we curate data to imagine and visualize space beyond--often artificially imposed--geographical and temporal constraints? Or how can create digital records about oppressed people that transcend their oppressors?</p>
<p>This panel brings together a diverse group of researchers from the Caribbean, South America, Central America, and Europe in order to place digitization and curatorial practices within the analytic framework of cultural encounter described as “post-colonial computing” (Dourish, 2010: 91) and of the post-custodial institutional partnership model (Kelleher, 2017; Alpert-Abrams, Bliss, & Carbajal, 2019). While these encounters and partnerships enable new and exciting collaborations, their processes, epistemological conditions, and products should be critically examined in order not to replicate neo-colonial attitudes often ingrained in digital technologies, and instead support local communities and promote public outreach.</p>
<p>The panel addresses one of the main themes of the conference--“the historical and continued impacts of colonialism, postcolonialism, and hegemony”--and talks will be both in English (Hubert and Levi) and Spanish (Rojas Castro, Kraft and Kraller, and Afanador-Llach); presenters will provide translations to facilitate dialogue between participants and audience.</p>
<p><strong>Protecting Haitian Patrimony Initiatives at the Digital Library of the Caribbean (dLOC)</strong></p>
<p>Hadassah St. Hubert</p>
<p>CLIR Postdoctoral Fellow</p>
<p>Digital Library of the Caribbean at Florida International University hsthuber@fiu.edu</p>
<p>How can we protect and preserve endangered archives in Haiti? In 2010, Digital Library of the Caribbean (dLOC) launched the Protecting Haitian Patrimony Initiative, which builds on strong, existing long term partnerships, with an emphasis on accountability and transparency. This talk will focus on the numerous digitization projects, digital exhibits created since the launch of the initiative, and how dLOC engages the public.</p>
<p>dLOC built Haiti: An Island Luminous, a tri-lingual website to help readers learn about Haiti’s history. “Haiti: An Island Luminous” combines rare books, manuscripts, and photos scanned by archives and libraries in Haiti and the United States with commentary by over one hundred (100) authors from universities around the world. “Haiti: An Island Luminous” contextualizes hundreds of historical books, documents and photos digitally preserved by dLOC’s partners, including the National Archives of Haiti, the National Library of Haiti, Haitian Library of the Fathers of the Holy Spirit (Pères du St-Esprit), Haitian Library of the Brothers of Christian Instruction (Frères de l’Instruction Chrètienne), University of Florida, Brown University, and University of Central Florida.</p>
<p>dLOC currently hosts over 40,000 titles with more than four million pages of content, much of which is accessible content related to Haiti. Since the launch of “Haiti: An Island Luminous”, exhibit stations were placed in the Little Haiti Cultural Center, Nova Southeastern University’s Museum of Art, Sant La Haitian Neighborhood Center and more recently at North Miami Public Library in an effort to reach an even larger audience.</p>
<p>dLOC partners in Haiti have been able to contribute more digital content due to the efforts of many scholars that have collaborated to apply for Endangered Archive Programme grants, as well as support from the World Bank, the U.S. Embassy in Haiti, and the Haitian Studies Association. All of these efforts have made dLOC the largest open access repository of Caribbean content and a significant resource for finding materials from and about the Caribbean for use in teaching, research, cultural and community life.</p>
<p><strong>Reconstruyendo la huella de Humboldt en Cuba. Retos y oportunidades de la digitalización del patrimonio documental cubano-alemán del siglo XIX</strong></p>
<p>Antonio Rojas Castro, Tobias Kraft y Kathrin Kraller</p>
<p>Berlin-Brandenburgische Akademie der Wissenschaften</p>
<p>antonio.rojas-castro@bbaw.de</p>
<p>kraft@bbaw.de</p>
<p>kathrin.kraller@bbaw.de</p>
<p>Grisel Terrón, Eritk Guerra y Alaina Solernou</p>
<p>Oficina del Historiador de la Ciudad de La Habana</p>
<p>grisel@patrimonio.ohc.cu</p>
<p>eritk@dic.ohc.cu</p>
<p>alaina@patrimonio.ohc.cu</p>
<p>Con esta comunicación pretendemos debatir sobre los principales retos y oportunidades que supone la cooperación, en el marco del Proyecto Humboldt Digital (ProHD), entre la Academia de las Ciencias y las Humanidades de Berlín (BBAW) y la Casa Humboldt de la Oficina del Historiador de la Ciudad de La Habana (OHCH). En concreto, nos gustaría responder a la siguiente pregunta: ¿cómo podemos cooperar para el beneficio mutuo (Sennett, 2012)? A continuación, analizaremos el estado actual de la digitalización y las Humanidades Digitales en Cuba. Por último, expondremos los primeros avances orientados hacia la construcción de un repositorio digital y la edición de documentos.</p>
<p>El futuro repositorio digital de ProHD tiene por objetivo preservar y dar acceso en línea a varias colecciones de documentos de naturaleza transnacional y multilingüe (español, francés y alemán), que permitan reconstruir no solo la “huella” del científico prusiano, sino también el pensamiento de muchos intelectuales y políticos locales, como Francisco Arango y Parreño, y las condiciones en que se producía el negocio esclavista, por ejemplo, mediante la digitalización de facturas de compra y venta. De esta manera, al poner la figura de Alexander von Humboldt en su contexto histórico, los usuarios obtendrán una mirada nueva y completa al problema de la esclavitud y al funcionamiento del sistema colonial, que condenaba a Cuba al monocultivo y a importar bienes de primera necesidad procedentes de la Metrópolis.</p>
<p>A fin de obtener digitalizaciones y ediciones académicas digitales fáciles de encontrar, accesibles, interoperables y sostenibles, se han planeado tres medidas principales, que están implementándose desde junio de 2019: en primer lugar, se ha adquirido equipamiento para reforzar la infraestructura tecnológica (ordenadores, escáneres de alta resolución, servidor, impresora, mobiliario, etc.) de la Casa Humboldt en La Habana en donde se llevará a cabo la digitalización de los documentos; en segundo lugar, se ha formado a los miembros del equipo mediante cursos (en línea, sobre todo a partir del confinamiento provocado por la COVID-19) sobre el proceso de digitalización, la creación de metadatos y la gestión de repositorios digitales. Por último, se han definido de manera conjunta unos criterios de selección de documentos, una política de digitalización y un flujo de trabajo compartido.</p>
<p><strong>Inventar el virreinato de la Nueva Granada: curaduría crítica de fuentes primarias y la construcción de conjuntos de datos espaciales, 1739-1810</strong></p>
<p>María José Afanador-Llach</p>
<p>Universidad de los Andes, Colombia</p>
<p>mj.afanador28@uniandes.edu.co</p>
<p>La invención del virreinato del Nuevo Reino de Granada (hoy Venezuela, Colombia, Ecuador y Panamá) en 1739 respondió a un proyecto económico de la monarquía española para proteger el norte de Suramérica de incursiones extranjeras y extraer mayores recursos para la corona. Dicho proyecto de búsqueda de unidad administrativa y geográfica se nutrió de la producción de conocimiento sobre economía política, es decir, sobre cómo crear riqueza en contextos de competencia. En fuentes primarias como mapas, textos sobre economía y descripciones geográficas creados por burócratas, naturalistas y militares en la colonia, se evidencian las dificultades de integrar un territorio montañoso y extenso y la importancia de la imaginación geográfica en este proceso.</p>
<p>El proyecto de integración territorial del virreinato se rompió con la crisis monárquica de 1808 cuando el reino se fragmentó en más de una docena de provincias autónomas. La crisis representó una oportunidad para que pueblos y ciudades avanzaran sus proyectos económicos locales y se imaginaran espacios económicos post-coloniales. La pregunta central que guía este proyecto es: ¿cómo se puede visualizar la relación entre la búsqueda de unidad territorial en el norte de Suramérica y la diversidad de paisajes y economías políticas en tensión en una interfaz de archivo y un mapa digital?</p>
<p>Esta ponencia explora los avances de este proyecto en sus dos primeras fases. La primera fase consiste en la curaduría crítica de fuentes primarias entre 1739 y 1810. Esta curaduría consiste en la selección de documentos, de fragmentos dentro de los documentos y de mapas del periodo, para organizarlos, enriquecer sus metadatos y disponerlos alrededor de un argumento y una narrativa histórica. Algunos de estos documentos se encuentran disponibles en libros editados, otros están digitalizados y un número pequeño se digitalizará. La segunda fase consiste la formulación de un proceso metodológico para extraer datos sobre las dimensiones espaciales y de economía política de este conjunto curado de fuentes primarias. Se trata de generar conjuntos de datos relacionales que permitan visualizar, analizar e interpretar transformaciones históricas-espaciales de la invención y crisis del virreinato de la Nueva Granada y los actores involucrados en este proceso. El proyecto busca que tanto la curaduría de fuentes primarias como los conjuntos de datos sean recursos abiertos en español para la investigación y docencia sobre el periodo. La ponencia discutirá los retos a los que se ha enfrentado el proyecto hasta el momento en términos de la curaduría de documentos y la definición metodológica para la extracción de datos.</p>
<p><strong>Beyond digitization: Engaging the Community to Decolonize the Archival Record for the Enslaved</strong></p>
<p>Amalia S. Levi</p>
<p>Archivist, Chair</p>
<p>The HeritEdge Connection, Barbados</p>
<p>amalia@heritedge.foundation</p>
<p>During 2018 and 2019, two historic newspapers, <em>The</em> <em>Barbados Mercury Gazette</em> (1783-1848) and <em>The Barbadian</em> (1822-1861), housed in the Barbados Department of Archives, were digitized through Endangered Archives Programme grants. As primary sources, they offer a detailed view of every aspect of the dystopian life in a British colony in the Caribbean during the eighteenth and nineteenth centuries. Digitization offers unparalleled access to these colonial newspapers that were previously largely inaccessible due to their fragility.</p>
<p>The presentation goes beyond digitization to discuss what happens after digitization is completed. Considering that the newspapers were part of the colonial information apparatus, how do we read against the grain to locate and reveal marginalized voices hidden in the digitized pages?</p>
<p>If our aim is to “lift” the voices of the enslaved from the pages, and retrospectively create a body of archival records, we need to provide access to the digitized pages as data. Such information however exists “locked” in digitized images that due to the deterioration of the paper and the discoloration of the pages are impossible to OCR. Finding and transcribing these ads is necessarily a manual process, at present.</p>
<p>Among the wealth of information appearing regularly in the newspapers, of particular importance are the “runaway slaves” ads. Archives usually preserve very little descriptive information about the enslaved, because their lived experiences were rarely recorded. When they appear in the archival record, they are enumerated or appraised as commodities. Thus these ads offer a rich trove of information about individual people, including name, age, physical appearance, skin color, clothing, accent, distinguishing features (such as body modifications from their country of origin or bodily harm, the result of violence), friends, relatives, and skills.</p>
<p>Digitization does not solve the gaps, silences, and omissions inherent in the archival record and the colonial epistemologies they contain. Colonial newspapers reflect the voices of the white class of planters, merchants, and colonial authorities. Simply digitizing and putting online risks elevating and amplifying the very colonial worldviews we mean to avoid.</p>
<p>To decolonize the record about the enslaved, digitization is only the starting point for further projects and initiatives to engage the community with its own history. Recent scholarship and public humanities efforts have shown that it is possible to challenge the erasure of colonial archival sources and read between the lines to tease out information that is not readily visible.  </p>
<p>Our aim is to create a collection of material by clipping “runaway slaves” ads from the digitized newspapers, transcribing the text, and enriching the human stories in each ad with additional contextual information. More specifically, the first part of the presentation will focus on work to develop the “Barbados Runaway Slaves Digital Collection,” a partnership between the Barbados Archives, the local non-profit HeritEdge Connection, and the Early Caribbean Digital Archive (ECDA) at Northeastern University (Boston).</p>
<p>This digital collection aims to foreground the centrality of enslaved voices by ‘lifting’ ads of individuals who chose to escape slavery from the pages of the newspapers, and turning them into individual, standalone archival records previously unrecorded in the archives. When completed, the “Runaway Slaves Digital Collection” will provide a central location for collecting and presenting these ads, and other opportunities for the public, especially students, both in Barbados, as well as abroad, to interact with the material in creative ways.</p>
<p>By ‘reading’ these ads in various ways and being able to ‘see’ them collectively, we can start seeing patterns and coping mechanisms. At the same time, the availability of this information points to what is not there, and invites us to be sensitive to gaps and silences.</p>
<p>The second part of the presentation will discuss public outreach initiatives. Material digitized through generous grants by institutions in the Global North are eventually hosted in digital platforms in those institutions. While praiseworthy, digitization through such grants ends up benefiting scholars in the Global North. Usually local people are unaware of these platforms, and often unable to access them, either due to bandwidth issues or simply to interfaces that might not be intuitive to use. We have tried to remedy this by focusing on public history work we are doing to increase awareness of and engagement with these digitized colonial records.</p>
<p>During the fall of 2019, we conducted a series of workshops aimed to familiarize the public with accessing the newspapers online and to transcribe ads. The workshops also provided a platform for the public to discuss the ads, and the many facets of slavery. People were able to see ancestors who chose to resist and escape bondage in adverse, inhumane conditions. Due to COVID-19, workshops planned for 2020 are being held online. The aim of these workshops is to engage people with the ads in creative ways, through genealogical research, speculative writing, or digital methods. In this way, we invite the public to contextualize the ads through their local knowledge of places mentioned in the ads, or intimate information about lived experiences. Beyond information that is there, we also hope that workshop participants can imagine what is not there, complete the stories, and give enslaved individuals their place in the archival record.</p>
<p>Additionally, the digitization of these primary sources has great potential for digital projects by students and other researchers that can highlight various aspects of the island’s history. The “runaway slaves” ads help people challenge the customary narrative of Barbadian passivity and submissiveness to slavery; reconstruct family and community networks that supported enslaved Barbadians; and they help give voice to ancestors, whom colonial records intentionally left voiceless.</p>
<p><strong>Bibliographic References</strong></p>
<p>Alpert-Abrams, H. Bliss, D. A., and Carbajal I. “Post-Custodialism for the Collective Good: Examining Neoliberalism in US-Latin American Archival Partnerships”. In: “Evidences, Implications, and Critical Interrogations of Neoliberalism in Information Studies”, eds. Marika Cifor and Jamie A. Lee. Special issue, <em>Journal of Critical Library and Information Studies</em> 2.1 (2019). DOI: https://doi.org/10.24242/jclis.v2i1.87</p>
<p>Dourish, P. “‘Computational Thinking’ and the Postcolonial in the Teaching from Country Programme”. <em>Learning Communities. International Journal of Learning in Social Contexts</em>, 2 (2010): 91-101.</p>
<p>Kelleher, Ch. “Archives without Archives: (Re)Locating and (Re)Defining the Archive Through Post-Custodial Praxis”. In: “Critical Archival Studies”, eds. Michelle Caswell, Ricardo Punzalan, and T-Kay Sangwand. Special issue, <em>Journal of Critical Library and Information Studies</em>, 1.2 (2017). DOI: https://doi.org/10.24242/jclis.v1i2.29                </p>
<p>Sennett, Richard. <em>Juntos. Rituales, Placeres y políticas de la cooperación.</em> Barcelona: Anagrama, 2012.</p>
","Compartir lo que nos une. Digitizing and Curating Colonial Records from the Caribbean and Central and South America for Public Outreach
Overview
When digitizing and curating the colonial past, digital objects are not simply surrogates, but also ways of seeing and interpreting the world: slavery, colonization, and transculturation are embedded in any document, and if interrogated, their digital representations can become tools for understanding the impact of imperialism and colonialism in the present.
Consequently, digitization and curation of nineteenth-century Caribbean and Central American collections give rise to new questions and challenges in the field of DH: How can we preserve endangered documents while keeping in mind their present-day needs & concerns? How can we digitize and give access to documents dispersed across international institutions to create meaningful collections around slavery? How can we curate data to imagine and visualize space beyond--often artificially imposed--geographical and temporal constraints? Or how can create digital records about oppressed people that transcend their oppressors?
This panel brings together a diverse group of researchers from the Caribbean, South America, Central America, and Europe in order to place digitization and curatorial practices within the analytic framework of cultural encounter described as “post-colonial computing” (Dourish, 2010: 91) and of the post-custodial institutional partnership model (Kelleher, 2017; Alpert-Abrams, Bliss, & Carbajal, 2019). While these encounters and partnerships enable new and exciting collaborations, their processes, epistemological conditions, and products should be critically examined in order not to replicate neo-colonial attitudes often ingrained in digital technologies, and instead support local communities and promote public outreach.
The panel addresses one of the main themes of the conference--“the historical and continued impacts of colonialism, postcolonialism, and hegemony”--and talks will be both in English (Hubert and Levi) and Spanish (Rojas Castro, Kraft and Kraller, and Afanador-Llach); presenters will provide translations to facilitate dialogue between participants and audience.
Protecting Haitian Patrimony Initiatives at the Digital Library of the Caribbean (dLOC)
Hadassah St. Hubert
CLIR Postdoctoral Fellow
Digital Library of the Caribbean at Florida International University hsthuber@fiu.edu
How can we protect and preserve endangered archives in Haiti? In 2010, Digital Library of the Caribbean (dLOC) launched the Protecting Haitian Patrimony Initiative, which builds on strong, existing long term partnerships, with an emphasis on accountability and transparency. This talk will focus on the numerous digitization projects, digital exhibits created since the launch of the initiative, and how dLOC engages the public.
dLOC built Haiti: An Island Luminous, a tri-lingual website to help readers learn about Haiti’s history. “Haiti: An Island Luminous” combines rare books, manuscripts, and photos scanned by archives and libraries in Haiti and the United States with commentary by over one hundred (100) authors from universities around the world. “Haiti: An Island Luminous” contextualizes hundreds of historical books, documents and photos digitally preserved by dLOC’s partners, including the National Archives of Haiti, the National Library of Haiti, Haitian Library of the Fathers of the Holy Spirit (Pères du St-Esprit), Haitian Library of the Brothers of Christian Instruction (Frères de l’Instruction Chrètienne), University of Florida, Brown University, and University of Central Florida.
dLOC currently hosts over 40,000 titles with more than four million pages of content, much of which is accessible content related to Haiti. Since the launch of “Haiti: An Island Luminous”, exhibit stations were placed in the Little Haiti Cultural Center, Nova Southeastern University’s Museum of Art, Sant La Haitian Neighborhood Center and more recently at North Miami Public Library in an effort to reach an even larger audience.
dLOC partners in Haiti have been able to contribute more digital content due to the efforts of many scholars that have collaborated to apply for Endangered Archive Programme grants, as well as support from the World Bank, the U.S. Embassy in Haiti, and the Haitian Studies Association. All of these efforts have made dLOC the largest open access repository of Caribbean content and a significant resource for finding materials from and about the Caribbean for use in teaching, research, cultural and community life.
Reconstruyendo la huella de Humboldt en Cuba. Retos y oportunidades de la digitalización del patrimonio documental cubano-alemán del siglo XIX
Antonio Rojas Castro, Tobias Kraft y Kathrin Kraller
Berlin-Brandenburgische Akademie der Wissenschaften
antonio.rojas-castro@bbaw.de
kraft@bbaw.de
kathrin.kraller@bbaw.de
Grisel Terrón, Eritk Guerra y Alaina Solernou
Oficina del Historiador de la Ciudad de La Habana
grisel@patrimonio.ohc.cu
eritk@dic.ohc.cu
alaina@patrimonio.ohc.cu
Con esta comunicación pretendemos debatir sobre los principales retos y oportunidades que supone la cooperación, en el marco del Proyecto Humboldt Digital (ProHD), entre la Academia de las Ciencias y las Humanidades de Berlín (BBAW) y la Casa Humboldt de la Oficina del Historiador de la Ciudad de La Habana (OHCH). En concreto, nos gustaría responder a la siguiente pregunta: ¿cómo podemos cooperar para el beneficio mutuo (Sennett, 2012)? A continuación, analizaremos el estado actual de la digitalización y las Humanidades Digitales en Cuba. Por último, expondremos los primeros avances orientados hacia la construcción de un repositorio digital y la edición de documentos.
El futuro repositorio digital de ProHD tiene por objetivo preservar y dar acceso en línea a varias colecciones de documentos de naturaleza transnacional y multilingüe (español, francés y alemán), que permitan reconstruir no solo la “huella” del científico prusiano, sino también el pensamiento de muchos intelectuales y políticos locales, como Francisco Arango y Parreño, y las condiciones en que se producía el negocio esclavista, por ejemplo, mediante la digitalización de facturas de compra y venta. De esta manera, al poner la figura de Alexander von Humboldt en su contexto histórico, los usuarios obtendrán una mirada nueva y completa al problema de la esclavitud y al funcionamiento del sistema colonial, que condenaba a Cuba al monocultivo y a importar bienes de primera necesidad procedentes de la Metrópolis.
A fin de obtener digitalizaciones y ediciones académicas digitales fáciles de encontrar, accesibles, interoperables y sostenibles, se han planeado tres medidas principales, que están implementándose desde junio de 2019: en primer lugar, se ha adquirido equipamiento para reforzar la infraestructura tecnológica (ordenadores, escáneres de alta resolución, servidor, impresora, mobiliario, etc.) de la Casa Humboldt en La Habana en donde se llevará a cabo la digitalización de los documentos; en segundo lugar, se ha formado a los miembros del equipo mediante cursos (en línea, sobre todo a partir del confinamiento provocado por la COVID-19) sobre el proceso de digitalización, la creación de metadatos y la gestión de repositorios digitales. Por último, se han definido de manera conjunta unos criterios de selección de documentos, una política de digitalización y un flujo de trabajo compartido.
Inventar el virreinato de la Nueva Granada: curaduría crítica de fuentes primarias y la construcción de conjuntos de datos espaciales, 1739-1810
María José Afanador-Llach
Universidad de los Andes, Colombia
mj.afanador28@uniandes.edu.co
La invención del virreinato del Nuevo Reino de Granada (hoy Venezuela, Colombia, Ecuador y Panamá) en 1739 respondió a un proyecto económico de la monarquía española para proteger el norte de Suramérica de incursiones extranjeras y extraer mayores recursos para la corona. Dicho proyecto de búsqueda de unidad administrativa y geográfica se nutrió de la producción de conocimiento sobre economía política, es decir, sobre cómo crear riqueza en contextos de competencia. En fuentes primarias como mapas, textos sobre economía y descripciones geográficas creados por burócratas, naturalistas y militares en la colonia, se evidencian las dificultades de integrar un territorio montañoso y extenso y la importancia de la imaginación geográfica en este proceso.
El proyecto de integración territorial del virreinato se rompió con la crisis monárquica de 1808 cuando el reino se fragmentó en más de una docena de provincias autónomas. La crisis representó una oportunidad para que pueblos y ciudades avanzaran sus proyectos económicos locales y se imaginaran espacios económicos post-coloniales. La pregunta central que guía este proyecto es: ¿cómo se puede visualizar la relación entre la búsqueda de unidad territorial en el norte de Suramérica y la diversidad de paisajes y economías políticas en tensión en una interfaz de archivo y un mapa digital?
Esta ponencia explora los avances de este proyecto en sus dos primeras fases. La primera fase consiste en la curaduría crítica de fuentes primarias entre 1739 y 1810. Esta curaduría consiste en la selección de documentos, de fragmentos dentro de los documentos y de mapas del periodo, para organizarlos, enriquecer sus metadatos y disponerlos alrededor de un argumento y una narrativa histórica. Algunos de estos documentos se encuentran disponibles en libros editados, otros están digitalizados y un número pequeño se digitalizará. La segunda fase consiste la formulación de un proceso metodológico para extraer datos sobre las dimensiones espaciales y de economía política de este conjunto curado de fuentes primarias. Se trata de generar conjuntos de datos relacionales que permitan visualizar, analizar e interpretar transformaciones históricas-espaciales de la invención y crisis del virreinato de la Nueva Granada y los actores involucrados en este proceso. El proyecto busca que tanto la curaduría de fuentes primarias como los conjuntos de datos sean recursos abiertos en español para la investigación y docencia sobre el periodo. La ponencia discutirá los retos a los que se ha enfrentado el proyecto hasta el momento en términos de la curaduría de documentos y la definición metodológica para la extracción de datos.
Beyond digitization: Engaging the Community to Decolonize the Archival Record for the Enslaved
Amalia S. Levi
Archivist, Chair
The HeritEdge Connection, Barbados
amalia@heritedge.foundation
During 2018 and 2019, two historic newspapers, The Barbados Mercury Gazette (1783-1848) and The Barbadian (1822-1861), housed in the Barbados Department of Archives, were digitized through Endangered Archives Programme grants. As primary sources, they offer a detailed view of every aspect of the dystopian life in a British colony in the Caribbean during the eighteenth and nineteenth centuries. Digitization offers unparalleled access to these colonial newspapers that were previously largely inaccessible due to their fragility.
The presentation goes beyond digitization to discuss what happens after digitization is completed. Considering that the newspapers were part of the colonial information apparatus, how do we read against the grain to locate and reveal marginalized voices hidden in the digitized pages?
If our aim is to “lift” the voices of the enslaved from the pages, and retrospectively create a body of archival records, we need to provide access to the digitized pages as data. Such information however exists “locked” in digitized images that due to the deterioration of the paper and the discoloration of the pages are impossible to OCR. Finding and transcribing these ads is necessarily a manual process, at present.
Among the wealth of information appearing regularly in the newspapers, of particular importance are the “runaway slaves” ads. Archives usually preserve very little descriptive information about the enslaved, because their lived experiences were rarely recorded. When they appear in the archival record, they are enumerated or appraised as commodities. Thus these ads offer a rich trove of information about individual people, including name, age, physical appearance, skin color, clothing, accent, distinguishing features (such as body modifications from their country of origin or bodily harm, the result of violence), friends, relatives, and skills.
Digitization does not solve the gaps, silences, and omissions inherent in the archival record and the colonial epistemologies they contain. Colonial newspapers reflect the voices of the white class of planters, merchants, and colonial authorities. Simply digitizing and putting online risks elevating and amplifying the very colonial worldviews we mean to avoid.
To decolonize the record about the enslaved, digitization is only the starting point for further projects and initiatives to engage the community with its own history. Recent scholarship and public humanities efforts have shown that it is possible to challenge the erasure of colonial archival sources and read between the lines to tease out information that is not readily visible.  
Our aim is to create a collection of material by clipping “runaway slaves” ads from the digitized newspapers, transcribing the text, and enriching the human stories in each ad with additional contextual information. More specifically, the first part of the presentation will focus on work to develop the “Barbados Runaway Slaves Digital Collection,” a partnership between the Barbados Archives, the local non-profit HeritEdge Connection, and the Early Caribbean Digital Archive (ECDA) at Northeastern University (Boston).
This digital collection aims to foreground the centrality of enslaved voices by ‘lifting’ ads of individuals who chose to escape slavery from the pages of the newspapers, and turning them into individual, standalone archival records previously unrecorded in the archives. When completed, the “Runaway Slaves Digital Collection” will provide a central location for collecting and presenting these ads, and other opportunities for the public, especially students, both in Barbados, as well as abroad, to interact with the material in creative ways.
By ‘reading’ these ads in various ways and being able to ‘see’ them collectively, we can start seeing patterns and coping mechanisms. At the same time, the availability of this information points to what is not there, and invites us to be sensitive to gaps and silences.
The second part of the presentation will discuss public outreach initiatives. Material digitized through generous grants by institutions in the Global North are eventually hosted in digital platforms in those institutions. While praiseworthy, digitization through such grants ends up benefiting scholars in the Global North. Usually local people are unaware of these platforms, and often unable to access them, either due to bandwidth issues or simply to interfaces that might not be intuitive to use. We have tried to remedy this by focusing on public history work we are doing to increase awareness of and engagement with these digitized colonial records.
During the fall of 2019, we conducted a series of workshops aimed to familiarize the public with accessing the newspapers online and to transcribe ads. The workshops also provided a platform for the public to discuss the ads, and the many facets of slavery. People were able to see ancestors who chose to resist and escape bondage in adverse, inhumane conditions. Due to COVID-19, workshops planned for 2020 are being held online. The aim of these workshops is to engage people with the ads in creative ways, through genealogical research, speculative writing, or digital methods. In this way, we invite the public to contextualize the ads through their local knowledge of places mentioned in the ads, or intimate information about lived experiences. Beyond information that is there, we also hope that workshop participants can imagine what is not there, complete the stories, and give enslaved individuals their place in the archival record.
Additionally, the digitization of these primary sources has great potential for digital projects by students and other researchers that can highlight various aspects of the island’s history. The “runaway slaves” ads help people challenge the customary narrative of Barbadian passivity and submissiveness to slavery; reconstruct family and community networks that supported enslaved Barbadians; and they help give voice to ancestors, whom colonial records intentionally left voiceless.
Bibliographic References
Alpert-Abrams, H. Bliss, D. A., and Carbajal I. “Post-Custodialism for the Collective Good: Examining Neoliberalism in US-Latin American Archival Partnerships”. In: “Evidences, Implications, and Critical Interrogations of Neoliberalism in Information Studies”, eds. Marika Cifor and Jamie A. Lee. Special issue, Journal of Critical Library and Information Studies 2.1 (2019). DOI: https://doi.org/10.24242/jclis.v2i1.87
Dourish, P. “‘Computational Thinking’ and the Postcolonial in the Teaching from Country Programme”. Learning Communities. International Journal of Learning in Social Contexts, 2 (2010): 91-101.
Kelleher, Ch. “Archives without Archives: (Re)Locating and (Re)Defining the Archive Through Post-Custodial Praxis”. In: “Critical Archival Studies”, eds. Michelle Caswell, Ricardo Punzalan, and T-Kay Sangwand. Special issue, Journal of Critical Library and Information Studies, 1.2 (2017). DOI: https://doi.org/10.24242/jclis.v1i2.29                
Sennett, Richard. Juntos. Rituales, Placeres y políticas de la cooperación. Barcelona: Anagrama, 2012.","hsthuber@fiu.edu, antonio.rojas-castro@bbaw.de, kraft@bbaw.de, kathrin.kraller@bbaw.de, mj.afanador28@uniandes.edu.co, amalia@heritedge.foundation",Panel
"Beals, Melodee;
Bell, Emily","Loughborough University, United Kingdom",Exploring the Atlas of Digitised Newspapers: Enhancing Access to and Collaborative Research with Digitised Historical Newspapers,"digitisation, newspapers, metadata, history, literature","Global
English
19th Century
20th Century
Contemporary
meta-criticism (reflections on digital humanities and humanities computing)
metadata standards, systems, and methods
History
Library & information science",English,Global,"19th Century
20th Century
Contemporary","meta-criticism (reflections on digital humanities and humanities computing)
metadata standards, systems, and methods","History
Library & information science","<p dir=""ltr"">Building upon the two-year Digging into Data project, <em>Oceanic Exchanges: Tracing Global Information Networks in Historical Newspaper Repositories, 1840-1914 </em>(http://www.oceanicexchanges.org)<em>, </em>this workshop will introduce participants to the <em>Atlas of Digitised Newspapers</em>, a comprehensive guide to the histories, structures and metadata of the digitised newspapers collections studied by the project, including those held by:</p>

<p>●      Chronicling America (The Library of Congress)</p>
<p>●      The Hemeroteca Nacional Digital de México</p>
<p>●      The British Library</p>
<p>●      The Times Digital Archive</p>
<p>●      Delpher (Koninklijke Bibliotheek)</p>
<p>●      Europeana</p>
<p>●      Suomen Kansalliskirjaston Digitoidut Sanomalehdet</p>
<p>●      Trove (The National Library of Australia)</p>
<p>●      Papers Past (The National Library of New Zealand)</p>
<p>●      ZEFYS (Berlin State Library) </p>

<p>The <em>Atlas</em> provides readers with a deep contextualisation of these collections as well as detailed technical information about how to obtain, interpret, manipulate and map metadata and content across collections. Discussions will be supported by two additional online resources–an interactive visualisation of the metadata mapping and a linked dataset to support cross-database research. </p>

<p>Starting with humans rather than technologies or tools, the <em>Atlas</em> specifically uncovers the people behind the metadata and selection decisions. The workshop demonstrates a commitment to continuously co-constructing this work, and we hope to foster future collaborations. The <em>Atlas</em> will encourage research that “analyzes the historical and continued impacts of colonialism, postcolonialism, and hegemony”; siloed national archives privilege collections that look inward, but<em> Oceanic Exchanges</em> has used the combined collections to explore multilingual/multicultural groups with future possibilities for work on comparative Indigenous studies.</p>

<p>The aims of the workshop are to: </p>

<p>●      Present the findings and outputs of the project;</p>
<p>●      Create a space for critical discussion of the findings and future research opportunities by participants of the workshop;</p>
<p>●      Allow for initial collaborations and hands-on use of the outputs in developing historical, literary, and computer science research questions.</p>


<p>All participants will have the opportunity to submit and review the following in advance of the workshop: </p>

<ol start=""1"" type=""1""><li>An online profile of previous work with digitised newspapers and historical periodicals.</li>
<li>Suggestions for possible in-workshop collaborations using the <em>Atlas</em>, visualisations and LOD datasets across our core expected disciplines.</li>
</ol>
<p dir=""ltr""><strong>Learning Outcomes</strong></p>

<p>By the end of the workshop, participants will: </p>

<p>-       Develop a nuanced understanding of available data beyond OCR text;</p>
<p>-       Practice cross-collection data management and cleaning;</p>
<p>-       Develop an understanding of metadata and selection practices.</p>

<p dir=""ltr""><strong>Schedule</strong></p>

<p>●      Introductions of workshop organisers and participants</p>
<p>●      Introduction to Oceanic Exchanges</p>


<p>Coffee break</p>


<p>●      Presentation of the Atlas of Digitised Newspapers</p>
<p>●      Discussion of collections not represented in the <em>Atlas</em> by participants</p>


<p>Lunch (30 min)</p>


<p>●      Walkthrough of Datasets and Use Cases</p>
<p>●      Introduction of Disciplinary Challenges</p>

<p>                ○      Historical / Literary Scholarship Challenge</p>
<p>                ○      Library and Archival Science Challenge</p>
<p>                ○      Data, Information and Computer Science Challenge</p>

<p>●      Division into groups for Disciplinary Challenges</p>
<p>●      Presentations, feedback and discussions</p>
<p>●      Closing Remarks</p>

<p dir=""ltr""><strong>Readings: Newspaper Preservation</strong></p>
<p>Bourke, Thomas A. ""Scholarly Micropublishing, Preservation Microfilming, and the National Preservation Effort in the Last Two Decades of the Twentieth Century: History and Prognosis."" <em>Microform Review</em> 19.1 (1990): 4–16. https://doi.org/10.1515/mfir.1990.19.1.4.</p>
<p>Silverman, Randy. ""Retaining hardcopy papers still important in digital age."" <em>Newspaper Research Journal </em>36.3 (2005): 363–72. https://doi.org/10.1177/0739532915600749. </p>
<p>Walravens, Hartmut, ed. <em>International Newspaper Librarianship for the 21st Century. </em>Müchen: K. G. Saur, 2006.</p>
<p dir=""ltr""><strong>Newspaper Digitisation</strong></p>
<p>Abruzzi, Ray, Luisa Calè, and Ana Parejo Vadillo. ""Gale Digital Collections: Ray Abruzzi Interviewed by Luisa Calè and Ana Parejo Vadillo."" 1<em>9: Interdisciplinary Studies in the Long Nineteenth Century</em> 21 (2015): http://19.bbk.ac.uk/articles/753/.</p>
<p>Sigauke, D. T. ‘Digitisation Technologies for Newspaper Archives in Zimbabwe: The ICT Requirements for Digitising a Selected Bulawayo Newspaper Publication at the National Archives of Zimbabwe’. In <em>2017 IST-Africa Week Conference (IST-Africa)</em> (2017): 1–10. https://doi.org/10.23919/ISTAFRICA.2017.8102315.</p>
<p>King, Edmund. ""Digitisation of Newspapers at the British Library."" <em>The Serials Librarian: From the Printed Page to the Digital Age </em>49.1-2  (2005): 165–81. https://doi.org/10.1300/J123v49n01_07.</p>
<p dir=""ltr""><strong>Metadata Standards</strong></p>
<p>Europeana Data Model. Europeana Pro. Accessed 11 October 2019. https://pro.europeana.eu/resources/standardization-tools/edm-documentation.</p>
<p>Wilkinson, M. D. et al. ""The FAIR Guiding Principles for scientific data management and stewardship."" <em>Scientific Data</em> 3 (2016): article number 160018. https://www.nature.com/articles/sdata201618</p>
<p dir=""ltr""><strong>Crowdsourcing Heritage Data</strong></p>
<p>Recker, Mimi M. and David A. Wiley. ""A Non-authoritative Educational Metadata Ontology for Filtering and Recommending Learning Objects,"" <em>Interactive Learning Environments </em>9.3 (2001): 255–71. https://doi.org/10.1076/ilee.9.3.255.3568.</p>
<p dir=""ltr""><strong>Logistics</strong></p>
<p>Participants must have experience working with digitised newspapers/periodicals.</p>
","Building upon the two-year Digging into Data project, Oceanic Exchanges: Tracing Global Information Networks in Historical Newspaper Repositories, 1840-1914 (http://www.oceanicexchanges.org), this workshop will introduce participants to the Atlas of Digitised Newspapers, a comprehensive guide to the histories, structures and metadata of the digitised newspapers collections studied by the project, including those held by:
●      Chronicling America (The Library of Congress)
●      The Hemeroteca Nacional Digital de México
●      The British Library
●      The Times Digital Archive
●      Delpher (Koninklijke Bibliotheek)
●      Europeana
●      Suomen Kansalliskirjaston Digitoidut Sanomalehdet
●      Trove (The National Library of Australia)
●      Papers Past (The National Library of New Zealand)
●      ZEFYS (Berlin State Library) 
The Atlas provides readers with a deep contextualisation of these collections as well as detailed technical information about how to obtain, interpret, manipulate and map metadata and content across collections. Discussions will be supported by two additional online resources–an interactive visualisation of the metadata mapping and a linked dataset to support cross-database research. 
Starting with humans rather than technologies or tools, the Atlas specifically uncovers the people behind the metadata and selection decisions. The workshop demonstrates a commitment to continuously co-constructing this work, and we hope to foster future collaborations. The Atlas will encourage research that “analyzes the historical and continued impacts of colonialism, postcolonialism, and hegemony”; siloed national archives privilege collections that look inward, but Oceanic Exchanges has used the combined collections to explore multilingual/multicultural groups with future possibilities for work on comparative Indigenous studies.
The aims of the workshop are to: 
●      Present the findings and outputs of the project;
●      Create a space for critical discussion of the findings and future research opportunities by participants of the workshop;
●      Allow for initial collaborations and hands-on use of the outputs in developing historical, literary, and computer science research questions.
All participants will have the opportunity to submit and review the following in advance of the workshop: 
An online profile of previous work with digitised newspapers and historical periodicals.
Suggestions for possible in-workshop collaborations using the Atlas, visualisations and LOD datasets across our core expected disciplines.
Learning Outcomes
By the end of the workshop, participants will: 
-       Develop a nuanced understanding of available data beyond OCR text;
-       Practice cross-collection data management and cleaning;
-       Develop an understanding of metadata and selection practices.
Schedule
●      Introductions of workshop organisers and participants
●      Introduction to Oceanic Exchanges
Coffee break
●      Presentation of the Atlas of Digitised Newspapers
●      Discussion of collections not represented in the Atlas by participants
Lunch (30 min)
●      Walkthrough of Datasets and Use Cases
●      Introduction of Disciplinary Challenges
 ○      Historical / Literary Scholarship Challenge
   ○      Library and Archival Science Challenge
   ○      Data, Information and Computer Science Challenge
●      Division into groups for Disciplinary Challenges
●      Presentations, feedback and discussions
●      Closing Remarks
Readings: Newspaper Preservation
Bourke, Thomas A. ""Scholarly Micropublishing, Preservation Microfilming, and the National Preservation Effort in the Last Two Decades of the Twentieth Century: History and Prognosis."" Microform Review 19.1 (1990): 4–16. https://doi.org/10.1515/mfir.1990.19.1.4.
Silverman, Randy. ""Retaining hardcopy papers still important in digital age."" Newspaper Research Journal 36.3 (2005): 363–72. https://doi.org/10.1177/0739532915600749. 
Walravens, Hartmut, ed. International Newspaper Librarianship for the 21st Century. Müchen: K. G. Saur, 2006.
Newspaper Digitisation
Abruzzi, Ray, Luisa Calè, and Ana Parejo Vadillo. ""Gale Digital Collections: Ray Abruzzi Interviewed by Luisa Calè and Ana Parejo Vadillo."" 19: Interdisciplinary Studies in the Long Nineteenth Century 21 (2015): http://19.bbk.ac.uk/articles/753/.
Sigauke, D. T. ‘Digitisation Technologies for Newspaper Archives in Zimbabwe: The ICT Requirements for Digitising a Selected Bulawayo Newspaper Publication at the National Archives of Zimbabwe’. In 2017 IST-Africa Week Conference (IST-Africa) (2017): 1–10. https://doi.org/10.23919/ISTAFRICA.2017.8102315.
King, Edmund. ""Digitisation of Newspapers at the British Library."" The Serials Librarian: From the Printed Page to the Digital Age 49.1-2 (2005): 165–81. https://doi.org/10.1300/J123v49n01_07.
Metadata Standards
Europeana Data Model. Europeana Pro. Accessed 11 October 2019. https://pro.europeana.eu/resources/standardization-tools/edm-documentation.
Wilkinson, M. D. et al. ""The FAIR Guiding Principles for scientific data management and stewardship."" Scientific Data 3 (2016): article number 160018. https://www.nature.com/articles/sdata201618
Crowdsourcing Heritage Data
Recker, Mimi M. and David A. Wiley. ""A Non-authoritative Educational Metadata Ontology for Filtering and Recommending Learning Objects,"" Interactive Learning Environments 9.3 (2001): 255–71. https://doi.org/10.1076/ilee.9.3.255.3568.
Logistics
Participants must have experience working with digitised newspapers/periodicals.","m.h.beals@lboro.ac.uk, e.bell@lboro.ac.uk",Workshop/Tutorial 4